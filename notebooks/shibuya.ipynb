{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFEaDbdXmg_a"
   },
   "source": [
    "# æ¯æœˆãƒ»æ¯é€±ã®é ­ã«å®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Lc0SFl_1vdlh"
   },
   "outputs": [],
   "source": [
    "#@title GAãƒ¬ãƒãƒ¼ãƒˆæ›´æ–° â†’_ch-m\n",
    "# GA APIã§ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¦é›†è¨ˆã—ãŸçµæœã‚’Google Sheetsã¸ä¿å­˜ã™ã‚‹\n",
    "# sim@bã§å®Ÿè¡Œã™ã‚‹ã“ã¨\n",
    "\n",
    "#@markdown æ›¸ãè¾¼ã‚€Google Sheets\n",
    "# URL = \"https://docs.google.com/spreadsheets/d/1wz20DITKF1GpIryhifiPShEbSgIVwSqlD22WEL6dpCY\" #@param {type:\"string\"}\n",
    "URL = \"https://docs.google.com/spreadsheets/d/10QO3zJeHQKLpBMhv-93PTHVtRIQ7UlLul3vuXOXO58k\"\n",
    "\n",
    "try:\n",
    "    from google.colab import data_table, files as colab_files\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    data_table = None\n",
    "    colab_files = None\n",
    "    IN_COLAB = False\n",
    "\n",
    "try:\n",
    "    from megaton import files, start\n",
    "except ModuleNotFoundError:\n",
    "    if IN_COLAB:\n",
    "        %pip install -U -q git+https://github.com/mak00s/megaton\n",
    "        from megaton import files, start\n",
    "    else:\n",
    "        _pip_install([\"install\", \"-e\", \".\"])\n",
    "        from megaton import files, start  # noqa: F401\n",
    "\n",
    "if IN_COLAB and colab_files is not None:\n",
    "    files = colab_files\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from IPython.display import clear_output\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "TIMEZONE = \"Asia/Tokyo\"\n",
    "KEY_COLS = ['month', 'clinic', 'channel', 'medium', 'source', 'users', 'cv', 'ad_cost']\n",
    "\n",
    "def get_past_date(n_days=None, n_months=None, timezone=TIMEZONE, return_date_obj=False):\n",
    "    \"\"\"\n",
    "    Returns a date N days ago, or the 1st of N months ago. Defaults to today.\n",
    "    \"\"\"\n",
    "    now = datetime.now(pytz.timezone(timezone))\n",
    "\n",
    "    if n_days is None and n_months is None:\n",
    "        result_date = now\n",
    "    elif n_days is not None and n_months is not None:\n",
    "        raise ValueError(\"Specify either 'n_days' or 'n_months', not both.\")\n",
    "    elif n_days is not None:\n",
    "        result_date = now - timedelta(days=n_days)\n",
    "    else:\n",
    "        result_date = now.replace(day=1) - relativedelta(months=n_months)\n",
    "\n",
    "    return result_date.date() if return_date_obj else result_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def parse_end_date(raw_date_str):\n",
    "    \"\"\"\n",
    "    Parse a date string in various formats:\n",
    "    YYYY-MM-DD, YYYYMMDD, YYYY-MM, YYYYMM â†’ datetime\n",
    "    If only year+month is provided, returns last day of that month.\n",
    "    \"\"\"\n",
    "    raw_date_str = raw_date_str.strip().replace(\"/\", \"-\")\n",
    "\n",
    "    try:\n",
    "        if re.fullmatch(r\"\\d{8}\", raw_date_str):  # YYYYMMDD\n",
    "            return datetime.strptime(raw_date_str, \"%Y%m%d\")\n",
    "        elif re.fullmatch(r\"\\d{6}\", raw_date_str):  # YYYYMM\n",
    "            dt = datetime.strptime(raw_date_str, \"%Y%m\")\n",
    "            return (dt.replace(day=1) + relativedelta(months=1)) - timedelta(days=1)\n",
    "        elif re.fullmatch(r\"\\d{4}-\\d{2}-\\d{2}\", raw_date_str):  # YYYY-MM-DD\n",
    "            return datetime.strptime(raw_date_str, \"%Y-%m-%d\")\n",
    "        elif re.fullmatch(r\"\\d{4}-\\d{2}\", raw_date_str):  # YYYY-MM\n",
    "            dt = datetime.strptime(raw_date_str, \"%Y-%m\")\n",
    "            return (dt.replace(day=1) + relativedelta(months=1)) - timedelta(days=1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid date format\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"âŒ end_dateã®å½¢å¼ãŒä¸æ­£ã§ã™: {e}\")\n",
    "\n",
    "def get_report_range(target_months_ago):\n",
    "    now = datetime.now(pytz.timezone(TIMEZONE))\n",
    "\n",
    "    if target_months_ago == 0:\n",
    "        # date_from = 13ãƒ¶æœˆå‰ã®æœˆåˆ\n",
    "        base = now.replace(day=1)\n",
    "        date_from = (base - relativedelta(months=12)).date().isoformat()\n",
    "        # date_to = æ˜¨æ—¥\n",
    "        date_to = (now - timedelta(days=1)).date().isoformat()\n",
    "    else:\n",
    "        base = now.replace(day=1) - relativedelta(months=target_months_ago)\n",
    "        date_from = (base - relativedelta(months=12)).date().isoformat()\n",
    "        date_to = (base + relativedelta(months=1) - timedelta(days=1)).date().isoformat()\n",
    "\n",
    "    return date_from, date_to\n",
    "\n",
    "def apply_normalization(df, column, pattern_map, new_col=None, drop_original=False):\n",
    "    \"\"\"\n",
    "    Normalizes a column using regex-based pattern matching and custom actions.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        column (str): The column to normalize.\n",
    "        pattern_map (dict): Mapping of regex patterns to replacements or actions.\n",
    "        new_col (str, optional): Column name to store result in (default: overwrite original).\n",
    "        drop_original (bool): If True and new_col is set, drop the original column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"Column '{column}' not found in DataFrame.\")\n",
    "\n",
    "    def normalize(value):\n",
    "        if not isinstance(value, str):\n",
    "            return value\n",
    "        value = value.lower()\n",
    "        for pattern, action in pattern_map.items():\n",
    "            try:\n",
    "                if re.search(pattern, value):\n",
    "                    if action == 'remove_m_prefix':\n",
    "                        return re.sub(r'^m\\.', '', value)\n",
    "                    elif action == 'remove_www_prefix':\n",
    "                        return re.sub(r'^www\\.', '', value)\n",
    "                    # Add more custom actions here if needed\n",
    "                    else:\n",
    "                        return action  # regular substitution\n",
    "            except re.error as err:\n",
    "                print(f\"âš ï¸ Skipping bad pattern '{pattern}': {err}\")\n",
    "        return value\n",
    "\n",
    "    target_col = new_col or column\n",
    "    df[target_col] = df[column].apply(normalize)\n",
    "\n",
    "    if drop_original and new_col:\n",
    "        df.drop(columns=[column], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def classify_channel(row):\n",
    "    ch = row.get(\"channel\", \"\")\n",
    "    med = row.get(\"medium\", \"\").lower()\n",
    "    src = row.get(\"source\", \"\").lower().replace(\"www.\", \"\")\n",
    "\n",
    "    # --- AIãƒãƒ£ãƒãƒ«æ¤œå‡º ---\n",
    "    ai_keywords = [\"bard\", \"chatgpt\", \"claude\", \"copilot\", \"gemini\", \"perplexity\"]\n",
    "    if any(keyword in src for keyword in ai_keywords) or any(keyword in med for keyword in ai_keywords):\n",
    "        return \"AI\"\n",
    "\n",
    "    # --- Map åˆ¤å®š ---\n",
    "    if med == \"map\" or re.search(r'(^|\\.)maps?\\.', src):\n",
    "        return \"Map\"\n",
    "\n",
    "    # --- Referral å†åˆ¤å®š ---\n",
    "    if ch == \"Referral\":\n",
    "        organic_keywords = [\"search\", \"docomo.ne.jp\", \".jword.jp\", \"jp.hao123.com\"]\n",
    "        if any(keyword in src for keyword in organic_keywords):\n",
    "            return \"Organic Search\"\n",
    "\n",
    "        sns_keywords = [\"threads.net\", \"threads\"]\n",
    "        if any(keyword in src for keyword in sns_keywords):\n",
    "            return \"Organic Social\"\n",
    "\n",
    "        if any(domain in src for domain in GROUP_DOMAINS):\n",
    "            return \"Group\"\n",
    "\n",
    "    # fallback: original GA4 channel\n",
    "    return ch\n",
    "\n",
    "def infer_clinic_from_lp(lp_val):\n",
    "    if not isinstance(lp_val, str) or not lp_val:\n",
    "        return \"ä¸æ˜\"\n",
    "\n",
    "    # 1ï¸âƒ£ dentamap_id ã§åˆ¤å®š\n",
    "    for site in sites:\n",
    "        if \"dentamap_id\" in site and f\"id={site['dentamap_id']}\" in lp_val:\n",
    "            return site[\"clinic\"]\n",
    "\n",
    "    # 2ï¸âƒ£ id=æ•°å­— ãŒå«ã¾ã‚Œã‚‹å ´åˆã¯æ•°å­—ã‚’è¿”ã™\n",
    "    match = re.search(r\"id=(\\d+)\", lp_val)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "\n",
    "    # 3ï¸âƒ£ ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ã‚‰ã‚¯ãƒªãƒ‹ãƒƒã‚¯åã‚’æ¨å®š\n",
    "    parsed = urlparse(lp_val if \"://\" in lp_val else f\"http://{lp_val}\")\n",
    "    domain = parsed.netloc.lower()\n",
    "\n",
    "    clinic_map = {\n",
    "        \"sapporo\": \"æœ­å¹Œ\",\n",
    "        \"sendai\": \"ä»™å°\",\n",
    "        \"ikebukuro\": \"æ± è¢‹\",\n",
    "        \"shinjuku\": \"æ–°å®¿\",\n",
    "        \"shibuya\": \"æ¸‹è°·\",\n",
    "        \"tokyo\": \"æ±äº¬\",\n",
    "        \"yokohama\": \"æ¨ªæµœ\",\n",
    "        \"umeda\": \"æ¢…ç”°\",\n",
    "        \"namba\": \"é›£æ³¢\",\n",
    "        \"hakata\": \"åšå¤š\",\n",
    "        \"tenjin\": \"å¤©ç¥\",\n",
    "    }\n",
    "\n",
    "    for key, name in clinic_map.items():\n",
    "        if key in domain:\n",
    "            return name\n",
    "\n",
    "    # 4ï¸âƒ£ ã©ã‚Œã«ã‚‚è©²å½“ã—ãªã„å ´åˆ\n",
    "    return \"ä¸æ˜\"\n",
    "\n",
    "# Main report generation\n",
    "def generate_channel_report(\n",
    "    end_date=None,\n",
    "    num_months=1,\n",
    "    sheet_name=\"_ch-m\",\n",
    "    sheet_url=None,\n",
    "    cell_sheet=None,\n",
    "    cell_start=None,\n",
    "    cell_end=None,\n",
    "    clinic_filter=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate GA4 channel report and optionally save to Google Sheets.\n",
    "\n",
    "    Args:\n",
    "        end_date (str, optional): Accepts YYYY-MM-DD, YYYYMMDD, YYYY-MM, or YYYYMM. Defaults to yesterday.\n",
    "        num_months (int): Number of months to include.\n",
    "        sheet_name (str): Sheet tab name for saving.\n",
    "        sheet_url (str, optional): Google Sheet URL. If set, saves automatically.\n",
    "        cell_sheet (str): Sheet to write start/end date into.\n",
    "        cell_start (str): Cell for start date (e.g., \"A1\").\n",
    "        cell_end (str): Cell for end date (e.g., \"B1\").\n",
    "        clinic_filter (list[str], optional): List of clinic names to include (e.g., [\"æ¸‹è°·\", \"æ–°å®¿\"]).\n",
    "    \"\"\"\n",
    "    if end_date is None:\n",
    "        end_date = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    end_dt = parse_end_date(end_date)\n",
    "    start_dt = end_dt.replace(day=1) - relativedelta(months=(num_months - 1))\n",
    "    start_date = start_dt.strftime(\"%Y-%m-%d\")\n",
    "    end_date = end_dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    mg.report.set_dates(start_date, end_date)\n",
    "    print(f\"ğŸ“… ãƒ¬ãƒãƒ¼ãƒˆæœŸé–“ï¼š{start_date}ã€œ{end_date} ({TARGET_MONTHS_AGO})\")\n",
    "\n",
    "    dfs = []\n",
    "    selected_sites = [s for s in sites if not clinic_filter or s[\"clinic\"] in clinic_filter]\n",
    "    # print(f\"ğŸ” å¯¾è±¡ã‚¯ãƒªãƒ‹ãƒƒã‚¯: {', '.join(s['clinic'] for s in selected_sites)}\")\n",
    "\n",
    "    # --- å„ã‚¯ãƒªãƒ‹ãƒƒã‚¯ã®å‡¦ç† ---\n",
    "    for site in selected_sites:\n",
    "        clinic_name = site['clinic']\n",
    "        print(f\"ğŸ”„ {clinic_name}...\", end='')\n",
    "        ga4_property_id = site['ga4_property_id']\n",
    "        mg.ga['4'].property.id = ga4_property_id\n",
    "\n",
    "        # Step 1: CV & ad cost\n",
    "        mg.report.run(d=[(\"yearMonth\", \"month\"), (\"defaultChannelGroup\", \"channel\"), \"medium\", \"source\"],\n",
    "                      m=[(site[\"cv\"], \"cv\"), (\"advertiserAdCost\", \"ad_cost\")])\n",
    "        df_metrics = mg.report.data\n",
    "\n",
    "        # Step 2: Active users\n",
    "        mg.report.run(d=[(\"yearMonth\", \"month\"), (\"sessionDefaultChannelGroup\", \"channel\"),\n",
    "                         (\"sessionMedium\", \"medium\"), (\"sessionSource\", \"source\")],\n",
    "                      m=[(\"activeUsers\", \"users\")])\n",
    "        df_users = mg.report.data\n",
    "\n",
    "        if df_users is not None and df_metrics is not None:\n",
    "            merged_df = pd.merge(df_users, df_metrics, on=[\"month\", \"channel\", \"medium\", \"source\"], how=\"left\")\n",
    "        else:\n",
    "            merged_df = pd.DataFrame(columns=KEY_COLS)\n",
    "            print(f\"âš ï¸ ãƒ‡ãƒ¼ã‚¿ãªã—: {clinic_name}\")\n",
    "\n",
    "        merged_df['clinic'] = clinic_name\n",
    "        dfs.append(merged_df)\n",
    "\n",
    "    # --- dentamap ç”¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ & CV ãƒ¬ãƒãƒ¼ãƒˆã‚’è¿½åŠ  ---\n",
    "    clinic_name = \"dentamap\"\n",
    "    print(f\"ğŸ”„ {clinic_name}\")\n",
    "    site = next((s for s in selected_sites if s[\"clinic\"] == clinic_name), None)\n",
    "    mg.ga['4'].property.id = site[\"ga4_property_id\"]\n",
    "    mg.report.run(\n",
    "        d=[\n",
    "            (\"yearMonth\", \"month\"),\n",
    "            (\"sessionDefaultChannelGroup\", \"channel\"),\n",
    "            (\"sessionSource\", \"source\"),\n",
    "            (\"landingPagePlusQueryString\", \"lp\")\n",
    "        ],\n",
    "        filter_d=\"sessionDefaultChannelGroup==Organic Social;landingPagePlusQueryString=@/apl/netuser/?id=\",\n",
    "        m=[\n",
    "            (\"activeUsers\", \"users\"),\n",
    "            (\"keyEvents\", \"cv\")\n",
    "        ]\n",
    "    )\n",
    "    if mg.report.data is not None:\n",
    "        df_dentamap = mg.report.data.copy()\n",
    "        # def infer_clinic_from_lp(lp_val):\n",
    "        #     for site in sites:\n",
    "        #         if \"dentamap_id\" in site and f\"id={site['dentamap_id']}\" in lp_val:\n",
    "        #             return site[\"clinic\"]\n",
    "        #     # clinic ã‚’ç‰¹å®šã§ããªã„å ´åˆã¯ã€id=XXXX ã®æ•°å­—éƒ¨åˆ†ã‚’ãã®ã¾ã¾ä½¿ã†\n",
    "        #     match = re.search(r\"id=(\\d+)\", lp_val)\n",
    "        #     return match.group(1) if match else \"ä¸æ˜\"\n",
    "\n",
    "        df_dentamap['clinic'] = df_dentamap['lp'].apply(infer_clinic_from_lp)\n",
    "        df_dentamap['medium'] = \"dentamap\"\n",
    "        # KEY_COLS ã«å¿…è¦ãªåˆ—ãŒè¶³ã‚Šãªã„å ´åˆã¯è£œå®Œ\n",
    "        for col in KEY_COLS:\n",
    "            if col not in df_dentamap.columns:\n",
    "                df_dentamap[col] = None\n",
    "        dfs.append(df_dentamap[KEY_COLS])\n",
    "\n",
    "    # å…±é€šã®å‰å‡¦ç†\n",
    "    df_all = pd.concat([df for df in dfs if not df.empty], ignore_index=True)\n",
    "\n",
    "    # source æ­£è¦åŒ–\n",
    "    df_all = apply_normalization(df_all, 'source', source_map)\n",
    "\n",
    "    # channel å†åˆ†é¡\n",
    "    df_all['channel'] = df_all.apply(classify_channel, axis=1)\n",
    "\n",
    "    # æ•°å€¤å¤‰æ›ï¼ˆæ—¢å­˜å‡¦ç†ï¼‰\n",
    "    df_all['cv'] = pd.to_numeric(df_all['cv'], errors='coerce').fillna(0).astype(int)\n",
    "    df_all['ad_cost'] = pd.to_numeric(df_all['ad_cost'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    # åˆ—é †ã‚’çµ±ä¸€\n",
    "    df_result = df_all[KEY_COLS]\n",
    "\n",
    "    if sheet_url:\n",
    "        if cell_sheet and cell_start and cell_end:\n",
    "            mg.sheets_service.update_cells(sheet_url, cell_sheet, {cell_start: start_date, cell_end: end_date})\n",
    "        mg.sheets_service.upsert_df(\n",
    "            sheet_url,\n",
    "            sheet_name,\n",
    "            df_result,\n",
    "            keys=[\"month\", \"clinic\"],\n",
    "            columns=KEY_COLS,\n",
    "            sort_by=[\"month\", \"clinic\", \"channel\", \"medium\", \"source\"],\n",
    "            create_if_missing=True,\n",
    "        )\n",
    "\n",
    "    return df_result\n",
    "\n",
    "def classify_page(df, mapping, new_col='page_category'):\n",
    "    \"\"\"\n",
    "    Classifies pages based on regex patterns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with a 'page' or 'landing_page' column.\n",
    "        mapping (dict): {regex_pattern: category_label}\n",
    "        new_col (str): Column name to write classification to.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Original df with an added classification column.\n",
    "    \"\"\"\n",
    "    if df.empty or not mapping:\n",
    "        return df\n",
    "\n",
    "    def map_page(url):\n",
    "        for pattern, label in mapping.items():\n",
    "            try:\n",
    "                if re.search(pattern, url):\n",
    "                    return label\n",
    "            except re.error as e:\n",
    "                print(f\"âš ï¸ Invalid regex pattern '{pattern}': {e}\")\n",
    "        return \"other\"  # fallback\n",
    "\n",
    "    df = df.copy()\n",
    "    df[new_col] = df['page'].apply(map_page)\n",
    "    return df\n",
    "\n",
    "# Setup\n",
    "CREDS_PATH = \"/nbs/key/sa-shibuya-kyousei.json\"\n",
    "mg = start.Megaton(CREDS_PATH)\n",
    "\n",
    "GA4_ACCOUNT = '141366107'\n",
    "GA4_PROPERTY = '254470346'\n",
    "mg.ga['4'].account.select(GA4_ACCOUNT)\n",
    "mg.ga['4'].property.select(GA4_PROPERTY)\n",
    "\n",
    "# BQæº–å‚™\n",
    "GCP_PROJECT = 'shibuya-kyousei'\n",
    "bq = mg.launch_bigquery(GCP_PROJECT)\n",
    "clear_output()\n",
    "print(f\"âœ… BigQueryã‚’ä½¿ã†æº–å‚™ãŒã§ãã¾ã—ãŸã€‚\")\n",
    "\n",
    "# Run Report\n",
    "\n",
    "# å¯¾è±¡ã‚¯ãƒªãƒ‹ãƒƒã‚¯ã¨source/pageã®ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ«ãƒ¼ãƒ«ã‚’GSã‹ã‚‰å–å¾—\n",
    "source_map = {}\n",
    "page_map = {}\n",
    "sites = []\n",
    "try:\n",
    "    if mg.open.sheet(URL):\n",
    "        mg.gs.sheet.select('config')\n",
    "        sites = mg.gs.sheet.data or []\n",
    "        df_config = pd.DataFrame(sites)\n",
    "        if not {'clinic', 'min_impressions', 'max_position'}.issubset(df_config.columns):\n",
    "            raise ValueError(\"âš ï¸ 'config' sheet must contain 'clinic', 'min_impressions', and 'max_position' columns.\")\n",
    "        df_config['min_impressions'] = pd.to_numeric(df_config['min_impressions'], errors='coerce').fillna(10)\n",
    "        df_config['max_position'] = pd.to_numeric(df_config['max_position'], errors='coerce').fillna(50)\n",
    "\n",
    "        mg.gs.sheet.select('source_map')\n",
    "        df_map = pd.DataFrame(mg.gs.sheet.data)\n",
    "        if {'pattern', 'normalized'}.issubset(df_map.columns):\n",
    "            source_map = dict(zip(df_map['pattern'], df_map['normalized']))\n",
    "        else:\n",
    "            raise ValueError(\"âš ï¸ 'source_map' sheet must include 'pattern' and 'normalized' columns.\")\n",
    "\n",
    "        try:\n",
    "            mg.gs.sheet.select('page_map')\n",
    "            df_pmap = pd.DataFrame(mg.gs.sheet.data)\n",
    "            if {'pattern', 'category'}.issubset(df_pmap.columns):\n",
    "                page_map = dict(zip(df_pmap['pattern'], df_pmap['category']))\n",
    "            else:\n",
    "                raise ValueError(\"âš ï¸ 'page_map' must include 'pattern' and 'category' columns.\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Failed to load page_map: {e}\")\n",
    "\n",
    "except PermissionError:\n",
    "    print(\"âŒ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©ãŒã‚ã‚Šã¾ã›ã‚“ã€‚è©²å½“ã®Google Sheetsã«ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ç·¨é›†æ¨©é™ã‚’ä»˜ä¸ã—ã¦ãã ã•ã„ã€‚\")\n",
    "except Exception as e:\n",
    "    print(\"âŒ ã‚·ãƒ¼ãƒˆã®æ›¸ãè¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:\", e)\n",
    "\n",
    "GROUP_DOMAINS = set(site['domain'].replace('www.', '') for site in sites if 'domain' in site)\n",
    "GROUP_DOMAINS |= {\n",
    "    'invisalignteen-kyousei.jp',\n",
    "    'mienaikyousei.tokyo',\n",
    "    'mienaiuragawakyousei.tokyo',\n",
    "}\n",
    "\n",
    "CLINIC_FILTER = [\n",
    "    \"æœ­å¹Œ\",\n",
    "    \"ä»™å°\",\n",
    "    \"æ± è¢‹\",\n",
    "    \"æ–°å®¿\",\n",
    "    \"æ¸‹è°·\",\n",
    "    \"æ±äº¬\",\n",
    "    \"æ¨ªæµœ\",\n",
    "    \"æ¢…ç”°\",\n",
    "    \"é›£æ³¢\",\n",
    "    \"åšå¤š\",\n",
    "    \"å¤©ç¥\",\n",
    "    # \"Gr.\",\n",
    "    \"dentamap\",\n",
    "]\n",
    "\n",
    "# Load query_map from sheet\n",
    "query_map = {}\n",
    "try:\n",
    "    mg.gs.sheet.select('query_map')\n",
    "    df_qmap = pd.DataFrame(mg.gs.sheet.data)\n",
    "    if {'pattern', 'mapped_to'}.issubset(df_qmap.columns):\n",
    "        query_map = dict(zip(df_qmap['pattern'], df_qmap['mapped_to']))\n",
    "    else:\n",
    "        raise ValueError(\"âš ï¸ 'query_map' must include 'pattern' and 'mapped_to' columns.\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Failed to load query_map: {e}\")\n",
    "\n",
    "# ãƒ¬ãƒãƒ¼ãƒˆæœŸé–“\n",
    "#@markdown ãƒ¬ãƒãƒ¼ãƒˆå¯¾è±¡æœˆï¼ˆ0 = å½“æœˆ, 1 = å…ˆæœˆ, 2 = 2ãƒ¶æœˆå‰ ...ï¼‰\n",
    "TARGET_MONTHS_AGO = 0  #@param {type: \"number\"}\n",
    "date_from, date_to = get_report_range(TARGET_MONTHS_AGO)\n",
    "# print(f\"æœŸé–“ï¼š{date_from}ã€œ{date_to}\")\n",
    "\n",
    "df = generate_channel_report(\n",
    "    end_date=date_to,\n",
    "    # num_months=13,\n",
    "    num_months=1,\n",
    "    clinic_filter=CLINIC_FILTER,\n",
    "    sheet_url=URL,\n",
    "    sheet_name=\"_ch-m\",\n",
    "    cell_sheet=\"CV\",\n",
    "    cell_start=\"L1\",\n",
    "    cell_end=\"N1\"\n",
    ")\n",
    "mg.show.table(df, rows=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "67gSJ8CcgTrE"
   },
   "outputs": [],
   "source": [
    "#@title GAã‹ã‚‰LPã®CVã‚’å¾—ã¦åˆ†é¡ â†’_lp_cat\n",
    "\n",
    "# åŒã˜æœŸé–“ã§GA4ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ä¿å­˜\n",
    "# date_from = get_past_date(n_months=TARGET_MONTHS_AGO)\n",
    "# date_to = (get_past_date(n_months=TARGET_MONTHS_AGO - 1, return_date_obj=True) - timedelta(days=1)).isoformat()\n",
    "print(f\"æœŸé–“ï¼š{date_from}ã€œ{date_to}\")\n",
    "\n",
    "mg.report.set_dates(date_from, date_to)\n",
    "\n",
    "dfs = []\n",
    "selected_sites = [s for s in sites if not CLINIC_FILTER or s[\"clinic\"] in CLINIC_FILTER]\n",
    "\n",
    "for site in selected_sites:\n",
    "    clinic = site['clinic']\n",
    "    ga4_id = site['ga4_property_id']\n",
    "    mg.ga['4'].property.id = ga4_id\n",
    "    print(f\"{clinic} ({ga4_id}) \", end='')\n",
    "\n",
    "    # Get users\n",
    "    mg.report.run(\n",
    "        d=[(\"yearMonth\", \"month\"), (\"landingPage\", \"page\")],\n",
    "        m=[(\"activeUsers\", \"users\")],\n",
    "        filter_d=\"sessionDefaultChannelGroup==Organic Search\"\n",
    "    )\n",
    "    df_users = mg.report.data\n",
    "\n",
    "    # Get CVs\n",
    "    mg.report.run(\n",
    "        d=[(\"yearMonth\", \"month\"), (\"landingPage\", \"page\")],\n",
    "        m=[(\"totalPurchasers\", \"cv\")],\n",
    "        filter_d=\"defaultChannelGroup==Organic Search\"\n",
    "    )\n",
    "    df_cv = mg.report.data if mg.report.data is not None else pd.DataFrame(columns=[\"month\", \"page\", \"cv\"])\n",
    "\n",
    "    # Merge & label\n",
    "    df = pd.merge(df_users, df_cv, on=[\"month\", \"page\"], how=\"outer\").fillna(0)\n",
    "    df['clinic'] = clinic\n",
    "    df['users'] = df['users'].astype(int)\n",
    "    df['cv'] = df['cv'].astype(int)\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "df_lp = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df_lp = classify_page(df_lp, page_map, new_col='page_category')\n",
    "\n",
    "# mg.show.table(df_lp, rows=10)\n",
    "\n",
    "# é›†è¨ˆã—ã¦_lp_catã‚·ãƒ¼ãƒˆã«ä¿å­˜\n",
    "\n",
    "landing_summary_df = (\n",
    "    df_lp\n",
    "    .groupby(['month', 'clinic', 'page_category'], as_index=False)\n",
    "    .agg(\n",
    "        users=('users', 'sum'),\n",
    "        cv=('cv', 'sum')\n",
    "    )\n",
    "    .sort_values(['month', 'clinic', 'users'], ascending=[True, True, False])\n",
    ")\n",
    "\n",
    "mg.sheets_service.upsert_df(\n",
    "    URL,\n",
    "    \"_lp_cat\",\n",
    "    landing_summary_df,\n",
    "    keys=[\"month\", \"page_category\"],\n",
    "    columns=[\"month\", \"clinic\", \"page_category\", \"users\", \"cv\"],\n",
    "    sort_by=[\"month\", \"page_category\"],\n",
    "    create_if_missing=True,\n",
    ")\n",
    "mg.show.table(landing_summary_df, rows=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "KFsEq_FZ5EGv"
   },
   "outputs": [],
   "source": [
    "#@title 3.GAã‹ã‚‰SNSæµå…¥ãƒ‡ãƒ¼ã‚¿ï¼ˆç›´æ¥ï¼‰ã‚’å¾—ã¦åˆ†é¡\n",
    "\n",
    "# date_from = \"2024-11-01\"\n",
    "# date_to = \"2025-10-31\"\n",
    "# print(f\"æœŸé–“ï¼š{date_from}ã€œ{date_to}\")\n",
    "# mg.report.set_dates(date_from, date_to)\n",
    "\n",
    "dfs = []\n",
    "selected_sites = [s for s in sites if s[\"clinic\"] in CLINIC_FILTER]\n",
    "# print(f\"ğŸ” å¯¾è±¡ã‚¯ãƒªãƒ‹ãƒƒã‚¯: {', '.join(s['clinic'] for s in selected_sites)}\")\n",
    "# --- å„ã‚¯ãƒªãƒ‹ãƒƒã‚¯ã®å‡¦ç† ---\n",
    "for site in selected_sites:\n",
    "    clinic_name = site['clinic']\n",
    "    if clinic_name == \"dentamap\":\n",
    "        continue  # dentamapã¯ã‚¹ã‚­ãƒƒãƒ—\n",
    "    print(f\"ğŸ”„ {clinic_name}...\", end='')\n",
    "    ga4_property_id = site['ga4_property_id']\n",
    "    mg.ga['4'].property.id = ga4_property_id\n",
    "\n",
    "    mg.report.run(\n",
    "        d=[\n",
    "            (\"yearMonth\", \"month\"),\n",
    "            (\"landingPage\", \"lp\"),\n",
    "            (\"sessionMedium\", \"medium\"),\n",
    "            (\"sessionSource\", \"source\"),\n",
    "            (\"sessionCampaignName\", \"campaign\"),\n",
    "            (\"sessionManualAdContent\", \"content\"),\n",
    "        ],\n",
    "        m=[(\"activeUsers\", \"users\"), \"sessions\", (\"totalPurchasers\", \"cv\")]\n",
    "    )\n",
    "    merged_df = mg.report.data\n",
    "    merged_df['clinic'] = clinic_name\n",
    "\n",
    "    # ğŸ”§ LP ã®å‰ã« site['url'] ã‚’ä»˜ä¸ï¼ˆã€Œ//ã€é˜²æ­¢ï¼‰\n",
    "    base_url = site['url'].rstrip('/')  # æœ«å°¾ã® / ã‚’å‰Šé™¤\n",
    "    merged_df['lp'] = base_url + merged_df['lp']\n",
    "\n",
    "    dfs.append(merged_df)\n",
    "\n",
    "# --- dentamap ç”¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ & CV ãƒ¬ãƒãƒ¼ãƒˆã‚’è¿½åŠ  ---\n",
    "clinic_name = \"dentamap\"\n",
    "print(f\"ğŸ”„ {clinic_name}\")\n",
    "site = [s for s in sites if s[\"clinic\"] in [clinic_name]][0]\n",
    "mg.ga['4'].property.id = site[\"ga4_property_id\"]\n",
    "mg.report.run(\n",
    "    d=[\n",
    "        (\"yearMonth\", \"month\"),\n",
    "        (\"landingPagePlusQueryString\", \"lp\"),\n",
    "        (\"sessionMedium\", \"medium\"),\n",
    "        (\"sessionSource\", \"source\"),\n",
    "        (\"sessionCampaignName\", \"campaign\"),\n",
    "        (\"sessionManualAdContent\", \"content\"),\n",
    "    ],\n",
    "    filter_d=\"landingPagePlusQueryString=@/apl/netuser/?id=\",\n",
    "    m=[\n",
    "        (\"activeUsers\", \"users\"),\n",
    "        \"sessions\",\n",
    "        (\"keyEvents\", \"cv\")\n",
    "    ]\n",
    ")\n",
    "if mg.report.data is not None:\n",
    "    df_dentamap = mg.report.data.copy()\n",
    "    def infer_clinic_from_lp(lp_val):\n",
    "        for site in sites:\n",
    "            if \"dentamap_id\" in site and f\"id={site['dentamap_id']}\" in lp_val:\n",
    "                return site[\"clinic\"]\n",
    "        # clinic ã‚’ç‰¹å®šã§ããªã„å ´åˆã¯ã€id=XXXX ã®æ•°å­—éƒ¨åˆ†ã‚’ãã®ã¾ã¾ä½¿ã†\n",
    "        match = re.search(r\"id=(\\d+)\", lp_val)\n",
    "        return match.group(1) if match else \"ä¸æ˜\"\n",
    "\n",
    "    df_dentamap['clinic'] = df_dentamap['lp'].apply(infer_clinic_from_lp)\n",
    "    df_dentamap['lp'] = df_dentamap['lp'].str.extract(r'(/apl/netuser/\\?id=\\d+)')\n",
    "    df_dentamap['lp'] = \"https://plus.dentamap.jp\" + df_dentamap['lp']\n",
    "\n",
    "    # paramåˆ†æ•£ã‚’ã¾ã¨ã‚ã‚‹\n",
    "    df_dentamap = (\n",
    "        df_dentamap\n",
    "        .groupby([\"month\", \"lp\", \"medium\", \"source\", \"campaign\", \"content\", \"clinic\"], as_index=False)\n",
    "        .agg({\n",
    "            \"users\": \"sum\",\n",
    "            \"sessions\": \"sum\",\n",
    "            \"cv\": \"sum\"\n",
    "        })\n",
    "    )\n",
    "    # df_dentamap['medium'] = \"dentamap\"\n",
    "    # KEY_COLS ã«å¿…è¦ãªåˆ—ãŒè¶³ã‚Šãªã„å ´åˆã¯è£œå®Œ\n",
    "    # for col in KEY_COLS:\n",
    "    #     if col not in df_dentamap.columns:\n",
    "    #         df_dentamap[col] = None\n",
    "    # dfs.append(df_dentamap[KEY_COLS])\n",
    "\n",
    "# å…±é€šã®å‰å‡¦ç†\n",
    "df_all = pd.concat(\n",
    "    [df for df in dfs if not df.empty] + [df_dentamap],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# campaign æ­£è¦åŒ–\n",
    "df_all[\"campaign\"] = df_all[\"campaign\"].str.replace(r\"\\([^)]*\\)\", \"\", regex=True)\n",
    "\n",
    "# content æ­£è¦åŒ–\n",
    "df_all[\"content\"] = df_all[\"content\"].str.replace(r\"\\([^)]*\\)\", \"\", regex=True)\n",
    "\n",
    "# source æ­£è¦åŒ–\n",
    "# df_all = apply_normalization(df_all, 'source', source_map)\n",
    "\n",
    "# channel å†åˆ†é¡\n",
    "# df_all['channel'] = df_all.apply(classify_channel, axis=1)\n",
    "\n",
    "# æ•°å€¤å¤‰æ›ï¼ˆæ—¢å­˜å‡¦ç†ï¼‰\n",
    "df_all['cv'] = pd.to_numeric(df_all['cv'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "col_order = [\"month\", \"clinic\", \"medium\", \"source\", \"campaign\", \"content\", \"lp\", \"users\", \"cv\"]\n",
    "\n",
    "df_filtered = df_all[\n",
    "    (df_all[\"medium\"] == \"social\") |\n",
    "    (df_all[\"source\"].str.contains(r'\\b(tiktok|instagram|youtube|t\\.co)\\b', case=False, na=False))\n",
    "][col_order]\n",
    "\n",
    "# Google Sheetsä¿å­˜\n",
    "sheet_url = \"https://docs.google.com/spreadsheets/d/1nUoS71GW2ej570FYp8RHZvLwFv5CWwGgV8i2BzuslL0\"\n",
    "sheet_name = \"_ga\"\n",
    "mg.sheets_service.upsert_df(\n",
    "    sheet_url,\n",
    "    sheet_name,\n",
    "    df_filtered,\n",
    "    keys=[\"month\", \"clinic\", \"lp\", \"source\", \"medium\", \"campaign\", \"content\"],\n",
    "    columns=[\"month\", \"clinic\", \"medium\", \"source\", \"campaign\", \"content\", \"lp\", \"users\", \"cv\"],\n",
    "    sort_by=[\"month\", \"clinic\", \"medium\", \"source\", \"campaign\", \"content\", \"lp\", \"users\", \"cv\"],\n",
    "    create_if_missing=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "4fWi5c0iKLSR"
   },
   "outputs": [],
   "source": [
    "#@title Gåºƒå‘Šãƒ»SNSã‹ã‚‰kyousei-clinic.jp / dentamapã¸ã®æµå…¥\n",
    "\n",
    "table_from = date_from.replace('-', '')\n",
    "table_from = \"20251101\"\n",
    "table_to = date_to.replace('-', '')\n",
    "print(f\"æœŸé–“ï¼š{table_from}ã€œ{table_to}\")\n",
    "\n",
    "df_h = bq.run(query=f\"\"\"--Colab\n",
    "WITH base AS (\n",
    "  SELECT\n",
    "    FORMAT_DATE('%Y%m', PARSE_DATE('%Y%m%d', event_date)) AS month,\n",
    "    user_pseudo_id,\n",
    "    (SELECT value.int_value FROM UNNEST(event_params) WHERE key = 'ga_session_id') AS ga_session_id,\n",
    "    (SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'medium') AS medium,\n",
    "    (SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'source') AS source,\n",
    "    event_name,\n",
    "    event_timestamp,\n",
    "\n",
    "    -- session_start ã®ã¿ page_location ã‚’æŠ½å‡ºã—ã¦lpã‚’è¨­å®š\n",
    "    CASE\n",
    "      WHEN event_name = 'session_start' THEN (\n",
    "        SELECT\n",
    "          COALESCE(\n",
    "            REGEXP_EXTRACT(value.string_value, r'^[^#?]*\\\\?id=\\\\d+'),\n",
    "            REGEXP_REPLACE(value.string_value, r'\\\\?.*', '')\n",
    "          )\n",
    "        FROM UNNEST(event_params)\n",
    "        WHERE key = 'page_location'\n",
    "      )\n",
    "    END AS lp,\n",
    "\n",
    "    CASE WHEN event_name LIKE 'dentamap_submit_%' THEN 1 ELSE 0 END AS cv\n",
    "  FROM `shibuya-kyousei.analytics_492311970.events_*`\n",
    "  WHERE\n",
    "    _TABLE_SUFFIX BETWEEN '{table_from}' AND '{table_to}'\n",
    "    AND (event_name IN ('session_start', 'page_view') OR event_name LIKE 'dentamap_submit_%')\n",
    "),\n",
    "-- ã‚»ãƒƒã‚·ãƒ§ãƒ³å†…ã§åŸ‹ã‚ã‚‹\n",
    "enriched AS (\n",
    "  SELECT\n",
    "    b.*,\n",
    "    LAST_VALUE(lp IGNORE NULLS) OVER (\n",
    "      PARTITION BY user_pseudo_id, ga_session_id\n",
    "      ORDER BY event_timestamp\n",
    "      ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n",
    "    ) AS session_lp,\n",
    "    -- medium, source ã¯æœ€åˆã®éNULLã‚’ä½¿ã†\n",
    "    LAST_VALUE(medium IGNORE NULLS) OVER (\n",
    "      PARTITION BY user_pseudo_id, ga_session_id\n",
    "      ORDER BY event_timestamp\n",
    "      ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n",
    "    ) AS session_medium,\n",
    "    LAST_VALUE(source IGNORE NULLS) OVER (\n",
    "      PARTITION BY user_pseudo_id, ga_session_id\n",
    "      ORDER BY event_timestamp\n",
    "      ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n",
    "    ) AS session_source\n",
    "  FROM base AS b\n",
    "),\n",
    "-- usersï¼ˆæ¯æ•°ï¼‰ã¨CVãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ãã‚Œãã‚ŒæŠ½å‡º\n",
    "users AS (\n",
    "  SELECT DISTINCT\n",
    "    month,\n",
    "    user_pseudo_id,\n",
    "    session_medium AS medium,\n",
    "    session_source AS source,\n",
    "    session_lp AS lp\n",
    "  FROM enriched\n",
    "  WHERE event_name = 'session_start'\n",
    "),\n",
    "cv_users AS (\n",
    "  SELECT DISTINCT\n",
    "    month,\n",
    "    user_pseudo_id,\n",
    "    session_medium AS medium,\n",
    "    session_source AS source,\n",
    "    session_lp AS lp\n",
    "  FROM enriched\n",
    "  WHERE cv = 1\n",
    ")\n",
    "\n",
    "-- æœˆÃ—åª’ä½“Ã—ã‚½ãƒ¼ã‚¹Ã—LPå˜ä½ã§é›†è¨ˆ\n",
    "SELECT\n",
    "  u.month,\n",
    "  u.medium,\n",
    "  u.source,\n",
    "  u.lp,\n",
    "  COUNT(DISTINCT u.user_pseudo_id) AS users,\n",
    "  COUNT(DISTINCT c.user_pseudo_id) AS cv\n",
    "FROM users AS u\n",
    "LEFT JOIN cv_users AS c\n",
    "  ON u.month = c.month\n",
    " AND u.user_pseudo_id = c.user_pseudo_id\n",
    " AND u.lp = c.lp\n",
    "GROUP BY u.month, u.medium, u.source, u.lp\n",
    "ORDER BY u.month, u.medium, u.source;\n",
    "\"\"\", to_dataframe=True)\n",
    "df_h['clinic'] = df_h['lp'].apply(infer_clinic_from_lp)\n",
    "\n",
    "# åŠ å·¥å‰ã® df_h ã¯ãã®ã¾ã¾ä¿æŒ\n",
    "df_proc = df_h.copy()\n",
    "\n",
    "# channelåˆ—ã‚’è¿½åŠ \n",
    "df_proc[\"channel\"] = np.select(\n",
    "    [\n",
    "        df_proc[\"lp\"].str.contains(\"plus.dentamap.jp\", na=False),\n",
    "        df_proc[\"lp\"].str.contains(\".kyousei-clinic.jp\", na=False),\n",
    "    ],\n",
    "    [\n",
    "        \"dentamap\",\n",
    "        \"kyousei-clinic.jp\",\n",
    "    ],\n",
    "    default=\"(not set)\"\n",
    ")\n",
    "\n",
    "# source æ­£è¦åŒ–\n",
    "df_proc = apply_normalization(df_proc, \"source\", source_map)\n",
    "\n",
    "# selected_sites ã® domain ã«ä¸€è‡´ã™ã‚‹ source ã‚’é™¤å¤–\n",
    "exclude_domains = [s[\"domain\"] for s in selected_sites if \"domain\" in s]\n",
    "pattern = \"|\".join(map(re.escape, exclude_domains))\n",
    "df_proc = df_proc[~df_proc[\"source\"].str.contains(pattern, na=False)]\n",
    "\n",
    "# clinic=\"ä¸æ˜\" ã‚‚é™¤å¤–\n",
    "df_proc = df_proc[df_proc[\"clinic\"] != \"ä¸æ˜\"]\n",
    "\n",
    "# åˆ—é †ã‚’æ•´ç†\n",
    "col_order = [\"month\", \"clinic\", \"channel\", \"medium\", \"source\", \"lp\", \"users\", \"cv\"]\n",
    "df_proc = df_proc[[c for c in col_order if c in df_proc.columns]]\n",
    "\n",
    "# groupby & sumï¼ˆæœ€å¾Œã®2åˆ—ã¯æ•´æ•°å‹ï¼‰\n",
    "group_keys = col_order[:-2]\n",
    "metric_cols = col_order[-2:]\n",
    "\n",
    "df_proc = (\n",
    "    df_proc.groupby(group_keys, dropna=False)[metric_cols]\n",
    "    .sum(min_count=1)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# intå‹ã‚’ä¿æŒ\n",
    "for col in metric_cols:\n",
    "    df_proc[col] = df_proc[col].fillna(0).astype(int)\n",
    "\n",
    "# Google Sheetsä¿å­˜\n",
    "sheet_name = \"_ch2-m\"\n",
    "mg.sheets_service.upsert_df(\n",
    "    URL,\n",
    "    sheet_name,\n",
    "    df_proc,\n",
    "    keys=group_keys,\n",
    "    columns=col_order,\n",
    "    sort_by=col_order,\n",
    "    create_if_missing=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "bTAkh0PMSM-T"
   },
   "outputs": [],
   "source": [
    "#@title Search Consoleãƒ¬ãƒãƒ¼ãƒˆæ›´æ–° â†’_device, _query, _query_cat\n",
    "\n",
    "\n",
    "def classify_page(df, mapping, new_col='page_category'):\n",
    "    if df.empty or not mapping:\n",
    "        return df\n",
    "\n",
    "    def map_page(url):\n",
    "        for pattern, label in mapping.items():\n",
    "            try:\n",
    "                if re.search(pattern, url):\n",
    "                    return label\n",
    "            except re.error as e:\n",
    "                print(f\"âš ï¸ Invalid pattern '{pattern}': {e}\")\n",
    "        return 'other'\n",
    "\n",
    "    df[new_col] = df['page'].apply(map_page)\n",
    "    return df\n",
    "\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch all Search Console rows using paging, retry, and safe parsing.\n",
    "    Optionally apply cleaning and aggregation logic at the end.\n",
    "\n",
    "    Args:\n",
    "        start_date (str): YYYY-MM-DD\n",
    "        end_date (str): YYYY-MM-DD\n",
    "        site_url (str): e.g., 'https://example.com'\n",
    "        dimensions (list): e.g., ['query', 'page'], ['page'], ['query', 'page', 'device']\n",
    "        country (str or None): e.g., 'jpn', or None to disable filtering\n",
    "        row_limit (int): Max rows per request (up to 25,000)\n",
    "        verbose (bool): Print logs per page (default: False)\n",
    "        max_rows (int): Safety limit to avoid runaway loops\n",
    "        max_retries (int): Retries per failed call\n",
    "        backoff_factor (float): Sleep multiplier for retries\n",
    "        clean (bool):\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined result with keys and metrics\n",
    "    \"\"\"\n",
    "    creds = service_account.Credentials.from_service_account_file(CREDS_PATH)\n",
    "    creds = creds.with_scopes(['https://www.googleapis.com/auth/webmasters.readonly'])\n",
    "    webmasters = build('searchconsole', 'v1', credentials=creds)\n",
    "\n",
    "    all_rows = []\n",
    "    start_row = 0\n",
    "    total_rows_fetched = 0\n",
    "\n",
    "    while True:\n",
    "        request_body = {\n",
    "            'startDate': start_date,\n",
    "            'endDate': end_date,\n",
    "            'dimensions': dimensions,\n",
    "            'rowLimit': row_limit,\n",
    "            'startRow': start_row\n",
    "        }\n",
    "\n",
    "        if country:\n",
    "            request_body['dimensionFilterGroups'] = [{\n",
    "                'filters': [{\n",
    "                    'dimension': 'country',\n",
    "                    'operator': 'equals',\n",
    "                    'expression': country\n",
    "                }]\n",
    "            }]\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = webmasters.searchanalytics().query(siteUrl=site_url, body=request_body).execute()\n",
    "                rows = response.get('rows', [])\n",
    "                break  # Success\n",
    "            except HttpError as e:\n",
    "                wait = backoff_factor * (2 ** attempt)\n",
    "                if verbose:\n",
    "                    print(f\"âš ï¸ API error on startRow={start_row}, retrying in {wait:.1f}s... ({attempt + 1}/{max_retries})\\n{e}\")\n",
    "                time.sleep(wait)\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Fatal error on startRow={start_row}: {e}\")\n",
    "                return pd.DataFrame()\n",
    "        else:\n",
    "            print(f\"âŒ Max retries exceeded at startRow={start_row}\")\n",
    "            break\n",
    "\n",
    "        if not rows:\n",
    "            break\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"ğŸ“„ {site_url} - Fetched {len(rows)} rows (startRow={start_row})\")\n",
    "\n",
    "        all_rows.extend(rows)\n",
    "        total_rows_fetched += len(rows)\n",
    "        start_row += len(rows)\n",
    "\n",
    "        if len(rows) < row_limit:\n",
    "            break  # No more pages\n",
    "\n",
    "        if total_rows_fetched >= max_rows:\n",
    "            print(f\"âš ï¸ Reached max_rows limit ({max_rows}), stopping.\")\n",
    "            break\n",
    "\n",
    "    if not all_rows:\n",
    "        if verbose:\n",
    "            print(\"âš ï¸ No rows collected after paging.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    data = []\n",
    "    for i, r in enumerate(all_rows):\n",
    "        keys = r.get(\"keys\", [])\n",
    "        if len(keys) < len(dimensions):\n",
    "            if verbose:\n",
    "                print(f\"âš ï¸ Skipping row {i} due to missing keys: {keys}\")\n",
    "            continue\n",
    "        try:\n",
    "            row_data = {dim: keys[idx] for idx, dim in enumerate(dimensions)}\n",
    "            row_data.update({\n",
    "                'clicks': r.get('clicks', 0),\n",
    "                'impressions': r.get('impressions', 0),\n",
    "                'position': r.get('position', 0.0)\n",
    "            })\n",
    "            data.append(row_data)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error in row {i}: {e}\")\n",
    "\n",
    "    if not data:\n",
    "        if verbose:\n",
    "            print(\"âš ï¸ No usable rows after parsing.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    if verbose:\n",
    "        print(f\"âœ… Total rows returned: {len(df):,}\")\n",
    "\n",
    "    # Clean if requested\n",
    "    if clean:\n",
    "        df['month'] = pd.to_datetime(start_date).strftime('%Y%m')\n",
    "        return aggregate_search_console_data(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "):\n",
    "    \"\"\"\n",
    "    Generic GSC data fetcher.\n",
    "    - Aggregates per specified dimensions (e.g., ['device'], ['query', 'page'])\n",
    "    - Adds 'clinic' and 'month' if specified\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    selected_sites = [s for s in sites if not clinic_filter or s[\"clinic\"] in clinic_filter]\n",
    "\n",
    "    for site in selected_sites:\n",
    "        clinic = site['clinic']\n",
    "        site_url = site.get('url', '').strip('/')\n",
    "        print(f\"ğŸ” Fetching GSC data: {clinic} ({site_url}) [{', '.join(dimensions)}]\")\n",
    "\n",
    "        df = query_search_console(\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            site_url=site_url,\n",
    "            dimensions=dimensions,\n",
    "            country=country,\n",
    "            clean=True\n",
    "        )\n",
    "\n",
    "        if not df.empty:\n",
    "            if include_clinic:\n",
    "                df['clinic'] = clinic\n",
    "            if include_month:\n",
    "                df['month'] = pd.to_datetime(start_date).strftime(\"%Y%m\")\n",
    "            dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        print(\"âš ï¸ No GSC data retrieved.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    if select_columns:\n",
    "        df_all = df_all[[col for col in select_columns if col in df_all.columns]]\n",
    "\n",
    "    return df_all\n",
    "\n",
    "def apply_query_mapping(df, mapping):\n",
    "    \"\"\"\n",
    "    Apply regex-based rule mapping to the 'query' column in place.\n",
    "    \"\"\"\n",
    "    if df.empty or not mapping:\n",
    "        return df\n",
    "\n",
    "    def map_query(q):\n",
    "        for pattern, mapped in mapping.items():\n",
    "            try:\n",
    "                if re.search(pattern, q):\n",
    "                    return mapped\n",
    "            except re.error as e:\n",
    "                print(f\"âš ï¸ Invalid regex pattern skipped: {pattern} â†’ {e}\")\n",
    "        return q\n",
    "\n",
    "    df['query'] = df['query'].astype(str).apply(map_query)\n",
    "    return df\n",
    "\n",
    "def deduplicate_queries(df):\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # Create a key by removing all whitespace\n",
    "    df['query_key'] = df['query'].str.replace(r\"\\s+\", \"\", regex=True)\n",
    "\n",
    "    # Sort to prefer highest-impression query in each group\n",
    "    df_sorted = df.sort_values(by=['month', 'clinic', 'page', 'query_key', 'impressions'], ascending=[True, True, True, True, False])\n",
    "\n",
    "    # Pick top query (with max impressions) per group\n",
    "    top_queries = df_sorted.groupby(['month', 'clinic', 'page', 'query_key'], as_index=False).first()\n",
    "\n",
    "    # Aggregate metrics per group\n",
    "    agg_metrics = df.groupby(['month', 'clinic', 'page', 'query_key']).agg(\n",
    "        impressions=('impressions', 'sum'),\n",
    "        clicks=('clicks', 'sum'),\n",
    "        weighted_pos=('position', lambda x: (x * df.loc[x.index, 'impressions']).sum())\n",
    "    ).reset_index()\n",
    "\n",
    "    # Merge to get best-query string back\n",
    "    result = pd.merge(agg_metrics, top_queries[['month', 'clinic', 'page', 'query_key', 'query']], on=['month', 'clinic', 'page', 'query_key'], how='left')\n",
    "\n",
    "    # Finalize\n",
    "    result['position'] = (result['weighted_pos'] / result['impressions']).round(6)\n",
    "    result = result.drop(columns=['query_key', 'weighted_pos'])\n",
    "\n",
    "    # Reorder columns\n",
    "    return result[['month', 'clinic', 'query', 'page', 'impressions', 'clicks', 'position']]\n",
    "\n",
    "def filter_by_clinic_thresholds(df, thresholds_df):\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    df_filtered = pd.DataFrame()\n",
    "\n",
    "    for _, row in thresholds_df.iterrows():\n",
    "        clinic = row['clinic']\n",
    "        min_imp = row['min_impressions']\n",
    "        max_pos = row['max_position']\n",
    "\n",
    "        df_sub = df[df['clinic'] == clinic]\n",
    "\n",
    "        df_sub = df_sub[\n",
    "            ~((df_sub['clicks'] == 0) & ((df_sub['impressions'] < min_imp) | (df_sub['position'] > max_pos)))\n",
    "        ]\n",
    "\n",
    "        df_filtered = pd.concat([df_filtered, df_sub], ignore_index=True)\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "def _normalize_gsc_keys(df):\n",
    "    df = df.copy()\n",
    "    df['month'] = df['month'].astype(str).str.strip().str.replace(r'\\.0$', '', regex=True)\n",
    "    df['clinic'] = df['clinic'].astype(str).str.strip()\n",
    "    df['query'] = df['query'].astype(str).str.strip().str.lstrip(\"'\")  # ğŸ§¼ remove saved quote\n",
    "    df['page'] = df['page'].astype(str).str.strip().str.lower()\n",
    "\n",
    "    def fix_query(val):\n",
    "        if isinstance(val, float) and val.is_integer():\n",
    "            return str(int(val))\n",
    "        return str(val)\n",
    "    df['query'] = df['query'].apply(fix_query)\n",
    "\n",
    "    return df\n",
    "\n",
    "def _force_text_on_numeric_queries(df):\n",
    "    \"\"\"\n",
    "    Prevent Google Sheets from converting numeric queries (e.g. phone numbers)\n",
    "    by prepending a single quote.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['query'] = df['query'].apply(\n",
    "        lambda x: f\"'{x}\" if re.fullmatch(r\"\\d+\", str(x)) else x\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# Pageã¨Queryã®ãƒ‡ãƒ¼ã‚¿å–å¾—\n",
    "\n",
    "# TARGET_MONTHS_AGO = 0 # é€šå¸¸ã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ\n",
    "\n",
    "date_from = get_past_date(n_months=TARGET_MONTHS_AGO)\n",
    "date_to = (get_past_date(n_months=TARGET_MONTHS_AGO - 1, return_date_obj=True) - timedelta(days=1)).isoformat()\n",
    "print(f\"æœŸé–“ï¼š{date_from}ã€œ{date_to}\")\n",
    "\n",
    "CLINIC_FILTER = [\n",
    "    \"æœ­å¹Œ\",\n",
    "    \"ä»™å°\",\n",
    "    \"æ± è¢‹\",\n",
    "    \"æ–°å®¿\",\n",
    "    \"æ¸‹è°·\",\n",
    "    \"æ±äº¬\",\n",
    "    \"æ¨ªæµœ\",\n",
    "    \"æ¢…ç”°\",\n",
    "    \"é›£æ³¢\",\n",
    "    \"åšå¤š\",\n",
    "    \"å¤©ç¥\",\n",
    "    # \"Gr.\",\n",
    "    # \"dentamap\",\n",
    "]\n",
    "\n",
    "# 1. Get Data, clean & decode pages, group by query + page\n",
    "gsc_df = mg.gsc_service.fetch_sites(\n",
    "    sites=sites,\n",
    "    clinic_filter=CLINIC_FILTER,\n",
    "    start_date=date_from,\n",
    "    end_date=date_to,\n",
    "    dimensions=['query', 'page'],\n",
    "    country='jpn',\n",
    "    include_clinic=True,\n",
    "    include_month=True,\n",
    "    clean=True,\n",
    "    aggregate=True,\n",
    ")\n",
    "\n",
    "# 2. Apply regex-based query mapping\n",
    "gsc_df = apply_query_mapping(gsc_df, query_map)\n",
    "\n",
    "# 3. Group query variations by clinic + page + query_key (e.g., \"abc\", \"a bc\")\n",
    "gsc_df = deduplicate_queries(gsc_df)\n",
    "\n",
    "# 4. åˆ†é¡ã¨é›†è¨ˆ\n",
    "gsc_df2 = classify_page(gsc_df, page_map)\n",
    "summary_df = (\n",
    "    gsc_df2\n",
    "    .groupby(['month', 'clinic', 'page_category'], as_index=False)\n",
    "    .agg(\n",
    "        impressions=('impressions', 'sum'),\n",
    "        clicks=('clicks', 'sum')\n",
    "    )\n",
    "    .sort_values(['month', 'clinic', 'impressions'], ascending=[True, True, False])\n",
    ")\n",
    "\n",
    "# 5. å·®åˆ†ã®ã¿_query_catã‚·ãƒ¼ãƒˆã¸è¿½è¨˜\n",
    "mg.sheets_service.upsert_df(\n",
    "    URL,\n",
    "    \"_query_cat\",\n",
    "    summary_df,\n",
    "    keys=[\"month\", \"clinic\", \"page_category\"],\n",
    "    columns=[\"month\", \"clinic\", \"page_category\", \"impressions\", \"clicks\"],\n",
    "    sort_by=[\"month\", \"clinic\", \"page_category\"],\n",
    "    create_if_missing=True,\n",
    ")\n",
    "\n",
    "if TARGET_MONTHS_AGO == 1:\n",
    "    # Filter low-value rows at the very end\n",
    "    gsc_df_filtered = filter_by_clinic_thresholds(gsc_df, df_config)\n",
    "\n",
    "    # Save\n",
    "    gsc_df_filtered = _normalize_gsc_keys(gsc_df_filtered)\n",
    "    gsc_df_filtered = _force_text_on_numeric_queries(gsc_df_filtered)\n",
    "    mg.sheets_service.upsert_df(\n",
    "        URL,\n",
    "        \"_query\",\n",
    "        gsc_df_filtered,\n",
    "        keys=[\"month\", \"clinic\", \"query\", \"page\"],\n",
    "        columns=[\"month\", \"clinic\", \"query\", \"page\", \"impressions\", \"clicks\", \"position\"],\n",
    "        sort_by=[\"month\", \"clinic\", \"query\", \"page\"],\n",
    "        create_if_missing=True,\n",
    "    )\n",
    "\n",
    "# ã‚µã‚¤ãƒˆå…¨ä½“ã®åˆè¨ˆæŒ‡æ¨™ã‚’æœˆåˆ¥ã«å¾—ã‚‹\n",
    "\n",
    "# 1. APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—\n",
    "df = mg.gsc_service.fetch_sites(\n",
    "    sites=sites,\n",
    "    clinic_filter=CLINIC_FILTER,\n",
    "    start_date=date_from,\n",
    "    end_date=date_to,\n",
    "    dimensions=['device'],\n",
    "    country='jpn',\n",
    "    include_clinic=True,\n",
    "    include_month=True,\n",
    "    clean=True,\n",
    "    aggregate=True,\n",
    ")\n",
    "# 2. å·®åˆ†ã®ã¿ã‚·ãƒ¼ãƒˆã¸è¿½è¨˜\n",
    "if df.empty:\n",
    "    print(\"âš ï¸ No data to save to _device.\")\n",
    "else:\n",
    "    print(f\"ğŸ“Š {len(df):,} rows prepared for saving to _device.\")\n",
    "    mg.sheets_service.upsert_df(\n",
    "        URL,\n",
    "        \"_device\",\n",
    "        df,\n",
    "        keys=[\"month\", \"clinic\", \"device\"],\n",
    "        columns=[\"month\", \"clinic\", \"device\", \"impressions\", \"clicks\", \"position\"],\n",
    "        sort_by=[\"month\", \"clinic\", \"device\"],\n",
    "        create_if_missing=True,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "rFEaDbdXmg_a",
    "TVXoERKY8uD7"
   ],
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}