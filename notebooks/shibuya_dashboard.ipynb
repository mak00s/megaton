{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CcIlkVtvMfhk",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title CV Dashboard â€” Cell 1 (Auth & Read)\n",
    "\n",
    "# ===== Settings =====\n",
    "# èª­ã¿å–ã‚Šå…ƒï¼ˆå…ƒãƒ‡ãƒ¼ã‚¿ï¼‰\n",
    "READ_GS_URL = \"https://docs.google.com/spreadsheets/d/1wz20DITKF1GpIryhifiPShEbSgIVwSqlD22WEL6dpCY\"  # å¸¸ã«æœ€æ–°ã®æœˆæ¬¡ãƒ¬ãƒãƒ¼ãƒˆ\n",
    "READ_SHEET_NAME = \"_ch-m\"\n",
    "\n",
    "# æ›¸ãå‡ºã—å…ˆï¼ˆåˆ¥ãƒ–ãƒƒã‚¯ï¼‰\n",
    "WRITE_GS_URL = \"https://docs.google.com/spreadsheets/d/1zw8BEwj6pxe-n7PKyC80GFr0EzIHqS3XOpuBVMVeF-4/\"  # @param {type:\"string\"}\n",
    "WRITE_SHEET_NAME = \"_summary-cv\" # @param {type:\"string\"}\n",
    "\n",
    "# ã‚¯ãƒªãƒ‹ãƒƒã‚¯ï¼ˆæ—¥æœ¬èªâ†’è‹±èªï¼‰12é™¢\n",
    "CLINIC_FILTER = [\n",
    "    (\"æ± è¢‹\", \"Ikebukuro\"),\n",
    "    (\"åšå¤š\", \"Hakata\"),\n",
    "    (\"ä»™å°\", \"Sendai\"),\n",
    "    (\"æ±äº¬\", \"Tokyo\"),\n",
    "    (\"æ¢…ç”°\", \"Umeda\"),\n",
    "    (\"æ–°å®¿\", \"Shinjuku\"),\n",
    "    (\"æ¨ªæµœ\", \"Yokohama\"),\n",
    "    (\"æ¸‹è°·\", \"Shibuya\"),\n",
    "    (\"é›£æ³¢\", \"Namba\"),\n",
    "    (\"æœ­å¹Œ\", \"Sapporo\"),\n",
    "    (\"å¤©ç¥\", \"Tenjin\"),\n",
    "    (\"åå¤å±‹\", \"Nagoya\"),\n",
    "]\n",
    "\n",
    "HALF_LIFE_MONTHS = 3   # EWMA half-life (months)\n",
    "SAVE_TO_SHEET = True  # True ã«ã™ã‚‹ã¨ WRITE_* ã«æ›¸ãå‡ºã—\n",
    "\n",
    "try:\n",
    "    from google.colab import data_table, files as colab_files\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    data_table = None\n",
    "    colab_files = None\n",
    "    IN_COLAB = False\n",
    "\n",
    "try:\n",
    "    from megaton import start\n",
    "except ModuleNotFoundError:\n",
    "    if IN_COLAB:\n",
    "        %pip install -U -q git+https://github.com/mak00s/megaton\n",
    "        from megaton import start\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# ===== Install & Imports =====\n",
    "import sys, subprocess, importlib.util\n",
    "def _pip_install(pkgs):\n",
    "    try:\n",
    "        missing = []\n",
    "        for p in pkgs:\n",
    "            mod = p.split(\"[\")[0].replace(\"-\", \"_\")\n",
    "            if importlib.util.find_spec(mod) is None:\n",
    "                missing.append(p)\n",
    "        if missing:\n",
    "            print(\"Installing:\", missing)\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *missing])\n",
    "    except Exception as e:\n",
    "        print(\"âš ï¸ pip install failed:\", e)\n",
    "\n",
    "_pip_install([\"gspread\", \"gspread-dataframe\", \"pandas\", \"numpy\", \"matplotlib\"])\n",
    "\n",
    "import pandas as pd\n",
    "import gspread\n",
    "from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
    "\n",
    "# ===== Google Auth & Read =====\n",
    "print(\"== Step 1/5: Google Auth ==\")\n",
    "try:\n",
    "    # Colab OAuth\n",
    "    try:\n",
    "        from google.colab import auth as colab_auth  # type: ignore\n",
    "        colab_auth.authenticate_user()\n",
    "        print(\"âœ… Colab OAuth OK\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Default creds -> gspread\n",
    "    try:\n",
    "        import google.auth  # type: ignore\n",
    "        from google.auth.transport.requests import Request  # type: ignore\n",
    "        scopes = [\n",
    "            \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "            \"https://www.googleapis.com/auth/drive\",\n",
    "        ]\n",
    "        creds, _ = google.auth.default(scopes=scopes)\n",
    "        if hasattr(creds, \"refresh\") and hasattr(creds, \"valid\") and not creds.valid:\n",
    "            creds.refresh(Request())\n",
    "        gc = gspread.authorize(creds)\n",
    "        print(\"âœ… google.auth.default() OK\")\n",
    "    except Exception:\n",
    "        # Fallbacks\n",
    "        try:\n",
    "            gc = gspread.oauth()\n",
    "            print(\"âœ… gspread.oauth() OK\")\n",
    "        except Exception:\n",
    "            from google.oauth2.service_account import Credentials  # type: ignore\n",
    "            import os\n",
    "            sa_path = os.getenv(\"SERVICE_ACCOUNT_JSON\")\n",
    "            if not sa_path:\n",
    "                raise RuntimeError(\"Auth failed. Use Colab OAuth or set SERVICE_ACCOUNT_JSON.\")\n",
    "            sa_scopes = [\n",
    "                \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "                \"https://www.googleapis.com/auth/drive\",\n",
    "            ]\n",
    "            creds = Credentials.from_service_account_file(sa_path, scopes=sa_scopes)\n",
    "            gc = gspread.authorize(creds)\n",
    "            print(\"âœ… Service Account OK\")\n",
    "\n",
    "    print(\"== Step 2/5: Read Sheet ==\")\n",
    "    if not READ_GS_URL or \"http\" not in READ_GS_URL:\n",
    "        raise ValueError(\"Set READ_GS_URL to your Sheet URL.\")\n",
    "    sh_read = gc.open_by_url(READ_GS_URL)\n",
    "    ws_read = sh_read.worksheet(READ_SHEET_NAME)\n",
    "\n",
    "    # header=0ï¼ˆ1è¡Œç›®ã«ãƒ˜ãƒƒãƒ€ãƒ¼ï¼‰\n",
    "    df_raw = get_as_dataframe(ws_read, evaluate_formulas=True, header=0)\n",
    "    df_raw.columns = [str(c).strip() for c in df_raw.columns]\n",
    "    print(f\"âœ… Read OK: {READ_SHEET_NAME} rows={len(df_raw)}\")\n",
    "    print(\"ğŸ§ª columns after read:\", list(df_raw.columns))\n",
    "\n",
    "    # === (UPDATE) _counsï¼ˆæ¥åº—ã‚«ã‚¦ãƒ³ã‚»ãƒªãƒ³ã‚°ä»¶æ•°ï¼‰ ===\n",
    "    READ_SHEET_NAME_COUNS = \"_couns\"\n",
    "    try:\n",
    "        ws_couns = sh_read.worksheet(READ_SHEET_NAME_COUNS)\n",
    "        # 1-basedã§2è¡Œç›®ãŒè‹±èªãƒ˜ãƒƒãƒ€ãƒ¼ â†’ header=1 (0-based)\n",
    "        df_couns = get_as_dataframe(ws_couns, evaluate_formulas=True, header=1)\n",
    "        df_couns.columns = [str(c).strip() for c in df_couns.columns]\n",
    "        # æœ€å°ã‚¹ã‚­ãƒ¼ãƒã«æ­£è¦åŒ–ï¼ˆæ—¥æœ¬èªè¦‹å‡ºã—ã ã£ãŸå ´åˆã‚‚ã‚±ã‚¢ï¼‰\n",
    "        alias = {\"å¹´æœˆ\":\"month\", \"ã‚¯ãƒªãƒ‹ãƒƒã‚¯\":\"clinic\", \"æ¥é™¢æ•°\":\"cv\"}\n",
    "        df_couns = df_couns.rename(columns={k:v for k,v in alias.items() if k in df_couns.columns})\n",
    "        # ä¸è¦ãªå®Œå…¨NaNè¡Œå‰Šé™¤\n",
    "        df_couns = df_couns.dropna(how=\"all\")\n",
    "        print(f\"âœ… Read OK: {READ_SHEET_NAME_COUNS} rows={len(df_couns)}\")\n",
    "        print(\"ğŸ§ª _couns columns:\", list(df_couns.columns)[:6])\n",
    "    except gspread.WorksheetNotFound:\n",
    "        df_couns = None\n",
    "        print(\"âš ï¸ _couns ã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆdf_couns=Noneï¼‰ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚\")\n",
    "\n",
    "except Exception as e:\n",
    "    raise SystemExit(f\"âŒ Auth/Read error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title CV Dashboard â€” Cell 2 (Prep, Metrics, Microcharts)\n",
    "\n",
    "print(\"== Step 3/5: Preprocessing / Validation ==\")\n",
    "\n",
    "# ASCII only labels in charts\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"font.family\"] = \"DejaVu Sans\"\n",
    "matplotlib.rcParams[\"axes.unicode_minus\"] = True\n",
    "\n",
    "# Guard\n",
    "import pandas as pd, numpy as np\n",
    "if 'df_raw' not in globals() or df_raw is None:\n",
    "    raise SystemExit(\"âŒ df_raw is None. Make sure Cell 1 completed successfully.\")\n",
    "\n",
    "# ã‚¯ãƒªãƒ‹ãƒƒã‚¯ã®æ—¥æœ¬èªâ†’è‹±èªãƒãƒƒãƒ—\n",
    "CLINIC_JA_TO_EN = {ja: en for ja, en in CLINIC_FILTER}\n",
    "CLINIC_ALLOWED_JA = [ja for ja, en in CLINIC_FILTER]\n",
    "\n",
    "# å‰å‡¦ç†\n",
    "_df = df_raw.copy()\n",
    "_df = _df.dropna(how=\"all\")\n",
    "\n",
    "# å¿…é ˆåˆ—\n",
    "required_cols = [\"month\", \"clinic\", \"channel\", \"medium\", \"source\", \"users\", \"cv\"]\n",
    "missing = [c for c in required_cols if c not in _df.columns]\n",
    "if missing:\n",
    "    raise SystemExit(\"âŒ Required columns missing: \" + str(missing))\n",
    "\n",
    "# â˜… å½“æœˆãƒ‡ãƒ¼ã‚¿ã‚’é™¤å¤–ï¼ˆä¸å®Œå…¨æœˆï¼‰\n",
    "#   month ã¯ 202509 ã®ã‚ˆã†ãªYYYYMMæƒ³å®šã€‚ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã¯æ—¥æœ¬æ™‚é–“ãƒ™ãƒ¼ã‚¹ã§OKã€‚\n",
    "current_ym = pd.Timestamp.now(tz=\"Asia/Tokyo\").strftime(\"%Y%m\")\n",
    "_df = _df[_df[\"month\"].astype(str) != current_ym].copy()\n",
    "\n",
    "# å‹å¤‰æ›\n",
    "_df[\"month\"] = pd.to_datetime(_df[\"month\"].astype(str), format=\"%Y%m\", errors=\"coerce\")\n",
    "if _df[\"month\"].isna().any():\n",
    "    bad = _df[_df[\"month\"].isna()].head(5)\n",
    "    raise SystemExit(f\"âŒ Failed to parse 'month' as YYYYMM. Top bad rows:\\n{bad}\")\n",
    "\n",
    "num_cols = [c for c in [\"users\", \"cv\", \"ad_cost\"] if c in _df.columns]\n",
    "for c in num_cols:\n",
    "    _df[c] = pd.to_numeric(_df[c], errors=\"coerce\")\n",
    "if num_cols:\n",
    "    _df[num_cols] = _df[num_cols].fillna(0)\n",
    "\n",
    "# â˜… ãƒãƒ£ãƒãƒ«æ¡ä»¶ï¼šDirect ã¯é™¤å¤–ã€ãã®ä»–ã¯å…¨ã¦å«ã‚ã‚‹ï¼ˆï¼Organic é™å®šã§ã¯ãªã„ï¼‰\n",
    "_df = _df[_df[\"channel\"] != \"Direct\"].copy()\n",
    "\n",
    "# â˜… ã‚¯ãƒªãƒ‹ãƒƒã‚¯ã¯12é™¢ï¼ˆå­˜åœ¨ã™ã‚‹ã‚‚ã®ã®ã¿ï¼‰\n",
    "_df = _df[_df[\"clinic\"].isin(CLINIC_ALLOWED_JA)].copy()\n",
    "if _df.empty:\n",
    "    raise SystemExit(\"âš ï¸ No rows after clinic filter. Check CLINIC_FILTER and data.\")\n",
    "\n",
    "print(\"âœ… Preprocess OK: rows=\", len(_df))\n",
    "\n",
    "# é›†è¨ˆï¼ˆé™¢Ã—æœˆï¼›å…¨ãƒãƒ£ãƒãƒ«åˆç®— â€»Directé™¤å¤–æ¸ˆã¿ï¼‰\n",
    "df_m = (\n",
    "    _df.groupby([\"clinic\", \"month\"], as_index=False)[[\"users\", \"cv\"]]\n",
    "      .sum()\n",
    "      .sort_values([\"clinic\", \"month\"])\n",
    ")\n",
    "\n",
    "# æŒ‡æ¨™å®šç¾©\n",
    "print(\"== Step 4/5: Metrics (TTM / YoY_3M / EWMA) ==\")\n",
    "def add_metrics(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    g = group.sort_values(\"month\").copy()\n",
    "    g[\"cvr\"] = np.where(g[\"users\"] > 0, g[\"cv\"] / g[\"users\"], np.nan)\n",
    "    g[\"cv_ttm12\"]   = g[\"cv\"].rolling(window=12, min_periods=1).sum()                # TTM\n",
    "    g[\"cv_3m\"]      = g[\"cv\"].rolling(window=3,  min_periods=1).sum()\n",
    "    g[\"cv_3m_lastyr\"]= g[\"cv_3m\"].shift(12)\n",
    "    g[\"yoy_3m\"]     = np.where(g[\"cv_3m_lastyr\"] > 0, (g[\"cv_3m\"]/g[\"cv_3m_lastyr\"]) - 1, np.nan)  # å­£ç¯€èª¿æ•´ã®è¿‘ä¼¼\n",
    "    g[\"cv_ewma\"]    = g[\"cv\"].ewm(halflife=HALF_LIFE_MONTHS, adjust=False).mean()    # EWMA\n",
    "    return g\n",
    "\n",
    "# groupby.applyï¼ˆå°†æ¥äº’æ›å¯¾å¿œï¼‰\n",
    "try:\n",
    "    df_metrics = (\n",
    "        df_m.sort_values([\"clinic\",\"month\"])\n",
    "            .groupby(\"clinic\", group_keys=True)\n",
    "            .apply(add_metrics, include_groups=False)\n",
    "            .reset_index(level=0)\n",
    "    )\n",
    "except TypeError:\n",
    "    df_metrics = (\n",
    "        df_m.sort_values([\"clinic\",\"month\"])\n",
    "            .groupby(\"clinic\", group_keys=True)\n",
    "            .apply(add_metrics)\n",
    "            .reset_index(level=0)\n",
    "    )\n",
    "\n",
    "latest_per_clinic = (\n",
    "    df_metrics.sort_values([\"clinic\",\"month\"])\n",
    "              .groupby(\"clinic\", as_index=False)\n",
    "              .tail(1)\n",
    "              .loc[:, [\"clinic\",\"month\",\"cv\",\"users\",\"cvr\",\"cv_ttm12\",\"cv_3m\",\"yoy_3m\",\"cv_ewma\"]]\n",
    ")\n",
    "\n",
    "print(\"âœ… Metrics computed: rows=\", len(df_metrics))\n",
    "\n",
    "# ä¸¦ã³é †ï¼ˆå¥½èª¿â†’ä¸èª¿ï¼‰\n",
    "# health_score = 0.6 * æœ€æ–°yoy_3m + 0.4 * (æœ€è¿‘6ãƒ¶æœˆã®EWMAå‚¾ã / ç›´è¿‘12ãƒ¶æœˆå¹³å‡CV)\n",
    "def _health_scores(dfm: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for clinic, g in dfm.groupby(\"clinic\"):\n",
    "        g = g.sort_values(\"month\")\n",
    "        yoy = g[\"yoy_3m\"].dropna().iloc[-1] if g[\"yoy_3m\"].notna().any() else np.nan\n",
    "        g2 = g.tail(6)\n",
    "        if len(g2) >= 3 and g2[\"cv_ewma\"].notna().sum() >= 3:\n",
    "            x = np.arange(len(g2)); y = g2[\"cv_ewma\"].to_numpy()\n",
    "            slope = np.polyfit(x, y, 1)[0]\n",
    "        else:\n",
    "            slope = np.nan\n",
    "        base = g.tail(12)[\"cv\"].mean()\n",
    "        slope_norm = slope / max(1.0, base) if pd.notna(slope) else np.nan\n",
    "        score = (0.6*yoy if pd.notna(yoy) else 0) + (0.4*slope_norm if pd.notna(slope_norm) else 0)\n",
    "        rows.append({\"clinic\": clinic, \"score\": score})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "scores = _health_scores(df_metrics)\n",
    "present = [ja for ja in CLINIC_ALLOWED_JA if ja in df_metrics[\"clinic\"].unique()]\n",
    "order = scores.sort_values(\"score\", ascending=False)[\"clinic\"].tolist()\n",
    "order = [c for c in order if c in present]  # 12é™¢ã«é™å®š\n",
    "\n",
    "# è¡¨ç¤ºç”¨ã«æ—¥æœ¬èªã®ä¸¦ã³ã‚’å‡ºåŠ›\n",
    "print(\"ğŸ“ ä¸¦ã³é †ï¼ˆå¥½èª¿â†’ä¸èª¿ï¼‰:\", order)\n",
    "\n",
    "# === Microcharts ===\n",
    "import matplotlib.pyplot as plt\n",
    "def english_label(clinic: str, idx: int) -> str:\n",
    "    return CLINIC_JA_TO_EN.get(clinic, f\"C{idx+1}\")\n",
    "\n",
    "def small_multiples(df_metrics, clinics_order, metric=\"cv_ttm12\", cols=12, normalize=False,\n",
    "                    width=1.6, height=1.2, fontsize=7):\n",
    "    n = len(clinics_order)\n",
    "    cols = max(1, min(cols, n))\n",
    "    rows = int(np.ceil(n / cols))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols*width, rows*height), squeeze=False)\n",
    "    axes_flat = axes.ravel()\n",
    "    for i, clinic in enumerate(clinics_order):\n",
    "        ax = axes_flat[i]\n",
    "        g = df_metrics[df_metrics[\"clinic\"] == clinic].sort_values(\"month\")\n",
    "        y = g[metric].copy()\n",
    "        if normalize:\n",
    "            base = y.dropna().iloc[0] if not y.dropna().empty else np.nan\n",
    "            if pd.notna(base) and base != 0: y = (y / base) * 100.0\n",
    "        ax.plot(g[\"month\"], y, linewidth=1)\n",
    "        if y.dropna().shape[0] > 0:\n",
    "            ax.plot(g[\"month\"].iloc[-1], y.dropna().iloc[-1], marker=\"o\", markersize=2)\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        for s in ax.spines.values(): s.set_visible(False)\n",
    "        ax.text(0.02, 0.80, english_label(clinic, i), transform=ax.transAxes,\n",
    "                fontsize=fontsize, fontweight=\"bold\")\n",
    "    for j in range(i+1, rows*cols): fig.delaxes(axes_flat[j])\n",
    "    title = f\"Microcharts â€” {metric}\" + (\" (indexed=100)\" if normalize else \"\")\n",
    "    fig.suptitle(title, y=0.98, fontsize=12)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# â–¼ãƒãƒ£ãƒ¼ãƒˆå‰ã«æ—¥æœ¬èªã®èª¬æ˜ã‚’å€‹åˆ¥è¡¨ç¤ºï¼ˆæŒ‡å®šæ–‡è¨€ï¼‰\n",
    "print(\"=== ç§»å‹•åˆè¨ˆï¼ˆTTM=Trailing 12-Monthï¼‰ ===\")\n",
    "print(\"ã€€ç›´è¿‘12ãƒ¶æœˆã®CVåˆè¨ˆã€‚å­£ç¯€æ€§ã‚’å¹³å‡åŒ–ï¼ˆæœˆãƒ–ãƒ¬ã‚’å¸åï¼‰ã—ã€ä¸­é•·æœŸã®å®ŸåŠ›ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’æŠŠæ¡\")\n",
    "print(\"ã€€å³è‚©ä¸ŠãŒã‚Šï¼åœ°åˆã„æ”¹å–„ã€‚æ¨ªã°ã„ï¼åœæ»ã€‚å³ç«¯ã®ç‚¹ãŒæœ€æ–°å€¤ã€‚\")\n",
    "small_multiples(df_metrics, order, metric=\"cv_ttm12\", cols=12, normalize=False)\n",
    "\n",
    "print(\"=== EWMAï¼ˆæŒ‡æ•°ç§»å‹•å¹³å‡ï¼‰ ===\")\n",
    "print(f\"ã€€ç›´è¿‘ã®é‡ã¿ã‚’å¤§ããã™ã‚‹å¹³å‡ï¼ˆåŠæ¸›æœŸ{HALF_LIFE_MONTHS}ãƒ¶æœˆï¼‰ã€‚ç·©ã‚„ã‹ãªå¤‰åŒ–ã‚’æ—©ã‚ã«æ‰ãˆã‚‹ãŒå¶ç™ºã‚¹ãƒ‘ã‚¤ã‚¯ã¯æŠ‘åˆ¶ã€‚\")\n",
    "print(\"ã€€å‚¾ãã§å‹¢ã„ã‚’è©•ä¾¡ã€‚TTMãŒä¸Šå‘ãï¼†EWMAã‚‚ä¸Šå‘ãï¼æ”¹å–„ã®ç¢ºåº¦ãŒé«˜ã„ã€‚\")\n",
    "small_multiples(df_metrics, order, metric=\"cv_ewma\", cols=12, normalize=False)\n",
    "\n",
    "print(\"=== 3ãƒ¶æœˆç§»å‹•ä¸­å¤®å€¤ï¼ˆãƒ­ãƒ¼ãƒªãƒ³ã‚°YoYï¼‰ ===\")\n",
    "print(\"ã€€3ãƒ¶æœˆç§»å‹•åˆè¨ˆã®å‰å¹´åŒæœŸæ¯”ã€‚å­£ç¯€è¦å› ã‚’åˆã‚ã›ãŸä¼¸ã³ç‡ã€‚\")\n",
    "print(\"ã€€0è¶…ï¼å‰å¹´ã‚ˆã‚Šæ”¹å–„ã€0æœªæº€ï¼æ‚ªåŒ–ã€‚å¤–ã‚Œå€¤ã«ã¯æ³¨æ„ã—ã€æŒç¶šæ€§ã‚’é‡è¦–ã€‚\")\n",
    "small_multiples(df_metrics, order, metric=\"yoy_3m\", cols=12, normalize=False)\n",
    "\n",
    "print(\"=== Indexï¼Ÿ ===\")\n",
    "print(\"ã€€å„ã‚¯ãƒªãƒ‹ãƒƒã‚¯ã®åˆæœŸå€¤=100ã§æ­£è¦åŒ–ã—ãŸæœˆæ¬¡CVã€‚ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã¨æœ€è¿‘ã®ä¼¸ã³ã‚’ç›´æ„Ÿçš„ã«æŠŠæ¡\")\n",
    "print(\"ã€€ä¸Šå‘ãï¼æ”¹å–„ãƒšãƒ¼ã‚¹åŠ é€Ÿã€ä¸Šä¸‹å‹•ãŒå¤§ãã„ï¼æœˆæ¬¡ãƒ–ãƒ¬ãŒå¤§ãã„ã€‚\")\n",
    "small_multiples(df_metrics, order, metric=\"cv\", cols=12, normalize=True)\n",
    "\n",
    "# print(\"== Step 5/5: Writeback ==\")\n",
    "# if SAVE_TO_SHEET:\n",
    "#     try:\n",
    "#         sh_write = gc.open_by_url(WRITE_GS_URL)\n",
    "#         try:\n",
    "#             ws_sum = sh_write.worksheet(WRITE_SHEET_NAME)\n",
    "#             ws_sum.clear()\n",
    "#         except gspread.WorksheetNotFound:\n",
    "#             ws_sum = sh_write.add_worksheet(title=WRITE_SHEET_NAME, rows=2000, cols=50)\n",
    "#         # ã‚¯ãƒªãƒ‹ãƒƒã‚¯æœ€æ–°è¡Œï¼‹health_scoreã‚’çµåˆã—ã¦æ›¸ãå‡ºã—\n",
    "#         out = latest_per_clinic.merge(scores, on=\"clinic\", how=\"left\")\n",
    "#         set_with_dataframe(ws_sum, out)\n",
    "#         print(f\"âœ… Wrote summary to {WRITE_SHEET_NAME} @ write book\")\n",
    "#     except Exception as e:\n",
    "#         print(\"âš ï¸ Writeback failed:\", e)\n",
    "# else:\n",
    "#     print(\"ï¼ˆæ›¸ãå‡ºã—ã¯ SAVE_TO_SHEET=False ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼‰\")\n",
    "\n",
    "# === 13ãƒ¶æœˆåˆ†ã®æŒ‡æ¨™ç”Ÿæˆï¼ˆEWMAå‚¾ããƒ»health_scoreãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆå«ã‚€ï¼‰ ===\n",
    "def _slope_np(arr):\n",
    "    arr = np.asarray(arr, dtype=float)\n",
    "    mask = np.isfinite(arr)\n",
    "    if mask.sum() < 3:\n",
    "        return np.nan\n",
    "    x = np.arange(mask.sum())\n",
    "    y = arr[mask]\n",
    "    return np.polyfit(x, y, 1)[0]\n",
    "\n",
    "df_metrics = df_metrics.sort_values([\"clinic\",\"month\"]).copy()\n",
    "\n",
    "# EWMAå‚¾ãï¼ˆç›´è¿‘6ãƒ¶æœˆã®ã‚¹ãƒ­ãƒ¼ãƒ—ï¼›æ¯æœˆè¨ˆç®—ï¼‰\n",
    "df_metrics[\"ewma_slope\"] = (\n",
    "    df_metrics.groupby(\"clinic\")[\"cv_ewma\"]\n",
    "              .apply(lambda s: s.rolling(window=6, min_periods=3).apply(_slope_np, raw=True))\n",
    "              .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "# æ­£è¦åŒ–ç”¨ã®ç›´è¿‘12ãƒ¶æœˆå¹³å‡CV\n",
    "df_metrics[\"cv_mean_12\"] = (\n",
    "    df_metrics.groupby(\"clinic\")[\"cv\"]\n",
    "              .transform(lambda s: s.rolling(window=12, min_periods=1).mean())\n",
    ")\n",
    "\n",
    "# å‚¾ãã®è¦æ¨¡æ­£è¦åŒ–\n",
    "df_metrics[\"ewma_slope_norm\"] = df_metrics[\"ewma_slope\"] / df_metrics[\"cv_mean_12\"].clip(lower=1.0)\n",
    "\n",
    "# health_scoreï¼ˆNaNã¯0ã§ä»£æ›¿ï¼šç„¡æƒ…å ±ï¼ä¸­ç«‹ï¼‰\n",
    "df_metrics[\"health_score\"] = 0.6*df_metrics[\"yoy_3m\"].fillna(0) + 0.4*df_metrics[\"ewma_slope_norm\"].fillna(0)\n",
    "\n",
    "# ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶\n",
    "df_metrics[\"alert_yoy2\"] = (\n",
    "    df_metrics.groupby(\"clinic\")[\"yoy_3m\"]\n",
    "              .transform(lambda s: s.rolling(window=2, min_periods=2)\n",
    "                                   .apply(lambda a: 1.0 if np.all(a < -0.10) else 0.0, raw=True))\n",
    ").fillna(0).astype(int)\n",
    "\n",
    "df_metrics[\"alert_slope3\"] = (\n",
    "    df_metrics.groupby(\"clinic\")[\"ewma_slope\"]\n",
    "              .transform(lambda s: s.rolling(window=3, min_periods=3)\n",
    "                                   .apply(lambda a: 1.0 if np.all(a < 0) else 0.0, raw=True))\n",
    ").fillna(0).astype(int)\n",
    "\n",
    "df_metrics[\"alert\"] = ((df_metrics[\"alert_yoy2\"]==1) | (df_metrics[\"alert_slope3\"]==1)).astype(int)\n",
    "\n",
    "# ç›´è¿‘13ãƒ¶æœˆï¼ˆå½“æœˆã¯æ—¢ã«é™¤å¤–æ¸ˆã¿ï¼‰\n",
    "max_m = df_metrics[\"month\"].max()\n",
    "start_m = (max_m - pd.DateOffset(months=12)).replace(day=1)\n",
    "df_13 = df_metrics[(df_metrics[\"month\"] >= start_m) & (df_metrics[\"clinic\"].isin(CLINIC_ALLOWED_JA))].copy()\n",
    "\n",
    "# ä¿å­˜ç”¨ï¼šæ‰±ã„ã‚„ã™ã„ãƒ¯ã‚¤ãƒ‰è¡¨\n",
    "df_13[\"ym\"] = df_13[\"month\"].dt.strftime(\"%Y%m\").astype(int)  # ä¾‹: 202508\n",
    "_summary_out = df_13[[\n",
    "    \"clinic\",\"ym\",\"month\",\"cv\",\"users\",\"cvr\",\n",
    "    \"cv_ttm12\",\"yoy_3m\",\"ewma_slope\",\"ewma_slope_norm\",\"health_score\",\n",
    "    \"alert\",\"alert_yoy2\",\"alert_slope3\"\n",
    "]].copy()\n",
    "\n",
    "# === _summary ã¸ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆï¼ˆé‡è¤‡ã‚­ãƒ¼: clinicÃ—ym ã‚’ä¸Šæ›¸ãï¼‰ ===\n",
    "if SAVE_TO_SHEET:\n",
    "    print(\"== Step 5/5: Writeback to _summary ==\")\n",
    "    try:\n",
    "        sh_write = gc.open_by_url(WRITE_GS_URL)\n",
    "        try:\n",
    "            ws_sum = sh_write.worksheet(\"_summary-cv\")\n",
    "            df_exist = get_as_dataframe(ws_sum, evaluate_formulas=True, header=0)\n",
    "            if df_exist is None or df_exist.empty:\n",
    "                df_exist = pd.DataFrame(columns=_summary_out.columns)\n",
    "            df_exist.columns = [str(c).strip() for c in df_exist.columns]\n",
    "        except gspread.WorksheetNotFound:\n",
    "            ws_sum = sh_write.add_worksheet(title=\"_summary\", rows=5000, cols=50)\n",
    "            df_exist = pd.DataFrame(columns=_summary_out.columns)\n",
    "\n",
    "        # ã‚­ãƒ¼æ•´å½¢\n",
    "        if \"ym\" in df_exist.columns:\n",
    "            df_exist[\"ym\"] = pd.to_numeric(df_exist[\"ym\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        else:\n",
    "            df_exist[\"ym\"] = pd.Series(dtype=\"Int64\")\n",
    "\n",
    "        # çµåˆâ†’é‡è¤‡ã‚­ãƒ¼ã‚’å¾Œå‹ã¡ï¼ˆæ–°ãƒ‡ãƒ¼ã‚¿å„ªå…ˆï¼‰ã§é™¤å»\n",
    "        merged = pd.concat([df_exist[_summary_out.columns], _summary_out], ignore_index=True)\n",
    "        merged[\"_key\"] = merged[\"clinic\"].astype(str) + \"_\" + merged[\"ym\"].astype(str)\n",
    "        merged = merged.sort_values([\"clinic\",\"ym\",\"month\"]).drop_duplicates(subset=[\"_key\"], keep=\"last\").drop(columns=[\"_key\"])\n",
    "\n",
    "        # ä¸¦ã³ï¼šæœ€æ–°é †ï¼ˆé™é †ï¼‰ã§è¦‹ã‚„ã™ã\n",
    "        merged = merged.sort_values([\"ym\",\"clinic\"], ascending=[False, True])\n",
    "\n",
    "        # æ›¸ãæˆ»ã—\n",
    "        ws_sum.clear()\n",
    "        set_with_dataframe(ws_sum, merged)\n",
    "        print(f\"âœ… Wrote {_summary_out.shape[0]} rows to _summary (upserted last 13 months)\")\n",
    "    except Exception as e:\n",
    "        print(\"âš ï¸ Writeback failed:\", e)\n",
    "else:\n",
    "    print(\"ï¼ˆæ›¸ãå‡ºã—ã¯ SAVE_TO_SHEET=False ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼‰\")\n",
    "\n",
    "# =========================\n",
    "# (FIX) _couns: æ¥åº—ã‚«ã‚¦ãƒ³ã‚»ãƒªãƒ³ã‚°ä»¶æ•°ï¼ˆè–„ã„ã‚¹ã‚­ãƒ¼ãƒ & ä½™è¨ˆãªåˆ—ã‚’ç„¡è¦–ï¼‰\n",
    "# =========================\n",
    "if 'df_couns' in globals() and df_couns is not None:\n",
    "    print(\"\\n== _counsï¼ˆæ¥åº—ã‚«ã‚¦ãƒ³ã‚»ãƒªãƒ³ã‚°ï¼‰å‡¦ç† ==\")\n",
    "    k = df_couns.copy().dropna(how=\"all\")\n",
    "\n",
    "    # å¿…é ˆåˆ—ã®ã¿ã‚’æ®‹ã™ï¼ˆD/Eåˆ—ãªã©ä½™è¨ˆãªåˆ—ã¯ã“ã“ã§ç„¡è¦–ï¼‰\n",
    "    alias = {\"å¹´æœˆ\":\"month\", \"ã‚¯ãƒªãƒ‹ãƒƒã‚¯\":\"clinic\", \"æ¥é™¢æ•°\":\"cv\"}\n",
    "    k = k.rename(columns={k0:v0 for k0,v0 in alias.items() if k0 in k.columns})\n",
    "    keep = [c for c in [\"month\",\"clinic\",\"cv\"] if c in k.columns]\n",
    "    k = k[keep].copy()\n",
    "\n",
    "    # å¿…é ˆåˆ—ãƒã‚§ãƒƒã‚¯\n",
    "    req = [\"month\",\"clinic\",\"cv\"]\n",
    "    miss = [c for c in req if c not in k.columns]\n",
    "    if miss:\n",
    "        raise SystemExit(\"âŒ _couns å¿…é ˆåˆ—ãŒä¸è¶³: \" + str(miss))\n",
    "\n",
    "    # å½“æœˆé™¤å¤–ï¼ˆä¸å®Œå…¨æœˆï¼‰\n",
    "    current_ym = pd.Timestamp.now(tz=\"Asia/Tokyo\").strftime(\"%Y%m\")\n",
    "    k = k[k[\"month\"].astype(str) != current_ym].copy()\n",
    "\n",
    "    # month ã‚’å …ç‰¢ã« YYYYMM ã¸ãƒ‘ãƒ¼ã‚¹ï¼ˆ202301, 202301.0, 2023-01, 2023/01 ç­‰ã‚’è¨±å®¹ï¼‰\n",
    "    import re, numpy as np, pandas as pd\n",
    "    def _parse_ym_series(s: pd.Series) -> pd.Series:\n",
    "        ss = s.astype(str).str.strip()\n",
    "        # æœ«å°¾ã® \".0\" ã‚’é™¤å» â†’ \"202301\"\n",
    "        ss = ss.str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "        # åŒºåˆ‡ã‚Šæ–‡å­—ã‚„æ—¥æœ¬èªè¡¨è¨˜ã‚’é™¤å» â†’ \"2023-01\"/\"2023/01\"/\"2023å¹´1æœˆ\" ç­‰ â†’ \"2023 01\"\n",
    "        ss = ss.str.replace(r\"[\\/\\-å¹´]\", \"\", regex=True).str.replace(\"æœˆ\", \"\", regex=True)\n",
    "        # å…ˆé ­ã‹ã‚‰6æ¡ã ã‘æŠœãï¼ˆYYYYMMï¼‰\n",
    "        ss = ss.str.extract(r\"(\\d{6})\", expand=False)\n",
    "        return pd.to_datetime(ss, format=\"%Y%m\", errors=\"coerce\")\n",
    "\n",
    "    k[\"month\"] = _parse_ym_series(k[\"month\"])\n",
    "    if k[\"month\"].isna().any():\n",
    "        bad = k[k[\"month\"].isna()].head(5)\n",
    "        raise SystemExit(f\"âŒ _couns: month å¤‰æ›å¤±æ•— ä¸Šä½ï¼ˆå€¤ã« '202301.0' ãªã©ãŒæ··åœ¨ã—ã¦ã„ãªã„ã‹ç¢ºèªï¼‰:\\n{bad}\")\n",
    "\n",
    "    # å€¤ã‚’æ•°å€¤åŒ–\n",
    "    k[\"cv\"] = pd.to_numeric(k[\"cv\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    # 12é™¢ã®ã¿\n",
    "    k = k[k[\"clinic\"].isin(CLINIC_ALLOWED_JA)].copy()\n",
    "    if k.empty:\n",
    "        raise SystemExit(\"âš ï¸ _couns: 12é™¢ãƒ•ã‚£ãƒ«ã‚¿å¾Œ0ä»¶ã€‚\")\n",
    "\n",
    "    # é™¢Ã—æœˆã«é›†è¨ˆï¼ˆé‡è¤‡ãŒã‚ã‚Œã°å’Œï¼‰\n",
    "    k_m = (\n",
    "        k.groupby([\"clinic\",\"month\"], as_index=False)[[\"cv\"]]\n",
    "         .sum()\n",
    "         .sort_values([\"clinic\",\"month\"])\n",
    "    ).rename(columns={\"cv\":\"couns\"})\n",
    "\n",
    "    # æŒ‡æ¨™\n",
    "    import numpy as np\n",
    "    def _add_metrics_couns(g):\n",
    "        g = g.sort_values(\"month\").copy()\n",
    "        g[\"couns_ttm12\"]     = g[\"couns\"].rolling(window=12, min_periods=1).sum()\n",
    "        g[\"couns_3m\"]        = g[\"couns\"].rolling(window=3,  min_periods=1).sum()\n",
    "        g[\"couns_3m_lastyr\"] = g[\"couns_3m\"].shift(12)\n",
    "        g[\"yoy_3m\"]          = np.where(g[\"couns_3m_lastyr\"]>0,\n",
    "                                        (g[\"couns_3m\"]/g[\"couns_3m_lastyr\"]) - 1, np.nan)\n",
    "        g[\"couns_ewma\"]      = g[\"couns\"].ewm(halflife=HALF_LIFE_MONTHS, adjust=False).mean()\n",
    "        return g\n",
    "\n",
    "    try:\n",
    "        m_c = (k_m.sort_values([\"clinic\",\"month\"])\n",
    "                 .groupby(\"clinic\", group_keys=True)\n",
    "                 .apply(_add_metrics_couns, include_groups=False)\n",
    "                 .reset_index(level=0))\n",
    "    except TypeError:\n",
    "        m_c = (k_m.sort_values([\"clinic\",\"month\"])\n",
    "                 .groupby(\"clinic\", group_keys=True)\n",
    "                 .apply(_add_metrics_couns)\n",
    "                 .reset_index(level=0))\n",
    "\n",
    "    # EWMAå‚¾ãï¼ˆ6ãƒ¶æœˆï¼‰ã¨æ­£è¦åŒ–\n",
    "    def _slope_np(arr):\n",
    "        arr = np.asarray(arr, dtype=float)\n",
    "        mask = np.isfinite(arr)\n",
    "        if mask.sum() < 3: return np.nan\n",
    "        x = np.arange(mask.sum()); y = arr[mask]\n",
    "        return np.polyfit(x, y, 1)[0]\n",
    "\n",
    "    m_c = m_c.sort_values([\"clinic\",\"month\"]).copy()\n",
    "    m_c[\"ewma_slope\"] = (\n",
    "        m_c.groupby(\"clinic\")[\"couns_ewma\"]\n",
    "           .apply(lambda s: s.rolling(window=6, min_periods=3).apply(_slope_np, raw=True))\n",
    "           .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    m_c[\"couns_mean_12\"]   = m_c.groupby(\"clinic\")[\"couns\"].transform(lambda s: s.rolling(12,1).mean())\n",
    "    m_c[\"ewma_slope_norm\"] = m_c[\"ewma_slope\"] / m_c[\"couns_mean_12\"].clip(lower=1.0)\n",
    "\n",
    "    # health_scoreï¼ˆCounsç‰ˆï¼‰\n",
    "    m_c[\"health_score\"] = 0.6*m_c[\"yoy_3m\"].fillna(0) + 0.4*m_c[\"ewma_slope_norm\"].fillna(0)\n",
    "\n",
    "    # ã‚¢ãƒ©ãƒ¼ãƒˆ\n",
    "    m_c[\"alert_yoy2\"] = (\n",
    "        m_c.groupby(\"clinic\")[\"yoy_3m\"]\n",
    "           .transform(lambda s: s.rolling(2,2).apply(lambda a: 1.0 if np.all(a < -0.10) else 0.0, raw=True))\n",
    "    ).fillna(0).astype(int)\n",
    "    m_c[\"alert_slope3\"] = (\n",
    "        m_c.groupby(\"clinic\")[\"ewma_slope\"]\n",
    "           .transform(lambda s: s.rolling(3,3).apply(lambda a: 1.0 if np.all(a < 0) else 0.0, raw=True))\n",
    "    ).fillna(0).astype(int)\n",
    "    m_c[\"alert\"] = ((m_c[\"alert_yoy2\"]==1) | (m_c[\"alert_slope3\"]==1)).astype(int)\n",
    "\n",
    "    # ç›´è¿‘13ãƒ¶æœˆ\n",
    "    max_mk = m_c[\"month\"].max()\n",
    "    start_mk = (max_mk - pd.DateOffset(months=12)).replace(day=1)\n",
    "    z = m_c[(m_c[\"month\"] >= start_mk) & (m_c[\"clinic\"].isin(CLINIC_ALLOWED_JA))].copy()\n",
    "    z[\"ym\"] = z[\"month\"].dt.strftime(\"%Y%m\").astype(int)\n",
    "\n",
    "    _summary_couns = z[[\n",
    "        \"clinic\",\"ym\",\"month\",\n",
    "        \"couns\",\n",
    "        \"couns_ttm12\",\"yoy_3m\",\"ewma_slope\",\"ewma_slope_norm\",\"health_score\",\n",
    "        \"alert\",\"alert_yoy2\",\"alert_slope3\"\n",
    "    ]].copy()\n",
    "\n",
    "    # æ›¸ãå‡ºã—ï¼ˆupsert: clinicÃ—ymï¼‰\n",
    "    if SAVE_TO_SHEET:\n",
    "        print(\"== Writeback to _summary_couns ==\")\n",
    "        try:\n",
    "            sh_write = gc.open_by_url(WRITE_GS_URL)\n",
    "            try:\n",
    "                ws_sumk = sh_write.worksheet(\"_summary_couns\")\n",
    "                exist = get_as_dataframe(ws_sumk, evaluate_formulas=True, header=0)\n",
    "                if exist is None or exist.empty:\n",
    "                    exist = pd.DataFrame(columns=_summary_couns.columns)\n",
    "                exist.columns = [str(c).strip() for c in exist.columns]\n",
    "            except gspread.WorksheetNotFound:\n",
    "                ws_sumk = sh_write.add_worksheet(title=\"_summary_couns\", rows=6000, cols=50)\n",
    "                exist = pd.DataFrame(columns=_summary_couns.columns)\n",
    "\n",
    "            if \"ym\" in exist.columns:\n",
    "                exist[\"ym\"] = pd.to_numeric(exist[\"ym\"], errors=\"coerce\").astype(\"Int64\")\n",
    "            else:\n",
    "                exist[\"ym\"] = pd.Series(dtype=\"Int64\")\n",
    "\n",
    "            merged_k = pd.concat([exist[_summary_couns.columns], _summary_couns], ignore_index=True)\n",
    "            merged_k[\"_key\"] = merged_k[\"clinic\"].astype(str) + \"_\" + merged_k[\"ym\"].astype(str)\n",
    "            merged_k = (merged_k\n",
    "                        .sort_values([\"clinic\",\"ym\",\"month\"])\n",
    "                        .drop_duplicates(subset=[\"_key\"], keep=\"last\")\n",
    "                        .drop(columns=[\"_key\"])\n",
    "                        .sort_values([\"ym\",\"clinic\"], ascending=[False, True]))\n",
    "\n",
    "            ws_sumk.clear()\n",
    "            set_with_dataframe(ws_sumk, merged_k)\n",
    "            print(f\"âœ… Wrote {_summary_couns.shape[0]} rows to _summary_couns (upserted last 13 months)\")\n",
    "        except Exception as e:\n",
    "            print(\"âš ï¸ Writeback(_summary_couns) failed:\", e)\n",
    "    else:\n",
    "        print(\"ï¼ˆ_summary_couns æ›¸ãå‡ºã—ã¯ SAVE_TO_SHEET=False ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼‰\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ df_couns ãŒç„¡ã„ãŸã‚ _couns ã®å‡¦ç†ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚\")\n",
    "\n"
   ],
   "metadata": {
    "cellView": "form",
    "id": "WHalUE5R-J0j"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title CV Dashboard â€” Cell 3 (Channel Mix & Mix-Adjusted CVR â†’ _channel_summary)\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "import gspread\n",
    "from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
    "\n",
    "print(\"== Cell 3: Channel mix & Mix-Adjusted CVR ==\")\n",
    "\n",
    "# å‰æ: Cell 2 ã§ä½œæˆæ¸ˆã¿ã® _dfï¼ˆå½“æœˆé™¤å¤–ãƒ»Directé™¤å¤–ãƒ»12é™¢ã®ã¿ï¼‰ã‚’ä½¿ç”¨\n",
    "if \"_df\" not in globals() or _df is None or _df.empty:\n",
    "    raise SystemExit(\"âŒ _df ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆCell 2 ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼‰\")\n",
    "\n",
    "# --- ä¸»è¦ãƒãƒ£ãƒãƒ«ã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆå¿…è¦ã«å¿œã˜ã¦èª¿æ•´ï¼‰ ---\n",
    "CHANNEL_MAP = {\n",
    "    \"Paid Search\": \"Ad\",\n",
    "    \"Cross-network\": \"Ad\",\n",
    "    \"Map\": \"Map\",                # Google Maps / Local / GBP çµŒç”±ã‚’æƒ³å®š\n",
    "    \"Organic Search\": \"Organic\",\n",
    "    \"Referral\": \"Referral\",\n",
    "    \"Display\": \"Display\",\n",
    "    \"Social\": \"Social\",\n",
    "    # ãã‚Œä»¥å¤–ã¯ Other ã«è½ã¨ã™\n",
    "}\n",
    "\n",
    "def map_channel(ch):\n",
    "    return CHANNEL_MAP.get(str(ch), \"Other\")\n",
    "\n",
    "df_ch = _df.copy()\n",
    "df_ch[\"ch_group\"] = df_ch[\"channel\"].map(map_channel)\n",
    "\n",
    "# --- æœˆæ¬¡ï¼ˆé™¢Ã—æœˆÃ—ãƒãƒ£ãƒãƒ«ï¼‰é›†è¨ˆ ---\n",
    "ch_month = (\n",
    "    df_ch.groupby([\"clinic\",\"month\",\"ch_group\"], as_index=False)[[\"users\",\"cv\"]]\n",
    "         .sum()\n",
    "         .sort_values([\"clinic\",\"ch_group\",\"month\"])\n",
    ")\n",
    "ch_month[\"cvr\"] = np.where(ch_month[\"users\"]>0, ch_month[\"cv\"]/ch_month[\"users\"], np.nan)\n",
    "\n",
    "# === åŸºæº–æœŸã®è‡ªå‹•é¸å®š & å†é‡ã¿ä»˜ã‘ (Mix-Adjusted CVR) ===\n",
    "# ãƒ«ãƒ¼ãƒ«ï¼š\n",
    "#  1) åŒæœˆå‰å¹´ï¼ˆYoYï¼‰ãŒååˆ†ãªãƒ‡ãƒ¼ã‚¿ï¼ˆusersåˆè¨ˆ>=50ï¼‰ãªã‚‰ãã‚Œã‚’ä½¿ã†  â†’ baseline_type=\"YoY\"\n",
    "#  2) ã ã‚ãªã‚‰ç›´è¿‘12ãƒ¶æœˆï¼ˆm-12ã€œm-1ï¼‰ã®åŠ é‡å¹³å‡ï¼ˆusersã§é‡ã¿ï¼‰   â†’ \"Roll12\"\n",
    "#  3) ãã‚Œã‚‚ç„¡ç†ãªã‚‰ m ã‚ˆã‚Šå‰ã®å…¨æœŸé–“ã®åŠ é‡å¹³å‡                   â†’ \"AllPast\"\n",
    "MIN_BASE_USERS = 50   # YoYåŸºæº–ã«æ¡ç”¨ã™ã‚‹æœ€ä½ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°\n",
    "ROLL_MIN_MONTHS = 6   # Roll12ã‚’æ¡ç”¨ã™ã‚‹æœ€ä½æœˆæ•°\n",
    "\n",
    "def _weights_from_block(df_block):\n",
    "    \"\"\"usersã‚·ã‚§ã‚¢ã‚’é‡ã¿ï¼ˆwï¼‰ã¨ã—ã¦ç®—å‡º\"\"\"\n",
    "    u = df_block.groupby(\"ch_group\", as_index=False)[\"users\"].sum()\n",
    "    total = u[\"users\"].sum()\n",
    "    if total <= 0 or np.isnan(total):\n",
    "        return {}\n",
    "    u[\"w\"] = u[\"users\"] / total\n",
    "    return dict(zip(u[\"ch_group\"], u[\"w\"]))\n",
    "\n",
    "def _cvr_by_block(df_block):\n",
    "    \"\"\"ãƒ–ãƒ­ãƒƒã‚¯å…¨ä½“ã®å®ŸCVRï¼ˆ= Î£cv / Î£usersï¼‰\"\"\"\n",
    "    s_users = df_block[\"users\"].sum()\n",
    "    s_cv = df_block[\"cv\"].sum()\n",
    "    return (s_cv / s_users) if s_users > 0 else np.nan\n",
    "\n",
    "def _cvr_by_group(df_block):\n",
    "    \"\"\"ãƒãƒ£ãƒãƒ«åˆ¥CVRï¼ˆè¾æ›¸ï¼‰\"\"\"\n",
    "    gg = df_block.groupby(\"ch_group\", as_index=False)[[\"users\",\"cv\"]].sum()\n",
    "    gg[\"cvr\"] = np.where(gg[\"users\"]>0, gg[\"cv\"]/gg[\"users\"], np.nan)\n",
    "    return dict(zip(gg[\"ch_group\"], gg[\"cvr\"]))\n",
    "\n",
    "rows = []\n",
    "for clinic, dfc in ch_month.groupby(\"clinic\"):\n",
    "    dfc = dfc.sort_values(\"month\")\n",
    "    months = sorted(dfc[\"month\"].unique())\n",
    "    for m in months:\n",
    "        now_block = dfc[dfc[\"month\"]==m]\n",
    "        # 1) YoY baseline\n",
    "        m_yoy = (m - pd.DateOffset(years=1)).replace(day=1)\n",
    "        yoy_block = dfc[dfc[\"month\"]==m_yoy]\n",
    "        base_type = None\n",
    "        base_ref_start = None\n",
    "        base_ref_end = None\n",
    "\n",
    "        if not yoy_block.empty and yoy_block[\"users\"].sum() >= MIN_BASE_USERS:\n",
    "            base_block = yoy_block\n",
    "            base_type = \"YoY\"\n",
    "            base_ref_start = base_ref_end = m_yoy\n",
    "        else:\n",
    "            # 2) Rolling 12 months (m-12..m-1)\n",
    "            start = (m - pd.DateOffset(months=12)).replace(day=1)\n",
    "            roll_block = dfc[(dfc[\"month\"]>=start) & (dfc[\"month\"]<m)]\n",
    "            months_in_roll = roll_block[\"month\"].nunique()\n",
    "            if (months_in_roll >= ROLL_MIN_MONTHS) and (roll_block[\"users\"].sum() >= MIN_BASE_USERS):\n",
    "                base_block = roll_block\n",
    "                base_type = \"Roll12\"\n",
    "                base_ref_start, base_ref_end = start, (m - pd.DateOffset(months=1)).replace(day=1)\n",
    "            else:\n",
    "                # 3) All past\n",
    "                allpast_block = dfc[dfc[\"month\"]<m]\n",
    "                if not allpast_block.empty and allpast_block[\"users\"].sum() >= MIN_BASE_USERS:\n",
    "                    base_block = allpast_block\n",
    "                    base_type = \"AllPast\"\n",
    "                    base_ref_start, base_ref_end = allpast_block[\"month\"].min(), allpast_block[\"month\"].max()\n",
    "                else:\n",
    "                    # åŸºæº–ãŒä½œã‚Œãªã„ï¼ˆåˆæœŸæœˆãªã©ï¼‰ï¼šweights=ç¾çŠ¶ã€base_cvr=NaN\n",
    "                    base_block = None\n",
    "                    base_type = \"None\"\n",
    "\n",
    "        # åŸºæº–é‡ã¿\n",
    "        if base_block is not None:\n",
    "            w_base = _weights_from_block(base_block)\n",
    "            base_cvr = _cvr_by_block(base_block)           # åŸºæº–æœŸã®å®ŸCVR\n",
    "            cvr_base_by_group = _cvr_by_group(base_block)  # å‚è€ƒï¼ˆåˆ†è§£ç”¨ï¼‰\n",
    "        else:\n",
    "            # åŸºæº–ãªã—ï¼šç¾çŠ¶usersã‚·ã‚§ã‚¢ã‚’æš«å®šé‡ã¿ã¨ã—ã¦ãŠã\n",
    "            w_base = _weights_from_block(now_block)\n",
    "            base_cvr = np.nan\n",
    "            cvr_base_by_group = {}\n",
    "\n",
    "        # ç¾åœ¨ã®ãƒãƒ£ãƒãƒ«åˆ¥CVR\n",
    "        cvr_now_by_group = _cvr_by_group(now_block)\n",
    "\n",
    "        # Mix-Adjusted CVRï¼ˆåŸºæº–é‡ã¿Ã—ç¾åœ¨CVRï¼‰\n",
    "        # ãªã„ãƒãƒ£ãƒãƒ«ã¯0é‡ã¿ã§OK\n",
    "        mixadj = 0.0\n",
    "        for g, w in w_base.items():\n",
    "            cg = cvr_now_by_group.get(g, np.nan)\n",
    "            mixadj += w * (0.0 if np.isnan(cg) else cg)\n",
    "\n",
    "        # å®ŸCVRï¼ˆç¾åœ¨ï¼‰\n",
    "        actual_cvr = _cvr_by_block(now_block)\n",
    "\n",
    "        # å¯„ä¸åˆ†è§£ï¼ˆç´”ç²‹æ”¹å–„ï¼CVRè‡ªä½“ã®æ”¹å–„ã€ãƒŸãƒƒã‚¯ã‚¹åŠ¹æœï¼‰\n",
    "        # base_cvr ã¯ã€ŒåŸºæº–æœŸã®å®ŸCVRã€ã€‚ç´”ç²‹æ”¹å–„ = mixadj - base_cvr\n",
    "        pure_impr = mixadj - base_cvr if pd.notna(base_cvr) else np.nan\n",
    "        mix_effect = actual_cvr - mixadj if pd.notna(mixadj) else np.nan\n",
    "\n",
    "        # ä¸»è¦ãƒãƒ£ãƒãƒ«ï¼ˆAd/Map/Organicï¼‰ã®æ¯”ç‡ï¼ˆåŸºæº– vs ç¾çŠ¶ï¼‰ã¨ç´”ç²‹æ”¹å–„å¯„ä¸ï¼ˆå‚è€ƒï¼‰\n",
    "        key_groups = [\"Ad\",\"Map\",\"Organic\"]\n",
    "        weights_base = {f\"w_base_{g}\": w_base.get(g, 0.0) for g in key_groups}\n",
    "        weights_now  = {f\"w_now_{g}\": _weights_from_block(now_block).get(g, 0.0) for g in key_groups}\n",
    "        pure_by_g = {}\n",
    "        if base_block is not None:\n",
    "            for g in key_groups:\n",
    "                cvr_b = cvr_base_by_group.get(g, np.nan)\n",
    "                cvr_n = cvr_now_by_group.get(g, np.nan)\n",
    "                w     = w_base.get(g, 0.0)\n",
    "                pure_by_g[f\"pure_{g}\"] = w * ((0.0 if np.isnan(cvr_n) else cvr_n) - (0.0 if np.isnan(cvr_b) else cvr_b))\n",
    "        else:\n",
    "            for g in key_groups:\n",
    "                pure_by_g[f\"pure_{g}\"] = np.nan\n",
    "\n",
    "        rows.append({\n",
    "            \"clinic\": clinic,\n",
    "            \"month\": m,\n",
    "            \"ym\": int(m.strftime(\"%Y%m\")),\n",
    "            \"users\": now_block[\"users\"].sum(),\n",
    "            \"cv\": now_block[\"cv\"].sum(),\n",
    "            \"actual_cvr\": actual_cvr,\n",
    "            \"mixadj_cvr\": mixadj,\n",
    "            \"base_cvr\": base_cvr,\n",
    "            \"pure_impr\": pure_impr,   # = mixadj_cvr - base_cvr\n",
    "            \"mix_effect\": mix_effect, # = actual_cvr - mixadj_cvr\n",
    "            \"baseline_type\": base_type,\n",
    "            \"base_start_ym\": None if base_ref_start is None else int(base_ref_start.strftime(\"%Y%m\")),\n",
    "            \"base_end_ym\": None if base_ref_end is None else int(base_ref_end.strftime(\"%Y%m\")),\n",
    "            **weights_base, **weights_now, **pure_by_g\n",
    "        })\n",
    "\n",
    "df_mix_summary = pd.DataFrame(rows).sort_values([\"clinic\",\"month\"])\n",
    "\n",
    "print(\"âœ… Mix-Adjusted summary built:\", df_mix_summary.shape)\n",
    "\n",
    "# --- æ›¸ãå‡ºã—ï¼š_channel_summaryï¼ˆé‡è¤‡ã‚­ãƒ¼ clinicÃ—ym ã‚’ä¸Šæ›¸ãï¼‰ ---\n",
    "SAVE_TO_SHEET = True if 'SAVE_TO_SHEET' in globals() and SAVE_TO_SHEET else False\n",
    "if SAVE_TO_SHEET:\n",
    "    print(\"== Writeback to _channel_summary ==\")\n",
    "    try:\n",
    "        sh_write = gc.open_by_url(WRITE_GS_URL)\n",
    "        try:\n",
    "            ws_ch = sh_write.worksheet(\"_channel_summary\")\n",
    "            df_exist = get_as_dataframe(ws_ch, evaluate_formulas=True, header=0)\n",
    "            if df_exist is None or df_exist.empty:\n",
    "                df_exist = pd.DataFrame(columns=df_mix_summary.columns)\n",
    "            df_exist.columns = [str(c).strip() for c in df_exist.columns]\n",
    "        except gspread.WorksheetNotFound:\n",
    "            ws_ch = sh_write.add_worksheet(title=\"_channel_summary\", rows=10000, cols=100)\n",
    "            df_exist = pd.DataFrame(columns=df_mix_summary.columns)\n",
    "\n",
    "        # upsertï¼ˆclinicÃ—ym å¾Œå‹ã¡ï¼‰\n",
    "        need_cols = list(df_mix_summary.columns)\n",
    "        for c in need_cols:\n",
    "            if c not in df_exist.columns: df_exist[c] = pd.Series(dtype=\"float64\" if c not in [\"clinic\",\"baseline_type\"] else \"object\")\n",
    "\n",
    "        merged = pd.concat([df_exist[need_cols], df_mix_summary[need_cols]], ignore_index=True)\n",
    "        merged[\"_key\"] = merged[\"clinic\"].astype(str) + \"_\" + merged[\"ym\"].astype(str)\n",
    "        merged = merged.sort_values([\"clinic\",\"ym\",\"month\"]).drop_duplicates(subset=[\"_key\"], keep=\"last\").drop(columns=[\"_key\"])\n",
    "        merged = merged.sort_values([\"ym\",\"clinic\"], ascending=[False, True])\n",
    "\n",
    "        ws_ch.clear()\n",
    "        set_with_dataframe(ws_ch, merged)\n",
    "        print(f\"âœ… Wrote {df_mix_summary.shape[0]} rows to _channel_summary (upsert)\")\n",
    "    except Exception as e:\n",
    "        print(\"âš ï¸ Writeback(_channel_summary) failed:\", e)\n",
    "else:\n",
    "    print(\"ï¼ˆæ›¸ãå‡ºã—ã¯ SAVE_TO_SHEET=False ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼‰\")\n",
    "\n",
    "print(\"âœ… Cell 3 complete.\")\n"
   ],
   "metadata": {
    "cellView": "form",
    "id": "Xupf51qgE_wL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Couns Microcharts â€” Cell 4 (TTM / YoY_3M / EWMA Slope / Health Score)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charts in ASCII only to avoid JP font issues\n",
    "matplotlib.rcParams[\"font.family\"] = \"DejaVu Sans\"\n",
    "matplotlib.rcParams[\"axes.unicode_minus\"] = True\n",
    "\n",
    "# ---- Guards ----\n",
    "if 'm_c' not in globals() or m_c is None or m_c.empty:\n",
    "    raise SystemExit(\"âŒ m_c (couns metrics) is empty. Run Cell 2 first (the _couns block).\")\n",
    "if 'CLINIC_JA_TO_EN' not in globals():\n",
    "    # fallback from CLINIC_FILTER if needed\n",
    "    try:\n",
    "        CLINIC_JA_TO_EN = {ja: en for ja, en in CLINIC_FILTER}\n",
    "    except Exception:\n",
    "        CLINIC_JA_TO_EN = {}\n",
    "\n",
    "# ---- Sort clinics: good â†’ poor by latest health_score ----\n",
    "latest_c = (m_c.sort_values([\"clinic\",\"month\"])\n",
    "              .groupby(\"clinic\", as_index=False)\n",
    "              .tail(1)[[\"clinic\",\"health_score\"]])\n",
    "order_couns = (latest_c.sort_values(\"health_score\", ascending=False)[\"clinic\"].tolist())\n",
    "\n",
    "# Fallback: keep declared order if something goes wrong\n",
    "if not order_couns:\n",
    "    order_couns = [ja for ja, _ in CLINIC_FILTER]\n",
    "\n",
    "def en_label(clinic_ja: str, idx: int) -> str:\n",
    "    return CLINIC_JA_TO_EN.get(clinic_ja, f\"C{idx+1}\")\n",
    "\n",
    "# ---- Small multiples helper ----\n",
    "def small_multiples_couns(df, clinics, metric, cols=12, last_n=13,\n",
    "                          ylim=None, title=\"\"):\n",
    "    \"\"\"\n",
    "    df: m_c (couns metrics)\n",
    "    metric: one of [\"couns_ttm12\",\"yoy_3m\",\"ewma_slope\",\"health_score\"]\n",
    "    cols: number of columns in the grid (e.g., 12)\n",
    "    last_n: use last N points per clinic (e.g., 13 to match Sheets)\n",
    "    ylim: tuple(ymin, ymax) or None for autoscale\n",
    "    \"\"\"\n",
    "    n = len(clinics)\n",
    "    cols = max(1, min(cols, n))\n",
    "    rows = int(np.ceil(n / cols))\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols,\n",
    "                             figsize=(cols*1.6, rows*1.2),\n",
    "                             squeeze=False)\n",
    "    axes_flat = axes.ravel()\n",
    "\n",
    "    for i, clinic in enumerate(clinics):\n",
    "        ax = axes_flat[i]\n",
    "        g = df[df[\"clinic\"] == clinic].sort_values(\"month\").copy()\n",
    "        if last_n is not None and last_n > 0:\n",
    "            g = g.tail(last_n)\n",
    "\n",
    "        y = g[metric]\n",
    "        ax.plot(g[\"month\"], y, linewidth=1)\n",
    "        if y.dropna().shape[0] > 0:\n",
    "            ax.plot(g[\"month\"].iloc[-1], y.dropna().iloc[-1], marker=\"o\", markersize=2)\n",
    "\n",
    "        # minimal look\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        for s in ax.spines.values():\n",
    "            s.set_visible(False)\n",
    "\n",
    "        ax.text(0.02, 0.80, en_label(clinic, i),\n",
    "                transform=ax.transAxes, fontsize=7, fontweight=\"bold\")\n",
    "\n",
    "        if ylim is not None:\n",
    "            ax.set_ylim(ylim[0], ylim[1])\n",
    "\n",
    "    # remove unused axes\n",
    "    for j in range(i+1, rows*cols):\n",
    "        fig.delaxes(axes_flat[j])\n",
    "\n",
    "    if title:\n",
    "        fig.suptitle(title, y=0.98, fontsize=12)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# ---- Explanations (JP) + Charts (EN in-figure) ----\n",
    "COLS = 12        # æ¨ªä¸¦ã³æœ¬æ•°ï¼ˆSheetsã¨åŒã˜è¦‹æ „ãˆï¼‰\n",
    "LAST_N = 13      # æœ€æ–°13ç‚¹ï¼ˆSheetsã®ã‚¹ãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ³ã¨ä¸€è‡´ï¼‰\n",
    "\n",
    "print(\"=== ç§»å‹•åˆè¨ˆï¼ˆTTM=Trailing 12-Monthï¼‰ ===\")\n",
    "print(\"  ç›´è¿‘12ãƒ¶æœˆã®æ¥åº—ã‚«ã‚¦ãƒ³ã‚»ãƒªãƒ³ã‚°åˆè¨ˆã€‚å­£ç¯€æ€§ã‚’å¹³å‡åŒ–ï¼ˆæœˆãƒ–ãƒ¬ã‚’å¸åï¼‰ã—ã€ä¸­é•·æœŸã®å®ŸåŠ›ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’æŠŠæ¡ã€‚\")\n",
    "print(\"  å³è‚©ä¸ŠãŒã‚Šï¼åœ°åˆã„æ”¹å–„ã€‚æ¨ªã°ã„ï¼åœæ»ã€‚å³ç«¯ã®ç‚¹ãŒæœ€æ–°å€¤ã€‚\")\n",
    "small_multiples_couns(m_c, order_couns, metric=\"couns_ttm12\",\n",
    "                      cols=COLS, last_n=LAST_N, ylim=None,\n",
    "                      title=\"Couns â€” TTM (Trailing 12-Month)\")\n",
    "\n",
    "print(\"=== EWMAï¼ˆæŒ‡æ•°ç§»å‹•å¹³å‡ï¼‰ã®å‚¾ã ===\")\n",
    "print(\"  ç›´è¿‘ã®é‡ã¿ã‚’å¤§ããã™ã‚‹å¹³å‡ï¼ˆåŠæ¸›æœŸ=Cell 2ã¨åŒã˜ï¼‰ã€‚å‚¾ããŒä¸Šå‘ããªã‚‰å‹¢ã„ãŒå‡ºã¦ã„ã‚‹ã€‚å¶ç™ºã‚¹ãƒ‘ã‚¤ã‚¯ã¯æŠ‘åˆ¶ã€‚\")\n",
    "print(\"  TTMãŒä¸Šå‘ãï¼†EWMAå‚¾ãã‚‚ãƒ—ãƒ©ã‚¹ãªã‚‰â€œæ”¹å–„ã®ç¢ºåº¦â€ãŒé«˜ã„ã€‚\")\n",
    "small_multiples_couns(m_c, order_couns, metric=\"ewma_slope\",\n",
    "                      cols=COLS, last_n=LAST_N, ylim=None,\n",
    "                      title=\"Couns â€” EWMA Slope (last 6m)\")\n",
    "\n",
    "print(\"=== 3ãƒ¶æœˆç§»å‹•åˆè¨ˆã®å‰å¹´åŒæœŸæ¯”ï¼ˆãƒ­ãƒ¼ãƒªãƒ³ã‚°YoYï¼‰ ===\")\n",
    "print(\"  å­£ç¯€è¦å› ã‚’åˆã‚ã›ãŸä¼¸ã³ç‡ã€‚0è¶…ï¼å‰å¹´ã‚ˆã‚Šæ”¹å–„ã€0æœªæº€ï¼æ‚ªåŒ–ã€‚å¤–ã‚Œå€¤ã‚ˆã‚ŠæŒç¶šæ€§ã‚’é‡è¦–ã€‚\")\n",
    "small_multiples_couns(m_c, order_couns, metric=\"yoy_3m\",\n",
    "                      cols=COLS, last_n=LAST_N, ylim=(-1, 1),\n",
    "                      title=\"Couns â€” Rolling YoY (3-month sum)\")\n",
    "\n",
    "print(\"=== health_scoreï¼ˆç·åˆã‚¹ã‚³ã‚¢ï¼‰ ===\")\n",
    "print(\"  å®šç¾©ï¼š0.6*YoY_3M + 0.4*(EWMAå‚¾ãã®è¦æ¨¡æ­£è¦åŒ–)ã€‚çŸ­ä¸­æœŸã®å‹¢ã„ã¨åœ°åˆã„ã‚’1æŒ‡æ¨™ã«é›†ç´„ã€‚\")\n",
    "print(\"  0ä»˜è¿‘ã¯æ¨ªã°ã„ã€ãƒ—ãƒ©ã‚¹ã¯æ”¹å–„ã€ãƒã‚¤ãƒŠã‚¹ã¯æ‚ªåŒ–ã€‚\")\n",
    "small_multiples_couns(m_c, order_couns, metric=\"health_score\",\n",
    "                      cols=COLS, last_n=LAST_N, ylim=(-1, 1),\n",
    "                      title=\"Couns â€” Health Score\")\n"
   ],
   "metadata": {
    "cellView": "form",
    "id": "yyix3eQeag-o"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Channel Microcharts â€” Cell 5 (TTM & Mix Share by Clinic)\n",
    "\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "KEY_GROUPS = [\"Ad\",\"Map\",\"Organic\"]\n",
    "\n",
    "# è‹±èªãƒ©ãƒ™ãƒ«ç”¨ãƒãƒƒãƒ—ï¼ˆCLINIC_FILTER ãŒ [(JA, EN), ...] ãªã‚‰è‡ªå‹•ç”Ÿæˆï¼‰\n",
    "try:\n",
    "    CLINIC_JA_TO_EN = {ja: en for ja, en in CLINIC_FILTER}\n",
    "except Exception:\n",
    "    # æ‰‹å‹•ã§æŒ‡å®šã™ã‚‹å ´åˆã¯ã“ã“ã‚’ç·¨é›†\n",
    "    CLINIC_JA_TO_EN = {\n",
    "        \"æ± è¢‹\":\"Ikebukuro\",\"åšå¤š\":\"Hakata\",\"ä»™å°\":\"Sendai\",\"æ¨ªæµœ\":\"Yokohama\",\n",
    "        \"æ¸‹è°·\":\"Shibuya\",\"é›£æ³¢\":\"Namba\",\"æœ­å¹Œ\":\"Sapporo\",\"å¤©ç¥\":\"Tenjin\",\n",
    "        \"æ±äº¬\":\"Tokyo\",\"æ¢…ç”°\":\"Umeda\",\"æ–°å®¿\":\"Shinjuku\"\n",
    "    }\n",
    "\n",
    "def en_label(clinic_ja: str) -> str:\n",
    "    return CLINIC_JA_TO_EN.get(clinic_ja, clinic_ja)\n",
    "\n",
    "# === ordering (same as Cell 4) ===\n",
    "try:\n",
    "    ORDER_CLINICS = list(order_couns)  # Cell 4 ã§ä½œã£ãŸæœ€æ–° health_score é™é †\n",
    "except NameError:\n",
    "    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: CLINIC_FILTER ã®é † â†’ ãã‚Œä»¥å¤–ã¯Î±é †ã§æœ«å°¾\n",
    "    try:\n",
    "        ORDER_CLINICS = [ja for ja, _ in CLINIC_FILTER]\n",
    "    except Exception:\n",
    "        ORDER_CLINICS = []\n",
    "\n",
    "def prep_channel_ttm_and_share(ch_month: pd.DataFrame):\n",
    "    z = ch_month.copy()\n",
    "    z = z[z[\"ch_group\"].isin(KEY_GROUPS)].copy()\n",
    "    # TTMï¼ˆ12ãƒ¶æœˆç§»å‹•åˆè¨ˆï¼‰ã‚’ clinicÃ—ch_group ã”ã¨ã«\n",
    "    z = z.sort_values([\"clinic\",\"ch_group\",\"month\"])\n",
    "    z[\"cv_ttm12\"] = (z.groupby([\"clinic\",\"ch_group\"])[\"cv\"]\n",
    "                       .transform(lambda s: s.rolling(12, min_periods=1).sum()))\n",
    "    z[\"users_ttm12\"] = (z.groupby([\"clinic\",\"ch_group\"])[\"users\"]\n",
    "                          .transform(lambda s: s.rolling(12, min_periods=1).sum()))\n",
    "    # Shareï¼ˆusers_ttm12 ã®æ¯”ç‡ï¼‰\n",
    "    sum_users = (z.groupby([\"clinic\",\"month\"])[\"users_ttm12\"].transform(\"sum\"))\n",
    "    z[\"share_users_ttm12\"] = np.where(sum_users>0, z[\"users_ttm12\"]/sum_users, np.nan)\n",
    "    return z\n",
    "\n",
    "def small_multiples_by_clinic(df, metric, title, cols=12, last_n=13):\n",
    "    # clinics = sorted(df[\"clinic\"].unique().tolist(),\n",
    "    #                  key=lambda c: -df[df[\"clinic\"]==c][metric].tail(1).mean())\n",
    "    uniq = df[\"clinic\"].unique().tolist()\n",
    "    if ORDER_CLINICS:\n",
    "        # ORDER_CLINICS ã«å‡ºç¾ã™ã‚‹ã‚‚ã® â†’ æ®‹ã‚Šã‚’Î±é †ã§å¾Œã‚ã«\n",
    "        ordered = [c for c in ORDER_CLINICS if c in uniq]\n",
    "        tail    = sorted([c for c in uniq if c not in ORDER_CLINICS])\n",
    "        clinics = ordered + tail\n",
    "    else:\n",
    "        clinics = sorted(uniq)  # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\n",
    "\n",
    "    n = len(clinics); cols = min(cols, n); rows = int(np.ceil(n/cols))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols*1.6, rows*1.2), squeeze=False)\n",
    "    axf = axes.ravel()\n",
    "    for i, clinic in enumerate(clinics):\n",
    "        ax = axf[i]\n",
    "        w = df[df[\"clinic\"]==clinic].copy()\n",
    "        w = w.sort_values(\"month\")\n",
    "        if last_n:\n",
    "            # æœ€æ–°Nãƒ¶æœˆã®ã¿\n",
    "            last_months = w[\"month\"].drop_duplicates().sort_values().tail(last_n)\n",
    "            w = w[w[\"month\"].isin(last_months)]\n",
    "        # å„ãƒãƒ£ãƒãƒ«ã‚’é‡ã­æãï¼ˆè‰²ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰\n",
    "        for g in KEY_GROUPS:\n",
    "            gg = w[w[\"ch_group\"]==g]\n",
    "            if gg.empty:\n",
    "                continue\n",
    "            ax.plot(gg[\"month\"], gg[metric], linewidth=1, label=g)\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        for s in ax.spines.values(): s.set_visible(False)\n",
    "        # ax.text(0.02, 0.80, clinic, transform=ax.transAxes, fontsize=7, fontweight=\"bold\")\n",
    "        ax.text(0.02, 0.80, en_label(clinic), transform=ax.transAxes, fontsize=7, fontweight=\"bold\")\n",
    "        if i==0:\n",
    "            ax.legend(loc=\"lower right\", fontsize=6, frameon=False)\n",
    "    for j in range(i+1, rows*cols):\n",
    "        fig.delaxes(axf[j])\n",
    "    if title: fig.suptitle(title, y=0.98, fontsize=12)\n",
    "    plt.tight_layout(rect=[0,0,1,0.96]); plt.show()\n",
    "\n",
    "# ---- build & plot ----\n",
    "if \"ch_month\" not in globals() or ch_month is None or ch_month.empty:\n",
    "    raise SystemExit(\"âŒ ch_month ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆCell 3 ã‚’å…ˆã«å®Ÿè¡Œï¼‰\")\n",
    "\n",
    "cm = prep_channel_ttm_and_share(ch_month)\n",
    "\n",
    "print(\"=== Channel TTM (Ad/Map/Organic) ===\")\n",
    "print(\"  é™¢Ã—ãƒãƒ£ãƒãƒ«ã®12ãƒ¶æœˆç§»å‹•åˆè¨ˆï¼ˆCVï¼‰ã€‚è¦æ¨¡å·®ã®ã‚ã‚‹é™¢é–“æ¯”è¼ƒã¯æ³¨æ„ã€é™¢å†…ã®å¯„ä¸æ¨ç§»ã‚’è¦‹ã‚‹ã€‚\")\n",
    "small_multiples_by_clinic(cm, metric=\"cv_ttm12\",\n",
    "                          title=\"Channel CV â€” TTM (12M)\", cols=12, last_n=13)\n",
    "\n",
    "print(\"=== Channel Mix Share (Users, 12M) ===\")\n",
    "print(\"  æµå…¥æ§‹æˆï¼ˆUsersæ¯”ç‡, 12Mï¼‰ã€‚æ§‹æˆã®å¤‰åŒ–ãŒ CVãƒ»CVR ã«æ³¢åŠã€‚\")\n",
    "small_multiples_by_clinic(cm, metric=\"share_users_ttm12\",\n",
    "                          title=\"Channel Mix â€” Users Share (12M)\", cols=12, last_n=13)\n"
   ],
   "metadata": {
    "cellView": "form",
    "id": "a2gxKETsj8a7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Mix-Adjusted CVR â€” Cell 6 (Actual vs Mix-Adjusted vs Baseline / Pure by Channel / Mix Effect)\n",
    "\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "# --- guards & helpers ---\n",
    "if \"df_mix_summary\" not in globals() or df_mix_summary is None or df_mix_summary.empty:\n",
    "    raise SystemExit(\"âŒ df_mix_summary ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆCell 3 ã‚’å…ˆã«å®Ÿè¡Œï¼‰\")\n",
    "\n",
    "try:\n",
    "    ORDER_CLINICS = list(order_couns)   # Cell 4 ã¨åŒã˜é †\n",
    "except Exception:\n",
    "    try:\n",
    "        ORDER_CLINICS = [ja for ja, _ in CLINIC_FILTER]\n",
    "    except Exception:\n",
    "        ORDER_CLINICS = sorted(df_mix_summary[\"clinic\"].unique().tolist())\n",
    "\n",
    "try:\n",
    "    CLINIC_JA_TO_EN = {ja: en for ja, en in CLINIC_FILTER}\n",
    "except Exception:\n",
    "    CLINIC_JA_TO_EN = {}\n",
    "\n",
    "def en_label(clinic_ja: str) -> str:\n",
    "    return CLINIC_JA_TO_EN.get(clinic_ja, clinic_ja)\n",
    "\n",
    "KEY_GROUPS = [\"Ad\",\"Map\",\"Organic\"]\n",
    "COLS      = 12\n",
    "LAST_N    = 13\n",
    "\n",
    "def _clinic_order(seq):\n",
    "    uniq = list(dict.fromkeys(seq))  # keep order\n",
    "    ordered = [c for c in ORDER_CLINICS if c in uniq]\n",
    "    tail = [c for c in uniq if c not in ORDER_CLINICS]\n",
    "    return ordered + tail\n",
    "\n",
    "def _grid(n, cols=12):\n",
    "    cols = max(1, min(cols, n))\n",
    "    rows = int(np.ceil(n/cols))\n",
    "    return rows, cols\n",
    "\n",
    "# --- 1) Actual vs Mix-Adjusted vs Baseline (CVR) ---\n",
    "print(\"=== Actual vs Mix-Adjusted vs Baseline CVR (last 13 months) ===\")\n",
    "d = df_mix_summary.copy().sort_values([\"clinic\",\"month\"])\n",
    "clinics = _clinic_order(d[\"clinic\"].unique())\n",
    "rows, cols = _grid(len(clinics), COLS)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(cols*1.6, rows*1.2), squeeze=False)\n",
    "axf = axes.ravel()\n",
    "for i, c in enumerate(clinics):\n",
    "    ax = axf[i]\n",
    "    g = d[d[\"clinic\"]==c].sort_values(\"month\").tail(LAST_N)\n",
    "    for col in [\"actual_cvr\",\"mixadj_cvr\",\"base_cvr\"]:\n",
    "        if col in g.columns:\n",
    "            ax.plot(g[\"month\"], g[col], linewidth=1)\n",
    "    if len(g):\n",
    "        ax.plot(g[\"month\"].iloc[-1], g[\"actual_cvr\"].iloc[-1], marker=\"o\", markersize=2)\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    for s in ax.spines.values(): s.set_visible(False)\n",
    "    ax.text(0.02, 0.80, en_label(c), transform=ax.transAxes, fontsize=7, fontweight=\"bold\")\n",
    "# legend once\n",
    "axf[0].legend([\"Actual\",\"Mix-Adj\",\"Baseline\"], loc=\"lower right\", fontsize=6, frameon=False)\n",
    "for j in range(i+1, rows*cols): fig.delaxes(axf[j])\n",
    "fig.suptitle(\"CVR â€” Actual vs Mix-Adjusted vs Baseline\", y=0.98, fontsize=12)\n",
    "plt.tight_layout(rect=[0,0,1,0.96]); plt.show()\n",
    "\n",
    "# --- 2) Pure Improvement by Channel (baseline-weighted Î”CVR) ---\n",
    "print(\"=== Pure Improvement by Channel (p.p., last 13 months) ===\")\n",
    "d2 = d.copy()\n",
    "# p.p. ã«å¤‰æ›ï¼ˆ*100ï¼‰\n",
    "for gk in KEY_GROUPS:\n",
    "    col = f\"pure_{gk}\"\n",
    "    if col in d2.columns:\n",
    "        d2[col] = d2[col] * 100.0\n",
    "\n",
    "clinics = _clinic_order(d2[\"clinic\"].unique())\n",
    "rows, cols = _grid(len(clinics), COLS)\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(cols*1.6, rows*1.2), squeeze=False); axf = axes.ravel()\n",
    "\n",
    "# ç›®ç››ãŒåˆ‡ã‚Œãªã„ã‚ˆã†ã« Â±max(|å€¤|) ã‚’å…¨é™¢å…±é€šã§å–å¾—ï¼ˆæœ€ä½Â±1p.p.ï¼‰\n",
    "all_absmax = 1.0\n",
    "for gk in KEY_GROUPS:\n",
    "    col = f\"pure_{gk}\"\n",
    "    if col in d2.columns:\n",
    "        m = d2[col].abs().max()\n",
    "        if pd.notna(m): all_absmax = max(all_absmax, float(m))\n",
    "yl = (-all_absmax, all_absmax)\n",
    "\n",
    "for i, c in enumerate(clinics):\n",
    "    ax = axf[i]\n",
    "    g = d2[d2[\"clinic\"]==c].sort_values(\"month\").tail(LAST_N)\n",
    "    for gk in KEY_GROUPS:\n",
    "        col = f\"pure_{gk}\"\n",
    "        if col in g.columns:\n",
    "            ax.plot(g[\"month\"], g[col], linewidth=1, label=gk)\n",
    "    ax.axhline(0, linewidth=0.8)\n",
    "    ax.set_ylim(yl[0], yl[1])\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    for s in ax.spines.values(): s.set_visible(False)\n",
    "    ax.text(0.02, 0.80, en_label(c), transform=ax.transAxes, fontsize=7, fontweight=\"bold\")\n",
    "# legend once\n",
    "axf[0].legend(loc=\"lower right\", fontsize=6, frameon=False, title=\"Pure Î”CVR (p.p.)\")\n",
    "for j in range(i+1, rows*cols): fig.delaxes(axf[j])\n",
    "fig.suptitle(\"Pure Improvement by Channel â€” baseline-weighted Î”CVR (p.p.)\", y=0.98, fontsize=12)\n",
    "plt.tight_layout(rect=[0,0,1,0.96]); plt.show()\n",
    "\n",
    "# --- 3) Mix Effect (actual âˆ’ mix-adj) and Ad share shift (optional) ---\n",
    "print(\"=== Mix Effect (actual âˆ’ mix-adj) (p.p., last 13 months) ===\")\n",
    "d3 = d.copy()\n",
    "d3[\"mix_effect_pp\"] = d3[\"mix_effect\"] * 100.0\n",
    "\n",
    "clinics = _clinic_order(d3[\"clinic\"].unique())\n",
    "rows, cols = _grid(len(clinics), COLS)\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(cols*1.6, rows*1.2), squeeze=False); axf = axes.ravel()\n",
    "\n",
    "absmax = max(1.0, float(d3[\"mix_effect_pp\"].abs().max()))\n",
    "yl = (-absmax, absmax)\n",
    "\n",
    "for i, c in enumerate(clinics):\n",
    "    ax = axf[i]\n",
    "    g = d3[d3[\"clinic\"]==c].sort_values(\"month\").tail(LAST_N)\n",
    "    ax.plot(g[\"month\"], g[\"mix_effect_pp\"], linewidth=1)\n",
    "    ax.axhline(0, linewidth=0.8)\n",
    "    ax.set_ylim(yl[0], yl[1])\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    for s in ax.spines.values(): s.set_visible(False)\n",
    "    ax.text(0.02, 0.80, en_label(c), transform=ax.transAxes, fontsize=7, fontweight=\"bold\")\n",
    "\n",
    "for j in range(i+1, rows*cols): fig.delaxes(axf[j])\n",
    "fig.suptitle(\"Mix Effect â€” (Actual CVR âˆ’ Mix-Adjusted CVR) (p.p.)\", y=0.98, fontsize=12)\n",
    "plt.tight_layout(rect=[0,0,1,0.96]); plt.show()\n",
    "\n",
    "print(\"Done: three microchart sets above (same order as Cell 4/5).\")\n"
   ],
   "metadata": {
    "cellView": "form",
    "id": "wHp9ZlK6TtWP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title GSC â†’ Brand/Non-Brandé›†è¨ˆ â†’ Google Sheetsæ›¸ãå‡ºã—ï¼ˆSAã¯userdataã‹ã‚‰ï¼‰\n",
    "# äº‹å‰æº–å‚™ï¼š\n",
    "# 1) Search Console ã®å¯¾è±¡ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã§ã€ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ãƒ¡ãƒ¼ãƒ«ã‚’ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨æ¨©é™ã€ã«è¿½åŠ ï¼ˆé–²è¦§ä»¥ä¸Šï¼‰\n",
    "# 2) æ›¸ãå‡ºã—å…ˆã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’ã€ãã®ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ã€Œç·¨é›†è€…ã€ã§å…±æœ‰\n",
    "# 3) Colabã§ä¸€åº¦ã ã‘ï¼šfrom google.colab import userdata; userdata.set('GCP_SA_KEY', '<ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆJSONæ–‡å­—åˆ—>')\n",
    "# 4) SITE_URL ã‚’ã‚ãªãŸã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã«å¤‰æ›´ï¼ˆURLãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹: https://example.com/ æœ«å°¾ã‚¹ãƒ©ãƒƒã‚·ãƒ¥å¿…é ˆ / ãƒ‰ãƒ¡ã‚¤ãƒ³: sc-domain:example.comï¼‰\n",
    "\n",
    "!pip -q install google-api-python-client gspread gspread-dataframe\n",
    "\n",
    "import json, base64, re\n",
    "from datetime import date, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from google.colab import userdata\n",
    "\n",
    "# ===== è¨­å®š =====\n",
    "# Search Console ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£\n",
    "SITE_URL = \"https://tenjinkyousei.com/\"  # ä¾‹: URLãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã€‚ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãªã‚‰ \"sc-domain:tenjinkyousei.com\"\n",
    "DAYS_PULL = 180                           # å–å¾—æœŸé–“ï¼ˆéå»180æ—¥ï¼‰\n",
    "WINDOW_COMPARE = 90                       # ç›´è¿‘90æ—¥ vs ãã®å‰90æ—¥\n",
    "\n",
    "# æ›¸ãå‡ºã—å…ˆã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ\n",
    "WRITE_GS_URL = \"https://docs.google.com/spreadsheets/d/1zw8BEwj6pxe-n7PKyC80GFr0EzIHqS3XOpuBVMVeF-4/\"\n",
    "\n",
    "# Brandåˆ¤å®šç”¨ï¼ˆé™¢åã®æ—¥æœ¬èª/è‹±èªã€ãƒ–ãƒ©ãƒ³ãƒ‰èªï¼‰\n",
    "CLINIC_FILTER = [\n",
    "    (\"æ± è¢‹\",\"Ikebukuro\"),(\"åšå¤š\",\"Hakata\"),(\"ä»™å°\",\"Sendai\"),\n",
    "    (\"æ¨ªæµœ\",\"Yokohama\"),(\"æ¸‹è°·\",\"Shibuya\"),(\"é›£æ³¢\",\"Namba\"),\n",
    "    (\"æœ­å¹Œ\",\"Sapporo\"),(\"å¤©ç¥\",\"Tenjin\"),(\"æ±äº¬\",\"Tokyo\"),\n",
    "    (\"æ¢…ç”°\",\"Umeda\"),(\"æ–°å®¿\",\"Shinjuku\"),\n",
    "]\n",
    "BRAND_EXTRA = [\"å¤©ç¥ã‚­ãƒ¥ã‚¢çŸ¯æ­£æ­¯ç§‘\",\"å¤©ç¥ã‚­ãƒ¥ã‚¢\",\"ã‚­ãƒ¥ã‚¢çŸ¯æ­£æ­¯ç§‘\",\"å¤©ç¥æ­¯ç§‘çŸ¯æ­£æ­¯ç§‘\"]\n",
    "\n",
    "# ===== èªè¨¼ï¼ˆColab userdata ã«ä¿å­˜ã—ãŸ SA JSON ã‚’ä½¿ç”¨ï¼‰=====\n",
    "raw = userdata.get('GCP_SA_KEY')\n",
    "if raw is None:\n",
    "    raise SystemExit(\"âŒ userdataã« GCP_SA_KEY ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æœ€åˆã« `userdata.set('GCP_SA_KEY', '<JSON>')` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "\n",
    "try:\n",
    "    sa_info = json.loads(raw)\n",
    "except Exception:\n",
    "    # base64ã§ä¿å­˜ã—ã¦ã„ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\n",
    "    try:\n",
    "        sa_info = json.loads(base64.b64decode(raw).decode('utf-8'))\n",
    "    except Exception as e:\n",
    "        raise SystemExit(f\"âŒ GCP_SA_KEY ã®JSONãƒ‡ã‚³ãƒ¼ãƒ‰ã«å¤±æ•—: {e}\")\n",
    "\n",
    "SCOPES = [\n",
    "    \"https://www.googleapis.com/auth/webmasters.readonly\",  # Search Console èª­ã¿å–ã‚Š\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\",         # Sheets æ›¸ãè¾¼ã¿\n",
    "    \"https://www.googleapis.com/auth/drive\",                # gspreadå†…éƒ¨ã§åˆ©ç”¨\n",
    "]\n",
    "creds = service_account.Credentials.from_service_account_info(sa_info, scopes=SCOPES)\n",
    "\n",
    "# ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ\n",
    "sc_service = build(\"searchconsole\", \"v1\", credentials=creds, cache_discovery=False)\n",
    "gc = gspread.authorize(creds)\n",
    "sa_email = sa_info.get(\"client_email\")\n",
    "print(\"âœ… SA loaded:\", sa_email)\n",
    "\n",
    "# ===== GSC å–å¾— =====\n",
    "def gsc_query(site_url: str, dimensions, startDate, endDate, rowLimit=25000) -> pd.DataFrame:\n",
    "    body = {\"startDate\": str(startDate), \"endDate\": str(endDate),\n",
    "            \"dimensions\": dimensions, \"rowLimit\": rowLimit}\n",
    "    res = sc_service.searchanalytics().query(siteUrl=site_url, body=body).execute()\n",
    "    rows = res.get(\"rows\", [])\n",
    "    out = []\n",
    "    for r in rows:\n",
    "        rec = {k: v for k, v in zip(dimensions, r.get(\"keys\", []))}\n",
    "        rec.update({k: r[k] for k in (\"clicks\",\"impressions\",\"ctr\",\"position\") if k in r})\n",
    "        out.append(rec)\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "end = date.today()\n",
    "start = end - timedelta(days=DAYS_PULL)\n",
    "dims = [\"query\",\"page\",\"device\",\"date\"]\n",
    "\n",
    "try:\n",
    "    df = gsc_query(SITE_URL, dims, start, end)\n",
    "except Exception as e:\n",
    "    raise SystemExit(f\"âŒ GSC API ã‚¨ãƒ©ãƒ¼: {e}\\nSITE_URL ãŒãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã¨ä¸€è‡´ã—ã¦ã„ã‚‹ã‹ã€SAã®ãƒ¦ãƒ¼ã‚¶ãƒ¼è¿½åŠ /ã‚¹ã‚³ãƒ¼ãƒ—ã‚’ç¢ºèªã€‚\")\n",
    "\n",
    "if df.empty:\n",
    "    raise SystemExit(\"âŒ GSC: ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã—ãŸã€‚SITE_URL/æ¨©é™/æœŸé–“ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n",
    "\n",
    "# å‹æ•´å‚™\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "if df[\"date\"].isna().any():\n",
    "    bad = df[df[\"date\"].isna()].head()\n",
    "    raise SystemExit(f\"âŒ GSC: dateã®ãƒ‘ãƒ¼ã‚¹å¤±æ•—ã€‚ä¾‹\\n{bad}\")\n",
    "\n",
    "# ctr/positionã¯APIãŒè¿”ã™å€¤ã‚’ä½¿ã†ï¼ˆctrã¯å‰²åˆã€positionã¯å¹³å‡æ²è¼‰é †ä½ï¼‰\n",
    "df[\"ctr\"] = pd.to_numeric(df[\"ctr\"], errors=\"coerce\")\n",
    "df[\"position\"] = pd.to_numeric(df[\"position\"], errors=\"coerce\")\n",
    "df[\"clicks\"] = pd.to_numeric(df[\"clicks\"], errors=\"coerce\")\n",
    "df[\"impressions\"] = pd.to_numeric(df[\"impressions\"], errors=\"coerce\")\n",
    "\n",
    "df[\"month\"] = df[\"date\"].values.astype(\"datetime64[M]\")\n",
    "\n",
    "# ===== Brand/Non-Brand ãƒ©ãƒ™ãƒªãƒ³ã‚° =====\n",
    "# names = [re.escape(ja) for ja,_ in CLINIC_FILTER] + [re.escape(en) for _,en in CLINIC_FILTER]\n",
    "names = [re.escape(x) for x in BRAND_EXTRA]\n",
    "BRAND_REGEX = re.compile(\"|\".join(names), flags=re.IGNORECASE) if names else re.compile(\"$^\")  # ä½•ã‚‚ãªã‘ã‚Œã°å…¨ä¸ä¸€è‡´\n",
    "\n",
    "df[\"brand_flag\"] = df[\"query\"].astype(str).apply(lambda q: bool(BRAND_REGEX.search(q)))\n",
    "df[\"brand\"] = np.where(df[\"brand_flag\"], \"Brand\", \"Non-Brand\")\n",
    "\n",
    "# ===== æœˆÃ—brandÃ—device é›†è¨ˆï¼ˆCTR/Positionã¯Impré‡ã¿ï¼‰=====\n",
    "agg0 = (df.groupby([\"month\",\"brand\",\"device\"], as_index=False)\n",
    "          .agg(clicks=(\"clicks\",\"sum\"), impressions=(\"impressions\",\"sum\")))\n",
    "agg0[\"ctr\"] = np.where(agg0[\"impressions\"]>0, agg0[\"clicks\"]/agg0[\"impressions\"], np.nan)\n",
    "\n",
    "# positionã®Impré‡ã¿å¹³å‡\n",
    "pos_w = (df.groupby([\"month\",\"brand\",\"device\"])\n",
    "           .apply(lambda g: (g[\"position\"]*g[\"impressions\"]).sum()/g[\"impressions\"].sum()\n",
    "                  if g[\"impressions\"].sum()>0 else np.nan)\n",
    "           .reset_index(name=\"position\"))\n",
    "agg = agg0.merge(pos_w, on=[\"month\",\"brand\",\"device\"], how=\"left\")\n",
    "\n",
    "# ===== ç›´è¿‘90æ—¥ vs ãã®å‰90æ—¥ æ¯”è¼ƒ =====\n",
    "cut = df[\"date\"].max() - pd.Timedelta(days=WINDOW_COMPARE)\n",
    "df[\"period\"] = np.where(df[\"date\"] > cut, \"last90d\", \"prev90d\")\n",
    "\n",
    "comp0 = (df.groupby([\"brand\",\"device\",\"period\"], as_index=False)\n",
    "           .agg(clicks=(\"clicks\",\"sum\"), impressions=(\"impressions\",\"sum\")))\n",
    "comp0[\"ctr\"] = np.where(comp0[\"impressions\"]>0, comp0[\"clicks\"]/comp0[\"impressions\"], np.nan)\n",
    "pos_w2 = (df.groupby([\"brand\",\"device\",\"period\"])\n",
    "            .apply(lambda g: (g[\"position\"]*g[\"impressions\"]).sum()/g[\"impressions\"].sum()\n",
    "                   if g[\"impressions\"].sum()>0 else np.nan)\n",
    "            .reset_index(name=\"position\"))\n",
    "comp = comp0.merge(pos_w2, on=[\"brand\",\"device\",\"period\"], how=\"left\")\n",
    "\n",
    "# ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼šè¦‹ã‚„ã™ã„å·®åˆ†ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆbrandÃ—deviceã§ lastâˆ’prev ã®Î”ã‚‚å‡ºã™ï¼‰\n",
    "pivot_last = comp[comp[\"period\"]==\"last90d\"].set_index([\"brand\",\"device\"])\n",
    "pivot_prev = comp[comp[\"period\"]==\"prev90d\"].set_index([\"brand\",\"device\"])\n",
    "delta = pivot_last[[\"clicks\",\"impressions\",\"ctr\",\"position\"]].join(\n",
    "    pivot_prev[[\"clicks\",\"impressions\",\"ctr\",\"position\"]], how=\"outer\", lsuffix=\"_last\", rsuffix=\"_prev\"\n",
    ")\n",
    "for col in [\"clicks\",\"impressions\",\"ctr\",\"position\"]:\n",
    "    delta[f\"delta_{col}\"] = delta[f\"{col}_last\"] - delta[f\"{col}_prev\"]\n",
    "delta = delta.reset_index()\n",
    "\n",
    "# ===== Sheets æ›¸ãå‡ºã— =====\n",
    "def write_ws(sh, title, data: pd.DataFrame):\n",
    "    import gspread\n",
    "    from gspread_dataframe import set_with_dataframe\n",
    "    try:\n",
    "        ws = sh.worksheet(title); ws.clear()\n",
    "    except gspread.WorksheetNotFound:\n",
    "        ws = sh.add_worksheet(title=title, rows=max(1000, len(data)+10), cols=max(26, len(data.columns)+2))\n",
    "    set_with_dataframe(ws, data)\n",
    "    print(f\"âœ… Wrote {title}: {data.shape[0]} rows x {data.shape[1]} cols\")\n",
    "\n",
    "try:\n",
    "    sh = gc.open_by_url(WRITE_GS_URL)\n",
    "except Exception as e:\n",
    "    raise SystemExit(f\"âŒ Sheetsã‚’é–‹ã‘ã¾ã›ã‚“ã€‚{e}\\nã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’ {sa_email} ã«ã€ç·¨é›†è€…ã€ã§å…±æœ‰ã—ã¦ãã ã•ã„ã€‚\")\n",
    "\n",
    "# ä¸¦ã¹æ›¿ãˆã¨æ›¸ãå‡ºã—\n",
    "agg_out = agg.sort_values([\"month\",\"brand\",\"device\"]).copy()\n",
    "comp_out = comp.sort_values([\"brand\",\"device\",\"period\"]).copy()\n",
    "delta_out = delta.sort_values([\"brand\",\"device\"]).copy()\n",
    "\n",
    "write_ws(sh, \"_gsc_month_brand\", agg_out)\n",
    "write_ws(sh, \"_gsc_comp_brand90\", comp_out)\n",
    "write_ws(sh, \"_gsc_comp_brand90_delta\", delta_out)\n",
    "\n",
    "print(\"ğŸ‰ Done.\\n- _gsc_month_brand: æœˆÃ—Brand/Non-BrandÃ—Device ã®é›†è¨ˆ\\n- _gsc_comp_brand90: 90æ—¥æ¯”è¼ƒï¼ˆlast/prev è¡Œï¼‰\\n- _gsc_comp_brand90_delta: lastâˆ’prev ã®å·®åˆ†ï¼ˆã‚¯ãƒªãƒƒã‚¯/è¡¨ç¤º/CTR/æ²è¼‰é †ä½ï¼‰\")\n"
   ],
   "metadata": {
    "cellView": "form",
    "id": "neJp6PST0izh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title GSC detail delta (page Ã— query Ã— device) â†’ _gsc_detail_delta / _gsc_page_summary\n",
    "import numpy as np, pandas as pd, gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "\n",
    "if \"df\" not in globals() or df.empty:\n",
    "    raise SystemExit(\"âŒ df ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆGSCå–å¾—ã‚»ãƒ«ã‚’å…ˆã«å®Ÿè¡Œï¼‰\")\n",
    "\n",
    "# æœŸé–“ãƒ•ãƒ©ã‚°ï¼ˆç›´è¿‘90æ—¥ vs ãã®å‰90æ—¥ï¼‰\n",
    "cut = df[\"date\"].max() - pd.Timedelta(days=WINDOW_COMPARE)\n",
    "z = df.copy()\n",
    "z[\"period\"] = np.where(z[\"date\"] > cut, \"last90d\", \"prev90d\")\n",
    "\n",
    "# --- pageÃ—queryÃ—deviceÃ—brandÃ—period ã§é›†è¨ˆ ---\n",
    "keys = [\"brand\",\"device\",\"page\",\"query\",\"period\"]\n",
    "\n",
    "# ã‚¯ãƒªãƒƒã‚¯/è¡¨ç¤ºã‚’åˆç®—\n",
    "base = (z.groupby(keys, as_index=False)\n",
    "          .agg(clicks=(\"clicks\",\"sum\"), impressions=(\"impressions\",\"sum\")))\n",
    "\n",
    "# CTR ã¯ clicks / impressionsï¼ˆImpré‡ã¿ã®å¹³å‡ã«ä¸€è‡´ï¼‰\n",
    "base[\"ctr\"] = np.where(base[\"impressions\"]>0, base[\"clicks\"]/base[\"impressions\"], np.nan)\n",
    "\n",
    "# position ã¯ Impré‡ã¿å¹³å‡\n",
    "wpos = (z.assign(w=lambda d: d[\"position\"]*d[\"impressions\"])\n",
    "          .groupby(keys, as_index=False)\n",
    "          .agg(w=(\"w\",\"sum\"), imp=(\"impressions\",\"sum\")))\n",
    "wpos[\"position\"] = np.where(wpos[\"imp\"]>0, wpos[\"w\"]/wpos[\"imp\"], np.nan)\n",
    "\n",
    "g = base.merge(wpos[keys+[\"position\"]], on=keys, how=\"left\")\n",
    "\n",
    "# last / prev ã‚’æ¨ªæŒã¡åŒ–\n",
    "last = g[g[\"period\"]==\"last90d\"].drop(columns=\"period\").add_suffix(\"_last\")\n",
    "prev = g[g[\"period\"]==\"prev90d\"].drop(columns=\"period\").add_suffix(\"_prev\")\n",
    "\n",
    "# ã‚­ãƒ¼ã‚’å¾©å…ƒã—ã¦çµåˆ\n",
    "for df_ in (last, prev):\n",
    "    for k in [\"brand\",\"device\",\"page\",\"query\"]:\n",
    "        df_[k] = df_[f\"{k}_last\"] if f\"{k}_last\" in df_.columns else df_[f\"{k}_prev\"]\n",
    "detail = pd.merge(\n",
    "    last.drop(columns=[c for c in last.columns if c.endswith(\"_last\") and c.split(\"_last\")[0] in [\"brand\",\"device\",\"page\",\"query\"]]),\n",
    "    prev.drop(columns=[c for c in prev.columns if c.endswith(\"_prev\") and c.split(\"_prev\")[0] in [\"brand\",\"device\",\"page\",\"query\"]]),\n",
    "    on=[\"brand\",\"device\",\"page\",\"query\"], how=\"outer\"\n",
    ")\n",
    "\n",
    "# æ¬ æå‡¦ç†ã¨å·®åˆ†\n",
    "for c in [\"clicks_last\",\"impressions_last\",\"clicks_prev\",\"impressions_prev\"]:\n",
    "    detail[c] = detail[c].fillna(0)\n",
    "# CTR/pos ã¯è¨ˆç®—ä¸èƒ½ãªã‚‰ NaNã®ã¾ã¾\n",
    "detail[\"delta_clicks\"]      = detail[\"clicks_last\"]      - detail[\"clicks_prev\"]\n",
    "detail[\"delta_impressions\"] = detail[\"impressions_last\"] - detail[\"impressions_prev\"]\n",
    "detail[\"delta_ctr\"]         = (detail[\"ctr_last\"].fillna(0) - detail[\"ctr_prev\"].fillna(0))\n",
    "detail[\"delta_position\"]    = (detail[\"position_last\"] - detail[\"position_prev\"])\n",
    "\n",
    "# ãƒã‚¤ã‚ºé™¤å»ï¼ˆå¯¾è±¡ã®è¡¨ç¤ºå›æ•°ãŒæ¥µå°ã®ã‚‚ã®ã‚’è½ã¨ã™ï¼‰\n",
    "detail = detail[(detail[\"impressions_last\"] + detail[\"impressions_prev\"]) >= 20].copy()\n",
    "\n",
    "# å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢ï¼ˆCTRæ‚ªåŒ–Ã—è¦æ¨¡ï¼‰â€»ä»»æ„\n",
    "detail[\"priority_score\"] = (-detail[\"delta_ctr\"].fillna(0)) * detail[\"impressions_last\"]\n",
    "\n",
    "# ã—ãã„å€¤ãƒ™ãƒ¼ã‚¹ã®ãƒ•ãƒ©ã‚°ï¼ˆåˆæœŸå€¤ï¼šÂ±0.5ä½/ -0.5ppï¼‰\n",
    "POS_WINDOW = 0.5   # ã€Œé †ä½ã¯ã»ã¼åŒã˜ã€ã®ç¯„å›²\n",
    "CTR_DROP   = 0.005 # CTR æ‚ªåŒ–ã®é–¾å€¤ï¼ˆ0.5ppï¼‰\n",
    "POS_WORSE  = 0.5   # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åŠ£åŒ–åˆ¤å®šã®é †ä½æ‚ªåŒ–å¹…\n",
    "\n",
    "detail[\"title_flag\"]   = ((detail[\"delta_position\"].abs() <= POS_WINDOW) & (detail[\"delta_ctr\"] < -CTR_DROP)).astype(int)\n",
    "detail[\"content_flag\"] = (detail[\"delta_position\"] >= POS_WORSE).astype(int)\n",
    "\n",
    "# ä¸¦ã³æ›¿ãˆ\n",
    "detail_sorted = detail.sort_values([\"priority_score\"], ascending=False)\n",
    "\n",
    "# --- pageå˜ä½ã®è¦ç´„ï¼ˆã©ã®ãƒšãƒ¼ã‚¸ã‚’ç›´ã™ã¹ãã‹ï¼‰ ---\n",
    "page_sum = (detail.assign(title_cnt=detail[\"title_flag\"], content_cnt=detail[\"content_flag\"])\n",
    "                 .groupby([\"brand\",\"device\",\"page\"], as_index=False)\n",
    "                 .agg(queries=(\"query\",\"nunique\"),\n",
    "                      title_cnt=(\"title_cnt\",\"sum\"),\n",
    "                      content_cnt=(\"content_cnt\",\"sum\"),\n",
    "                      impr_last=(\"impressions_last\",\"sum\"),\n",
    "                      ctr_last=(\"clicks_last\",\"sum\")))\n",
    "page_sum[\"ctr_last\"] = np.where(page_sum[\"impr_last\"]>0, page_sum[\"ctr_last\"]/page_sum[\"impr_last\"], np.nan)\n",
    "page_sum = page_sum.sort_values([\"title_cnt\",\"content_cnt\",\"impr_last\"], ascending=[False,False,False])\n",
    "\n",
    "# --- Sheets æ›¸ãå‡ºã— ---\n",
    "try:\n",
    "    sh = gc.open_by_url(WRITE_GS_URL)\n",
    "except Exception as e:\n",
    "    raise SystemExit(f\"âŒ Sheetsã‚’é–‹ã‘ã¾ã›ã‚“ã€‚{e}\")\n",
    "\n",
    "def write_ws(title, data):\n",
    "    try:\n",
    "        ws = sh.worksheet(title); ws.clear()\n",
    "    except gspread.WorksheetNotFound:\n",
    "        ws = sh.add_worksheet(title=title, rows=max(1000, len(data)+50), cols=40)\n",
    "    set_with_dataframe(ws, data)\n",
    "    print(f\"âœ… Wrote {title}: {data.shape[0]} rows\")\n",
    "\n",
    "write_ws(\"_gsc_detail_delta\", detail_sorted[\n",
    "    [\"brand\",\"device\",\"page\",\"query\",\n",
    "     \"clicks_last\",\"impressions_last\",\"ctr_last\",\"position_last\",\n",
    "     \"clicks_prev\",\"impressions_prev\",\"ctr_prev\",\"position_prev\",\n",
    "     \"delta_clicks\",\"delta_impressions\",\"delta_ctr\",\"delta_position\",\n",
    "     \"priority_score\",\"title_flag\",\"content_flag\"]\n",
    "])\n",
    "write_ws(\"_gsc_page_summary\", page_sum)\n",
    "print(\"ğŸ‰ Done: _gsc_detail_delta / _gsc_page_summary ã‚’å‡ºåŠ›ã—ã¾ã—ãŸã€‚\")\n"
   ],
   "metadata": {
    "id": "b3CXI3yo9ugB",
    "cellView": "form"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "WIHfdwYgHYc1"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}