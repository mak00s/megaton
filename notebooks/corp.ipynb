{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9h6nV8KaGiBD"
   },
   "source": [
    "# GA4データ抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "QCRwW8-LVUAp",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "a42f255187684fd0bfcfec591e0f0db8",
      "6f5905ddb5954b388898a6ef2050ca28",
      "0fb956005a3d42598bf014007767f0df",
      "3d0b35cf47cc4ff6a38eb169f9e8f5f0",
      "285452a6a1994f6881dad7731e4c7952",
      "76266de82e4146c49dfeb063b3bdf74f",
      "c9d283909a0f49bbb7319fc9ed2c3427",
      "92c4e657f7c744f89add94c5c48cd17b",
      "cc9172dd035e4408b92fc3c0904be700",
      "dcb92e63da364140959799caaa54a478"
     ]
    },
    "outputId": "0888d45b-0de5-4cd5-ad42-e27f6cf0041a"
   },
   "outputs": [],
   "source": [
    "#@title 初期設定\n",
    "\n",
    "try:\n",
    "    from google.colab import data_table, files as colab_files\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    data_table = None\n",
    "    colab_files = None\n",
    "    IN_COLAB = False\n",
    "\n",
    "try:\n",
    "    from megaton import start\n",
    "except ModuleNotFoundError:\n",
    "    if IN_COLAB:\n",
    "        %pip install -U -q git+https://github.com/mak00s/megaton\n",
    "        from megaton import start\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "import calendar\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import re\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "def get_past_date(n_days=None, n_months=None, first_or_last=None, timezone=\"Asia/Tokyo\", date_format=\"%Y-%m-%d\"):\n",
    "    \"\"\"\n",
    "    Returns today's date, the first day of the current month, a date N days ago,\n",
    "    or the first/last day of N months ago in a specified format.\n",
    "\n",
    "    Args:\n",
    "        n_days (int, optional): The number of days ago. Default is None.\n",
    "        n_months (int, optional): The number of months ago. Default is None.\n",
    "        first_or_last (str, optional): 'first' or 'last' to specify the first or last day of the month. Default is None.\n",
    "        timezone (str): Timezone for the calculation. Default is 'Asia/Tokyo'.\n",
    "        date_format (str): The desired output date format. Default is '%Y-%m-%d'.\n",
    "\n",
    "    Returns:\n",
    "        str: The calculated date in the specified format.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If both 'n_days' and 'n_months' are provided, or if 'first_or_last' is invalid.\n",
    "    \"\"\"\n",
    "    if n_days is not None and n_months is not None:\n",
    "        raise ValueError(\"Specify either 'n_days' or 'n_months', but not both.\")\n",
    "    if first_or_last and first_or_last not in ['first', 'last']:\n",
    "        raise ValueError(\"Invalid value for 'first_or_last'. Use 'first' or 'last'.\")\n",
    "\n",
    "    # Current datetime in the specified timezone\n",
    "    now = datetime.now(pytz.timezone(timezone))\n",
    "\n",
    "    if n_days is not None:\n",
    "        # Calculate N days ago\n",
    "        result_date = now - timedelta(days=n_days)\n",
    "    elif n_months is not None:\n",
    "        # Calculate the first or last day of N months ago\n",
    "        year = now.year\n",
    "        month = now.month - n_months\n",
    "        while month <= 0:\n",
    "            year -= 1\n",
    "            month += 12\n",
    "        if first_or_last == 'first':\n",
    "            result_date = datetime(year, month, 1, tzinfo=pytz.timezone(timezone))\n",
    "        elif first_or_last == 'last':\n",
    "            last_day = calendar.monthrange(year, month)[1]\n",
    "            result_date = datetime(year, month, last_day, tzinfo=pytz.timezone(timezone))\n",
    "        else:\n",
    "            # Default to the start of the month if first_or_last is None\n",
    "            result_date = datetime(year, month, 1, tzinfo=pytz.timezone(timezone))\n",
    "    else:\n",
    "        # Default to today or the first day of the current month\n",
    "        if first_or_last == 'first':\n",
    "            result_date = datetime(now.year, now.month, 1, tzinfo=pytz.timezone(timezone))\n",
    "        else:\n",
    "            result_date = now\n",
    "\n",
    "    return result_date.strftime(date_format)\n",
    "\n",
    "def duplicate_sheet(source_sheet_title, new_sheet_name, cell_update=None):\n",
    "    \"\"\"\n",
    "    Duplicates a Google Sheets worksheet and updates a specific cell in the new sheet.\n",
    "\n",
    "    Args:\n",
    "        source_sheet_title (str): The title of the worksheet to duplicate.\n",
    "        new_sheet_name (str): The title for the new duplicated worksheet.\n",
    "        cell_update (dict, optional): A dictionary specifying a cell to update, e.g., {\"cell\": \"B1\", \"value\": \"some_value\"}.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get the source worksheet\n",
    "        source_worksheet = mg.gs._driver.worksheet(source_sheet_title)\n",
    "\n",
    "        # Duplicate the worksheet\n",
    "        mg.gs._driver.duplicate_sheet(\n",
    "            source_worksheet.id,\n",
    "            new_sheet_name=new_sheet_name\n",
    "        )\n",
    "        logger.info(f\"Worksheet '{new_sheet_name}' duplicated from '{source_sheet_title}'.\")\n",
    "\n",
    "        # Update a specific cell in the duplicated sheet if specified\n",
    "        if cell_update:\n",
    "            mg.gs.sheet._driver.update_acell(cell_update[\"cell\"], cell_update[\"value\"])\n",
    "            logger.info(f\"Cell '{cell_update['cell']}' updated with value '{cell_update['value']}' in '{new_sheet_name}'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error duplicating worksheet '{source_sheet_title}' to '{new_sheet_name}': {e}\")\n",
    "\n",
    "def save_to_google_sheet(gs_url, sheet_name, df, sort_by=None):\n",
    "    \"\"\"\n",
    "    Saves a DataFrame to a Google Sheet. If the sheet exists, it clears the existing data;\n",
    "    otherwise, it creates a new sheet. Adjusts column widths based on the data and freezes the first row.\n",
    "\n",
    "    Args:\n",
    "        gs_url (str): The URL of the Google Sheets document.\n",
    "        sheet_name (str): The name of the sheet to save the data.\n",
    "        df (pd.DataFrame): The DataFrame to save.\n",
    "    \"\"\"\n",
    "    def calculate_pixel_size(value, single_byte_multiplier=7, multi_byte_multiplier=14):\n",
    "        \"\"\"\n",
    "        Calculates the pixel size for a given value, accounting for multi-byte characters.\n",
    "\n",
    "        Args:\n",
    "            value (str): The string value to calculate the size for.\n",
    "            single_byte_multiplier (int): Width multiplier for single-byte characters.\n",
    "            multi_byte_multiplier (int): Width multiplier for multi-byte characters.\n",
    "\n",
    "        Returns:\n",
    "            int: The calculated pixel size.\n",
    "        \"\"\"\n",
    "        total_width = 0\n",
    "        for char in str(value):\n",
    "            if ord(char) < 128:  # Single-byte character\n",
    "                total_width += single_byte_multiplier\n",
    "            else:  # Multi-byte character\n",
    "                total_width += multi_byte_multiplier\n",
    "        return total_width\n",
    "\n",
    "    # Sort the DataFrame if specified\n",
    "    if sort_by:\n",
    "        df = df.sort_values(by=sort_by, ascending=False)\n",
    "\n",
    "    if mg.open.sheet(gs_url):\n",
    "        try:\n",
    "            # Try to create a new sheet\n",
    "            mg.gs._driver.add_worksheet(title=sheet_name, rows=10, cols=10)\n",
    "        except Exception:\n",
    "            # If the sheet already exists, select it and clear the data\n",
    "            mg.gs.sheet.select(sheet_name)\n",
    "            mg.gs.sheet.clear()\n",
    "\n",
    "        # Save the DataFrame to the sheet\n",
    "        mg.save.to.sheet(df=df, sheet_name=sheet_name)\n",
    "\n",
    "        # Get the sheet object and its ID\n",
    "        sheet = mg.gs._driver.worksheet(sheet_name)\n",
    "        sheet_id = sheet.id\n",
    "\n",
    "        # Calculate column widths\n",
    "        column_widths = []\n",
    "        for col_name in df.columns:\n",
    "            max_length = max(\n",
    "                df[col_name].astype(str).map(\n",
    "                    lambda x: calculate_pixel_size(x)\n",
    "                ).max(),\n",
    "                calculate_pixel_size(col_name)\n",
    "            )\n",
    "            pixel_size = max(min(max_length, 500), 50)  # Minimum width: 50, Maximum width: 500\n",
    "            column_widths.append(pixel_size)\n",
    "\n",
    "        # Prepare batch update requests for column resizing and freezing the first row\n",
    "        requests = [\n",
    "            # Column resizing\n",
    "            {\n",
    "                \"updateDimensionProperties\": {\n",
    "                    \"range\": {\n",
    "                        \"sheetId\": sheet_id,\n",
    "                        \"dimension\": \"COLUMNS\",\n",
    "                        \"startIndex\": i,\n",
    "                        \"endIndex\": i + 1\n",
    "                    },\n",
    "                    \"properties\": {\"pixelSize\": column_widths[i]},\n",
    "                    \"fields\": \"pixelSize\"\n",
    "                }\n",
    "            }\n",
    "            for i in range(len(column_widths))\n",
    "        ]\n",
    "\n",
    "        # Add request to freeze the first row\n",
    "        requests.append({\n",
    "            \"updateSheetProperties\": {\n",
    "                \"properties\": {\n",
    "                    \"sheetId\": sheet_id,\n",
    "                    \"gridProperties\": {\"frozenRowCount\": 1}\n",
    "                },\n",
    "                \"fields\": \"gridProperties.frozenRowCount\"\n",
    "            }\n",
    "        })\n",
    "\n",
    "        # Execute batch update\n",
    "        mg.gs._driver.batch_update({\"requests\": requests})\n",
    "        print(f\"Data successfully saved to the sheet: {sheet_name}, column widths adjusted, and first row frozen.\")\n",
    "    else:\n",
    "        print(\"Failed to open the Google Sheets document.\")\n",
    "\n",
    "def update_sheets_cells(cells_to_update):\n",
    "    \"\"\"\n",
    "    Updates specific cells across multiple sheets in Google Sheets with provided values.\n",
    "\n",
    "    Args:\n",
    "        cells_to_update (dict): A dictionary where keys are sheet names, and values are\n",
    "                                dictionaries mapping cell names to their values.\n",
    "\n",
    "                                Example:\n",
    "                                {\n",
    "                                    \"Page\": {\"I1\": From, \"K1\": __To},\n",
    "                                    \"Summary\": {\"A1\": \"Report Start\", \"B1\": From}\n",
    "                                }\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for sheet_name, updates in cells_to_update.items():\n",
    "            # Select the target sheet\n",
    "            mg.gs.sheet.select(sheet_name)\n",
    "\n",
    "            # Update the specified cells with the provided values\n",
    "            for cell, value in updates.items():\n",
    "                mg.gs.sheet._driver.update_acell(cell, value)\n",
    "\n",
    "            print(f\"Successfully updated sheet '{sheet_name}' with updates: {updates}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while updating the sheets: {e}\")\n",
    "\n",
    "CREDS_PATH = \"/nbs/key/sa-shiseido-corp-dts.json\"\n",
    "mg = start.Megaton(CREDS_PATH, use_ga3=False)\n",
    "\n",
    "GA4_ACCOUNT = '151965783'  #param {type:\"string\"}\n",
    "GA4_PROPERTY = '334854563'  #param {type:\"string\"}\n",
    "mg.ga['4'].account.select(GA4_ACCOUNT)\n",
    "mg.ga['4'].property.select(GA4_PROPERTY)\n",
    "\n",
    "# BQ準備\n",
    "GCP_PROJECT = \"shiseido-corp-1872\"  #param {type:\"string\"}\n",
    "bq = mg.launch_bigquery(GCP_PROJECT)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "#markdown サイト定義\n",
    "content = [ # コンテンツカテゴリの定義\n",
    "    (\"Top\", \"^/(en|jp)/($|\\?)\"),\n",
    "    (\"Company\", \"^/(en|jp)/(americas|apac|china|company|emea|japan|sports|tr)/\"),\n",
    "    (\"Brand\", \"^/(en|jp)/brands/\"),\n",
    "    (\"Sustainability\", \"^/(en|jp)/sustainability/\"),\n",
    "    (\"Innovation\", \"^/(en|jp)/rd/\"),\n",
    "    (\"Careers\", \"^/(en|jp)/(careers|recruit|scp/careers)\"),\n",
    "    (\"Investors\", \"^/(en|jp)/ir/\"),\n",
    "    (\"News Releases\", \"^/(en|jp)/news/\"),\n",
    "]\n",
    "\n",
    "#markdown 書き込むGoogle SheetsのURL\n",
    "URL1 = \"https://docs.google.com/spreadsheets/d/1pDMKBzIc-1brDQRRjEtWqJb0ZfONqReiXTSHJ9iqKTU\" #アクセス推移\n",
    "URL2 = \"https://docs.google.com/spreadsheets/d/1dM7PSHS48lBKTEuXB1xJDV5rw4qDeBWUWdZ_itqN6sw\" #入口コンテンツ\n",
    "URL3 = \"https://docs.google.com/spreadsheets/d/1UjooCBtrqEbmQK5uBZlWaVbVTGvvgeNPtP-IdLPIVG8\" #Shiseido Talks\n",
    "URL4 = \"https://docs.google.com/spreadsheets/d/1ZHXZBgtsGZmiMdmO2NnRP31p6QU-9lyqVOW4IdybzuY\" #News Release\n",
    "URL5 = \"https://docs.google.com/spreadsheets/d/1v1oL3kq6dhWGmuFsk870YSrcBscJkfq6Rx7lCeQWNCE\" #検索ワード\n",
    "URL6 = \"https://docs.google.com/spreadsheets/d/1G-yKox0_ZIp3NG_agkSFplsjqW5BrH4QbjVwsItp8_E\" #TOPクリック\n",
    "URL8 = \"https://docs.google.com/spreadsheets/d/1mVXEbJ-DxWA25oCpOio9XeEA7ae0PI3mZrpwr8RH8Co\"\n",
    "\n",
    "# レポート期間\n",
    "# now = datetime.now(pytz.timezone(\"Asia/Tokyo\"))\n",
    "\n",
    "# 先月\n",
    "LM_From = get_past_date(n_months=1, first_or_last='first')\n",
    "LM___To = get_past_date(n_months=1, first_or_last='last')\n",
    "print(f\"先月の期間は{LM_From} - {LM___To}\")\n",
    "\n",
    "# 今月\n",
    "TM_From = get_past_date(first_or_last='first')\n",
    "TM___To = get_past_date(first_or_last='last')\n",
    "if TM___To < TM_From:\n",
    "    TM___To = TM_From\n",
    "print(f\"直近の期間は{TM_From} - {TM___To}\")\n",
    "\n",
    "# 年間\n",
    "YR___To = LM___To\n",
    "YR_From = get_past_date(n_months=13, first_or_last='first')\n",
    "print(f\"13ヶ月の期間は{YR_From} - {YR___To}\")\n",
    "YR_From12 = get_past_date(n_months=12, first_or_last='first')\n",
    "print(f\"12ヶ月の期間は{YR_From12} - {YR___To}\")\n",
    "\n",
    "print(\"準備ができました。次のセルへ進んでください。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylf6KYYMVh0m"
   },
   "source": [
    "## 01. アクセス推移"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "4kkQ6iMcWRes",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "044fd483-8aa2-4a2a-81d4-e5486682e757"
   },
   "outputs": [],
   "source": [
    "#@title Corp：先月\n",
    "mg.report.set_dates(LM_From, LM___To)\n",
    "\n",
    "mg.report.run(\n",
    "    d = [\"contentGroup\", (\"customEvent:lang\", \"language\")],\n",
    "    m = [\n",
    "        \"eventCount\",\n",
    "        \"sessions\",\n",
    "        \"totalUsers\",\n",
    "        \"averageSessionDuration\",\n",
    "        \"userEngagementDuration\",\n",
    "    ],\n",
    "    filter_d = \"eventName==page_view;contentGroup!@not set\",\n",
    "    sort = \"-sessions\")\n",
    "\n",
    "#@markdown 結果を書き込むGoogle Sheets\n",
    "Sheet = \"_corp_m\" #@param {type:\"string\"}\n",
    "\n",
    "if mg.open.sheet(URL1):\n",
    "    mg.save.to.sheet(sheet_name=Sheet)  # 上書きする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "sIBn-oQcVicm",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ccbf754c-0184-4216-ee4c-1fc2ce210c3d"
   },
   "outputs": [],
   "source": [
    "#@title Corp：先月〜昨日daily\n",
    "mg.report.set_dates(LM_From, TM___To)\n",
    "\n",
    "mg.report.run(\n",
    "    d = [\"date\", \"contentGroup\", (\"customEvent:lang\", \"language\")],\n",
    "    m = [\"sessions\"],\n",
    "    filter_d = \"eventName==page_view;contentGroup!@not set\",\n",
    "    sort = \"date\")\n",
    "\n",
    "#@markdown 結果を書き込むGoogle Sheets\n",
    "Sheet = \"_corp_m-d\" #@param {type:\"string\"}\n",
    "\n",
    "if mg.open.sheet(URL1):\n",
    "    mg.save.to.sheet(sheet_name=Sheet)\n",
    "\n",
    "# mg.gs.sheet.select(\"Corp先月日別\")\n",
    "# mg.gs.sheet._driver.update_acell(\"E1\", LM_From)\n",
    "# mg.gs.sheet._driver.update_acell(\"H1\", LM___To)\n",
    "update_sheets_cells({\"Corp先月日別\": {\"F1\": LM_From, \"I1\": LM___To}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "7o5SAO92XXfI",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0617bf4a-a357-40e0-a116-b5bda38f3b5e"
   },
   "outputs": [],
   "source": [
    "#@title Corp：daily Page\n",
    "mg.report.set_dates(LM_From, TM___To)\n",
    "\n",
    "# Define content groups as a dict: {label: match_condition}\n",
    "content_groups = {\n",
    "    \"企業情報\": \"会社案内\",\n",
    "    \"ブランド\": \"ブランド\",\n",
    "    \"サステナビリティ\": \"サステナビリティ\",\n",
    "    \"イノベーション\": \"研究\",  # using partial match due to delimiter issues\n",
    "    \"採用情報\": \"採用情報\",\n",
    "    \"投資家情報\": \"投資家情報\",\n",
    "    \"ニュースリリース\": \"ニュースリリース\"\n",
    "}\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for group_name, match_text in content_groups.items():\n",
    "    # Use full URL with query string for ニュースリリース\n",
    "    page_dim = (\"pagePathPlusQueryString\", \"page\") if group_name == \"ニュースリリース\" else (\"pagePath\", \"page\")\n",
    "\n",
    "    # Step 1: Get all pages in this content group\n",
    "    mg.report.run(\n",
    "        d=[page_dim, (\"customEvent:lang\", \"language\")],\n",
    "        m=[\"sessions\"],\n",
    "        filter_d=f\"eventName==page_view;contentGroup=@{match_text}\",\n",
    "        sort=\"-sessions\"\n",
    "    )\n",
    "    df_all_pages = mg.report.data\n",
    "\n",
    "    if df_all_pages.empty:\n",
    "        continue\n",
    "\n",
    "    # Step 2: Split by language and get top 5 per language\n",
    "    for lang in [\"JP\", \"EN\"]:\n",
    "        df_lang = df_all_pages[df_all_pages[\"language\"] == lang]\n",
    "        top_pages = df_lang[\"page\"].head(5).tolist()\n",
    "\n",
    "        if not top_pages:\n",
    "            continue\n",
    "\n",
    "        page_regex = \"|\".join(re.escape(p) + \"$\" for p in top_pages)\n",
    "\n",
    "        # Step 3: Fetch daily sessions for those pages\n",
    "        mg.report.run(\n",
    "            d=[\"date\", page_dim],\n",
    "            m=[\"sessions\"],\n",
    "            filter_d=f\"eventName==page_view;{page_dim[0]}=~({page_regex})\",\n",
    "            sort=\"date\"\n",
    "        )\n",
    "        df_daily = mg.report.data.copy()\n",
    "        df_daily[\"contentGroup\"] = group_name\n",
    "        df_daily[\"language\"] = lang\n",
    "\n",
    "        all_data.append(df_daily)\n",
    "\n",
    "# Final result\n",
    "df_final = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "#@markdown 結果を書き込むGoogle Sheets\n",
    "Sheet = \"_corp_page-d\" #@param {type:\"string\"}\n",
    "\n",
    "if mg.open.sheet(URL1):\n",
    "    mg.save.to.sheet(df=df_final, sheet_name=Sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "EFBWwl-xWp_t",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ad046e4a-f8e0-4638-ad02-ce6195788491"
   },
   "outputs": [],
   "source": [
    "#@title 独立サイト：当月→先月\n",
    "mg.report.set_dates(LM_From, LM___To)\n",
    "\n",
    "mg.report.run(\n",
    "    d = [(\"customEvent:site\", \"site\"), (\"customEvent:lang\", \"language\")],\n",
    "    m = [\n",
    "        \"eventCount\",\n",
    "        \"sessions\",\n",
    "        \"totalUsers\",\n",
    "        \"averageSessionDuration\",\n",
    "    ],\n",
    "    filter_d = \"eventName==page_view\",\n",
    "    sort = \"-sessions\")\n",
    "\n",
    "#@markdown 結果を書き込むGoogle Sheets\n",
    "Sheet = \"_site_m\" #@param {type:\"string\"}\n",
    "\n",
    "if mg.open.sheet(URL1):\n",
    "    mg.save.to.sheet(sheet_name=Sheet)  # 上書きする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "etZTyXTSNn3q",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0fb5c57b-89ec-4d67-9b90-dbfb70809de1"
   },
   "outputs": [],
   "source": [
    "#@title 独立サイト：先月〜昨日daily\n",
    "mg.report.set_dates(LM_From, TM___To)\n",
    "\n",
    "mg.report.run(\n",
    "    d = [\"date\", (\"customEvent:site\", \"site\"), (\"customEvent:lang\", \"language\")],\n",
    "    m = [\"sessions\"],\n",
    "    filter_d = \"eventName==page_view\",\n",
    "    sort = \"date\"\n",
    ")\n",
    "\n",
    "#@markdown 結果を書き込むGoogle Sheets\n",
    "Sheet = \"_site_m-d\" #@param {type:\"string\"}\n",
    "\n",
    "if mg.open.sheet(URL1):\n",
    "    mg.save.to.sheet(sheet_name=Sheet)  # 上書きする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "7fycUkbnWcQm",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2fef9a84-751c-4b32-8c1b-f517eccb009c"
   },
   "outputs": [],
   "source": [
    "#@title Corp：12m\n",
    "mg.report.set_dates(YR_From12, LM___To)\n",
    "print(f\"12ヶ月の期間は{YR_From12} - {YR___To}\")\n",
    "\n",
    "mg.report.run(\n",
    "    d = [\"contentGroup\", (\"customEvent:lang\", \"language\")],\n",
    "    m = [\n",
    "        (\"eventCount\", \"pv\"),\n",
    "        \"sessions\",\n",
    "        \"totalUsers\",\n",
    "        \"averageSessionDuration\",\n",
    "    ],\n",
    "    filter_d = \"eventName==page_view;contentGroup!@not set\",\n",
    "    sort = \"-sessions\")\n",
    "\n",
    "#@markdown 結果を書き込むGoogle Sheets\n",
    "Sheet = \"_corp_y\" #@param {type:\"string\"}\n",
    "#markdown シートを消去して上書き保存\n",
    "Overwrite = True #param {type:\"boolean\"}\n",
    "\n",
    "if mg.open.sheet(URL1):\n",
    "    if Overwrite:\n",
    "        mg.save.to.sheet(sheet_name=Sheet)  # 上書きする\n",
    "    else:\n",
    "        mg.append.to.sheet(sheet_name=Sheet)  # 追記する\n",
    "\n",
    "update_sheets_cells({\"Corp月別\": {\"F1\": YR_From12, \"H1\": LM___To}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ANOMGzfJWjc7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "33cc6b51-4cd0-4c24-93fb-5deb6a3edf11"
   },
   "outputs": [],
   "source": [
    "#@title Corp：13m月別\n",
    "mg.report.set_dates(YR_From, YR___To)\n",
    "\n",
    "mg.report.run(\n",
    "    d = [\n",
    "        \"year\", \"month\",\n",
    "        \"contentGroup\",\n",
    "        (\"customEvent:lang\", \"language\"),\n",
    "    ],\n",
    "    m = [\n",
    "        (\"eventCount\", \"pv\"),\n",
    "        \"sessions\",\n",
    "        \"totalUsers\",\n",
    "        \"averageSessionDuration\",\n",
    "    ],\n",
    "    filter_d = \"eventName==page_view;contentGroup!@not set\",\n",
    "    sort = \"-sessions\")\n",
    "\n",
    "df = mg.report.data\n",
    "df[\"ym\"] = df[\"year\"].astype(str) + \"/\" + df[\"month\"].astype(str) + \"/1\"\n",
    "df = df[['ym', 'contentGroup', 'language', 'pv', 'sessions', 'totalUsers', 'averageSessionDuration']].sort_values(by=['ym', 'contentGroup'])\n",
    "\n",
    "#@markdown 結果を書き込むGoogle Sheets\n",
    "Sheet = \"_corp_y-m\" #@param {type:\"string\"}\n",
    "#markdown シートを消去して上書き保存\n",
    "Overwrite = True #param {type:\"boolean\"}\n",
    "\n",
    "if mg.open.sheet(URL1):\n",
    "    if Overwrite:\n",
    "        mg.save.to.sheet(sheet_name=Sheet, df=df)  # 上書きする\n",
    "    else:\n",
    "        mg.append.to.sheet(sheet_name=Sheet)  # 追記する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "XlgOsUqsWywS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "338ce85d-dd86-410a-bb43-64b264c60160"
   },
   "outputs": [],
   "source": [
    "#@title 独立サイト：12m\n",
    "mg.report.set_dates(YR_From12, YR___To)\n",
    "\n",
    "mg.report.run(\n",
    "    d = [(\"customEvent:site\", \"site\"), (\"customEvent:lang\", \"language\")],\n",
    "    m = [\n",
    "        (\"eventCount\", \"pv\"),\n",
    "        \"sessions\",\n",
    "        \"totalUsers\",\n",
    "        \"averageSessionDuration\",\n",
    "    ],\n",
    "    filter_d = \"eventName==page_view\")\n",
    "\n",
    "#@markdown 結果を書き込むGoogle Sheets\n",
    "Sheet = \"_site_y\" #@param {type:\"string\"}\n",
    "\n",
    "if mg.open.sheet(URL1):\n",
    "    mg.save.to.sheet(sheet_name=Sheet)  # 上書きする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "IidgIv-aWvwz",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cd968f59-56c1-44ca-9335-a30fa38f27f7"
   },
   "outputs": [],
   "source": [
    "#@title 独立サイト：13m月別\n",
    "mg.report.set_dates(YR_From, YR___To)\n",
    "\n",
    "mg.report.run(\n",
    "    d = [\n",
    "        (\"customEvent:site\", \"site\"),\n",
    "        (\"customEvent:lang\", \"language\"),\n",
    "        \"year\",\n",
    "        \"month\",\n",
    "    ],\n",
    "    m = [\n",
    "        (\"eventCount\", \"pv\"),\n",
    "        \"sessions\",\n",
    "        \"totalUsers\",\n",
    "    ],\n",
    "    filter_d= \"eventName==page_view\")\n",
    "df = mg.report.data\n",
    "df[\"ym\"] = df[\"year\"].astype(str) + \"/\" + df[\"month\"].astype(str) + \"/1\"\n",
    "df = df[['ym', 'site', 'language', 'pv', 'sessions', 'totalUsers']].sort_values(by=['ym', 'site'])\n",
    "\n",
    "#@markdown 結果を書き込むGoogle Sheets\n",
    "Sheet = \"_site_y-m\" #@param {type:\"string\"}\n",
    "\n",
    "if mg.open.sheet(URL1):\n",
    "    mg.save.to.sheet(sheet_name=Sheet, df=df)  # 上書きする"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title 先月のNews Release本数をHTMLから取得 → GS差分保存\n",
    "\n",
    "#@markdown 属性を保存するGoogle Sheetsシート名\n",
    "GS_URL = URL1\n",
    "Sheet = \"_meta\"  #@param {type:\"string\"}\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def _ym_slash(date_str: str) -> str:\n",
    "    \"\"\"'YYYY-MM-DD' -> 'YYYY/M/1'（ゼロ詰めしない）\"\"\"\n",
    "    d = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "    return f\"{d.year}/{d.month}/1\"\n",
    "\n",
    "def _build_search_url(date_from: str, date_to: str) -> str:\n",
    "    \"\"\"Shiseido corp news 月間検索URLを生成\"\"\"\n",
    "    df = datetime.strptime(date_from, \"%Y-%m-%d\")\n",
    "    dt = datetime.strptime(date_to, \"%Y-%m-%d\")\n",
    "    base = \"https://corp.shiseido.com/jp/news/search.html\"\n",
    "    qs = (\n",
    "        f\"k1=&ca=ALL&pa=ALL\"\n",
    "        f\"&yf={df.year}&mf={df.month}&df={df.day}\"\n",
    "        f\"&yt={dt.year}&mt={dt.month}&dt={dt.day}\"\n",
    "        f\"&p=1\"\n",
    "    )\n",
    "    return f\"{base}?{qs}\"\n",
    "\n",
    "def _fetch_release_count(url: str, timeout: int = 20) -> int:\n",
    "    \"\"\"検索結果ページから件数（整数）を抽出。例: <p class='m_item_txt'>10<span>件</span></p>\"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
    "                      \"(KHTML, like Gecko) Chrome/120.0 Safari/537.36\"\n",
    "    }\n",
    "    r = requests.get(url, headers=headers, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    node = soup.select_one(\"div.p-resultNum.mb-s p.m_item_txt\")\n",
    "    if not node:\n",
    "        raise ValueError(\"件数DOMが見つかりませんでした: div.p-resultNum.mb-s p.m_item_txt\")\n",
    "    text = node.get_text(strip=True)  # 例) \"10件\" / \"1,234件\"\n",
    "    m = re.search(r\"([\\d,]+)\", text)\n",
    "    if not m:\n",
    "        raise ValueError(f\"件数が抽出できませんでした: '{text}'\")\n",
    "    return int(m.group(1).replace(\",\", \"\"))\n",
    "\n",
    "def _load_meta_sheet(gs_url: str, sheet_name: str) -> pd.DataFrame:\n",
    "    \"\"\"_metaシートを DataFrame で取得（ym は 'YYYY/M/1' 扱いに統一）\"\"\"\n",
    "    ok = mg.open.sheet(gs_url)\n",
    "    if not ok:\n",
    "        raise RuntimeError(f\"Google Sheets を開けませんでした: {gs_url}\")\n",
    "\n",
    "    # 存在しない場合は空DFを返す（ヘッダ固定）\n",
    "    try:\n",
    "        mg.gs.sheet.select(sheet_name)\n",
    "        df = pd.DataFrame(mg.gs.sheet.data)\n",
    "    except Exception:\n",
    "        return pd.DataFrame(columns=[\"ym\", \"release_num\"])\n",
    "\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\"ym\", \"release_num\"])\n",
    "\n",
    "    # ヘッダ確認・補正\n",
    "    if \"ym\" not in df.columns or \"release_num\" not in df.columns:\n",
    "        # 1行目をヘッダとして再解釈（ズレ救済）\n",
    "        df = pd.DataFrame(df.iloc[1:].values, columns=df.iloc[0].tolist())\n",
    "        if \"ym\" not in df.columns or \"release_num\" not in df.columns:\n",
    "            return pd.DataFrame(columns=[\"ym\", \"release_num\"])\n",
    "\n",
    "    # 文字列化 & 不要スペース除去\n",
    "    df[\"ym\"] = df[\"ym\"].astype(str).str.strip()\n",
    "    df[\"release_num\"] = pd.to_numeric(df[\"release_num\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # もし '202410' など数値形式で入っていた行が混在しても、\n",
    "    # 'YYYY/M/1' に正規化（最終的に 'YYYY/M/1' で統一）\n",
    "    def _normalize_ym(v: str) -> str:\n",
    "        v = (v or \"\").strip()\n",
    "        # 既に 'YYYY/M/1' ならそのまま\n",
    "        if re.fullmatch(r\"\\d{4}/\\d{1,2}/1\", v):\n",
    "            return v\n",
    "        # 'YYYYMM' / 'YYYY-MM' 等のケースも救済\n",
    "        m = re.fullmatch(r\"(\\d{4})[-/]?(\\d{1,2})\", v)\n",
    "        if m:\n",
    "            y, mo = int(m.group(1)), int(m.group(2))\n",
    "            return f\"{y}/{mo}/1\"\n",
    "        # 'YYYYMMDD' 形式 → 月初へ\n",
    "        m2 = re.fullmatch(r\"(\\d{4})(\\d{2})(\\d{2})\", v)\n",
    "        if m2:\n",
    "            y, mo = int(m2.group(1)), int(m2.group(2))\n",
    "            return f\"{y}/{mo}/1\"\n",
    "        # 'YYYY/M/D' なら日付を 1 に\n",
    "        m3 = re.fullmatch(r\"(\\d{4})/(\\d{1,2})/(\\d{1,2})\", v)\n",
    "        if m3:\n",
    "            y, mo = int(m3.group(1)), int(m3.group(2))\n",
    "            return f\"{y}/{mo}/1\"\n",
    "        # どうにもならなければ空\n",
    "        return \"\"\n",
    "\n",
    "    df[\"ym\"] = df[\"ym\"].apply(_normalize_ym)\n",
    "    df = df[df[\"ym\"] != \"\"].copy().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def _upsert_count(df_meta: pd.DataFrame, ym_str: str, count: int):\n",
    "    \"\"\"ym='YYYY/M/1' ごとに upsert。既存と同値なら無変更。\"\"\"\n",
    "    action = \"no_change\"\n",
    "    if (df_meta[\"ym\"] == ym_str).any():\n",
    "        cur = df_meta.loc[df_meta[\"ym\"] == ym_str, \"release_num\"].iloc[0]\n",
    "        if pd.isna(cur) or int(cur) != int(count):\n",
    "            df_meta.loc[df_meta[\"ym\"] == ym_str, \"release_num\"] = int(count)\n",
    "            action = \"updated\"\n",
    "    else:\n",
    "        df_meta = pd.concat(\n",
    "            [df_meta, pd.DataFrame([{\"ym\": ym_str, \"release_num\": int(count)}])],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "        action = \"inserted\"\n",
    "\n",
    "    # 並べ替え（年月昇順）\n",
    "    def _key(v: str):\n",
    "        m = re.fullmatch(r\"(\\d{4})/(\\d{1,2})/1\", v)\n",
    "        return (int(m.group(1)), int(m.group(2))) if m else (9999, 99)\n",
    "\n",
    "    df_meta = df_meta.sort_values(\"ym\", key=lambda s: s.map(_key)).reset_index(drop=True)\n",
    "    df_meta[\"release_num\"] = df_meta[\"release_num\"].astype(\"Int64\")\n",
    "    return df_meta, action\n",
    "\n",
    "# === 実行 ===\n",
    "target_ym_str = _ym_slash(LM_From)\n",
    "url = _build_search_url(LM_From, LM___To)\n",
    "\n",
    "print(\"=== News Release 件数 取得 & 反映 ===\")\n",
    "print(f\"対象月(ym): {target_ym_str}\")\n",
    "print(f\"期間     : {LM_From} ～ {LM___To}\")\n",
    "print(f\"URL      : {url}\")\n",
    "\n",
    "try:\n",
    "    count = _fetch_release_count(url)\n",
    "    print(f\"抽出件数 : {count}\")\n",
    "\n",
    "    df_meta = _load_meta_sheet(GS_URL, Sheet)\n",
    "\n",
    "    before = None\n",
    "    if not df_meta.empty and (df_meta[\"ym\"] == target_ym_str).any():\n",
    "        before = df_meta.loc[df_meta[\"ym\"] == target_ym_str, \"release_num\"].iloc[0]\n",
    "\n",
    "    df_meta_updated, action = _upsert_count(df_meta, target_ym_str, count)\n",
    "\n",
    "    # 書き込み（過去データは保持）\n",
    "    mg.save.to.sheet(df=df_meta_updated, sheet_name=Sheet)\n",
    "\n",
    "    after = df_meta_updated.loc[df_meta_updated[\"ym\"] == target_ym_str, \"release_num\"].iloc[0]\n",
    "    if action == \"no_change\":\n",
    "        print(f\"結果     : 変更なし（既存 {int(after)} 件）\")\n",
    "    elif action == \"updated\":\n",
    "        print(f\"結果     : 更新（{int(before) if pd.notna(before) else 'NA'} → {int(after)} 件）\")\n",
    "    else:\n",
    "        print(f\"結果     : 新規追加（{int(after)} 件）\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"エラー   : {e}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "cellView": "form",
    "id": "8qr9JKxLFqZI",
    "outputId": "1d88c1f5-4667-4dab-a897-1f7f6986ba9d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhbvI2FkW1cB"
   },
   "source": [
    "## 02.入口からの遷移 BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wVMAnsxDpLvo",
    "outputId": "8b960ccb-294e-4788-f5db-ebc599b6103f"
   },
   "outputs": [],
   "source": [
    "#@title BQからヒットデータ取得\n",
    "date1 = LM_From  # \"2022-12-01\" などのフォーマット\n",
    "date2 = LM___To\n",
    "\n",
    "df_h = bq.run(query=f\"\"\"\n",
    "WITH T1 AS (\n",
    "  --元データ\n",
    "  SELECT\n",
    "    client_id,\n",
    "    session_number AS session_num,\n",
    "    date,\n",
    "    datetime,\n",
    "    --event_name,\n",
    "    IF(event_name=\"page_view\", page_path, NULL) AS page,\n",
    "    entrances,\n",
    "    IF(event_name=\"session_start\" AND session_number > 1, 1, 0) AS return,\n",
    "  FROM `analytics_334854563.flat`\n",
    "  WHERE debug_mode is NULL\n",
    "    AND \"{date1}\" <= date AND date <= \"{date2}\"\n",
    "    AND event_name IN (\"page_view\", \"session_start\")\n",
    "    AND hostname = \"corp.shiseido.com\"\n",
    "), T2 AS (\n",
    "  --アトリビューション\n",
    "  SELECT *,\n",
    "    MAX(return)\n",
    "      OVER(PARTITION BY client_id\n",
    "           ORDER BY datetime DESC\n",
    "           ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS returns,\n",
    "  FROM T1\n",
    "), T3 AS (\n",
    "  --page_viewのみ残して連番を入れる\n",
    "  SELECT * EXCEPT(datetime),\n",
    "    ROW_NUMBER() OVER(PARTITION BY client_id ORDER BY datetime) AS hit_num,\n",
    "  FROM T2\n",
    "  WHERE page is NOT NULL\n",
    "), T4 AS (\n",
    "  --離脱を判定\n",
    "  SELECT\n",
    "    * EXCEPT(return),\n",
    "    IF(hit_num=MAX(hit_num) OVER(PARTITION BY client_id,session_num ORDER BY hit_num DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW), 1, 0) AS exits,\n",
    "  FROM T3\n",
    ")\n",
    "--直帰を判定\n",
    "SELECT\n",
    "  date,\n",
    "  client_id,\n",
    "  session_num,\n",
    "  hit_num,\n",
    "  page,\n",
    "  CASE\n",
    "    WHEN REGEXP_CONTAINS(page, r'/(en|jp)/company/') THEN 'Company'\n",
    "    WHEN REGEXP_CONTAINS(page, r'/(en|jp)/brands/') THEN 'Brand'\n",
    "    WHEN REGEXP_CONTAINS(page, r'/(en|jp)/sustainability/') THEN 'Sustainability'\n",
    "    WHEN REGEXP_CONTAINS(page, r'/(en|jp)/rd/') THEN 'Innovation'\n",
    "    WHEN REGEXP_CONTAINS(page, r'/(en|jp)/careers/') THEN 'Careers'\n",
    "    WHEN REGEXP_CONTAINS(page, r'/(en|jp)/ir/') THEN 'Investors'\n",
    "    WHEN REGEXP_CONTAINS(page, r'/(en|jp)/news/') THEN 'News Releases'\n",
    "    WHEN REGEXP_CONTAINS(page, r'/(en|jp)/(americas|apac|china|emea|japan|tr)/') THEN 'Company'\n",
    "    WHEN REGEXP_CONTAINS(page, r'/(en|jp)/recruit/') THEN 'Careers'\n",
    "    WHEN REGEXP_CONTAINS(page, r'/(en|jp)/scp/careers/') THEN 'Careers'\n",
    "    WHEN REGEXP_CONTAINS(page, r'/(en|jp)/sports/') THEN 'Company'\n",
    "    WHEN REGEXP_CONTAINS(page, r'/sustainability') THEN 'Sustainability'\n",
    "    WHEN REGEXP_CONTAINS(page, r'/report/(en|jp)/') THEN 'Company'\n",
    "    WHEN REGEXP_CONTAINS(page, r'/150th/') THEN 'Company'\n",
    "    WHEN REGEXP_CONTAINS(page, r'/(en|jp)/$') THEN 'Top'\n",
    "  END AS contentGroup,\n",
    "  CASE\n",
    "    WHEN REGEXP_CONTAINS(page, '/jp/') THEN 'JP'\n",
    "    WHEN REGEXP_CONTAINS(page, '/en/') THEN 'EN'\n",
    "    WHEN REGEXP_CONTAINS(page, r'(/|\\.)cn/') THEN 'CN'\n",
    "    ELSE 'JP'\n",
    "  END AS language,\n",
    "  entrances,\n",
    "  exits,\n",
    "  IF(entrances=1 AND exits=1, 1, 0) AS bounces,\n",
    "  returns,\n",
    "FROM T4\n",
    "ORDER BY date,client_id,hit_num\n",
    "\n",
    "\"\"\", to_dataframe=True)\n",
    "df_h['ym'] = pd.to_datetime(df_h['date']).dt.strftime('%Y-%m-01')\n",
    "df_h['session_id'] = df_h['client_id'] + '_' + df_h['session_num'].astype(str)\n",
    "df_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fzc3fAcmpb_r",
    "outputId": "9706d3a3-dd24-4d97-b9fb-1442698e1278"
   },
   "outputs": [],
   "source": [
    "#@title 集計\n",
    "# 入口とcontentGroupがセットされた行に絞り、不要カラム除外\n",
    "df = df_h[df_h['entrances']==1&df_h['contentGroup'].notnull()][[\n",
    "    'ym','contentGroup','language','session_id','bounces','returns'\n",
    "]].groupby(['ym','contentGroup','language'], as_index=False).agg({'session_id': 'nunique', 'bounces': 'sum', 'returns': 'sum'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SFpoEskEqJjM",
    "outputId": "1d05e24b-8ab8-413b-81e0-6289527fe71a"
   },
   "outputs": [],
   "source": [
    "#@title 結果をGoogle Sheetsへ\n",
    "\n",
    "#@markdown 結果を書き込むGoogle Sheets\n",
    "Sheet = \"_entry\" #@param {type:\"string\"}\n",
    "#markdown シートを消去して上書き保存\n",
    "Overwrite = True #param {type:\"boolean\"}\n",
    "\n",
    "if mg.open.sheet(URL2):\n",
    "    if Overwrite:\n",
    "        mg.save.to.sheet(sheet_name=Sheet, df=df)  # 上書きする\n",
    "    else:\n",
    "        mg.append.to.sheet(sheet_name=Sheet)  # 追記する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24VMmzKPbjYS",
    "outputId": "960d09b6-cb4c-458f-9bed-f73109215598"
   },
   "outputs": [],
   "source": [
    "#@title コンテンツ到達\n",
    "mg.report.set_dates(LM_From, LM___To)\n",
    "\n",
    "def get_path(conf):\n",
    "    appended_data = []\n",
    "    for which, regex in conf:\n",
    "        filter_d = f\"eventName==page_view;landingPagePlusQueryString=~{regex}\" #;contentGroup!@not set\n",
    "        mg.report.run(\n",
    "            d=[\"year\", \"month\", \"contentGroup\", (\"customEvent:lang\", \"language\")],\n",
    "            m=[\"sessions\"],\n",
    "            filter_d=filter_d\n",
    "        )\n",
    "        _df = mg.report.data\n",
    "        _df[\"entry\"] = which\n",
    "        _df[\"ym\"] = _df[\"year\"].astype(str) + \"/\" + _df[\"month\"].astype(str) + \"/1\"\n",
    "        # store DataFrame in list\n",
    "        appended_data.append(_df)\n",
    "    return pd.concat(appended_data)[['ym', 'entry', 'language', 'contentGroup', 'sessions']]\n",
    "df = get_path(content)\n",
    "df = df.sort_values(by=['ym', 'entry', 'contentGroup'])\n",
    "\n",
    "#@markdown 結果を書き込むGoogle Sheets\n",
    "Sheet = \"_content\" #@param {type:\"string\"}\n",
    "#markdown シートを消去して上書き保存\n",
    "Overwrite = True #param {type:\"boolean\"}\n",
    "\n",
    "if mg.open.sheet(URL2):\n",
    "    if Overwrite:\n",
    "        mg.save.to.sheet(sheet_name=Sheet, df=df)  # 上書きする\n",
    "    else:\n",
    "        mg.append.to.sheet(sheet_name=Sheet)  # 追記する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "txU7-_QrW94r",
    "outputId": "e151b611-c3ce-4047-d240-c578cfb76fdf"
   },
   "outputs": [],
   "source": [
    "#@title 関連サイト到達\n",
    "\n",
    "def get_site(conf):\n",
    "    appended_data = []\n",
    "    for which, regex in conf:\n",
    "        mg.report.run(\n",
    "            d = [\"year\",\n",
    "                 \"month\",\n",
    "                 (\"customEvent:site\", \"site\"),\n",
    "                 (\"customEvent:lang\", \"language\")],\n",
    "            m = [\"sessions\"],\n",
    "            filter_d = f\"eventName==page_view;landingPagePlusQueryString=~{regex}\"\n",
    "        )\n",
    "        _df = mg.report.data\n",
    "        _df[\"entry\"] = which\n",
    "        _df[\"ym\"] = _df[\"year\"].astype(str) + \"/\" + _df[\"month\"].astype(str) + \"/1\"\n",
    "        # store DataFrame in list\n",
    "        appended_data.append(_df)\n",
    "    return pd.concat(appended_data)[['ym', 'entry', 'language', 'site', 'sessions']]\n",
    "df = get_site(content)\n",
    "df = df.sort_values(by=['ym', 'entry', 'language', 'site'])\n",
    "\n",
    "#@markdown 結果を書き込むGoogle Sheets\n",
    "Sheet = \"_site\" #@param {type:\"string\"}\n",
    "#markdown シートを消去して上書き保存\n",
    "Overwrite = True #param {type:\"boolean\"}\n",
    "\n",
    "if mg.open.sheet(URL2):\n",
    "    if Overwrite:\n",
    "        mg.save.to.sheet(sheet_name=Sheet, df=df[df['site']!='Company'])  # 上書きする\n",
    "    else:\n",
    "        mg.append.to.sheet(sheet_name=Sheet)  # 追記する\n",
    "\n",
    "#df#[df['site']!='Company']#.pivot(index='entry', columns='site', values='sessions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Z95zbY6XDT5",
    "outputId": "2bf9b462-1544-43b4-d168-7efae41635a0"
   },
   "outputs": [],
   "source": [
    "#@title CVアクション\n",
    "#@markdown （CVの定義が増えたら追加が必要）\n",
    "\n",
    "def get_entry(conf):\n",
    "    appended_data = []\n",
    "    for which, regex in conf:\n",
    "        filter_d = f\"landingPagePlusQueryString=~{regex};eventName=~click_www|click_brand|sns_click|pdf_click|contact_submit_click|job_apply_thanks|mail_subscribe_thanks|factory_reserve_thanks\"\n",
    "        mg.report.run(\n",
    "            d=[\"year\", \"month\", \"eventName\", (\"customEvent:lang\", \"language\")],\n",
    "            m=[\"sessions\"],\n",
    "            filter_d=filter_d\n",
    "        )\n",
    "        _df = mg.report.data\n",
    "        _df[\"entry\"] = which\n",
    "        _df[\"ym\"] = _df[\"year\"].astype(str) + \"/\" + _df[\"month\"].astype(str) + \"/1\"\n",
    "        # store DataFrame in list\n",
    "        appended_data.append(_df)\n",
    "    return pd.concat(appended_data)[['ym', 'entry', 'language', 'eventName', 'sessions']]\n",
    "df = get_entry(content)\n",
    "df = df.sort_values(by=['ym', 'entry', 'language', 'eventName'])\n",
    "\n",
    "#@markdown 結果を書き込むGoogle Sheets\n",
    "Sheet = \"_cv\" #@param {type:\"string\"}\n",
    "#markdown シートを消去して上書き保存\n",
    "Overwrite = True #param {type:\"boolean\"}\n",
    "\n",
    "if mg.open.sheet(URL2):\n",
    "    if Overwrite:\n",
    "        mg.save.to.sheet(sheet_name=Sheet, df=df)  # 上書きする\n",
    "    else:\n",
    "        mg.append.to.sheet(sheet_name=Sheet)  # 追記する\n",
    "\n",
    "# mg.gs.sheet.select(\"先月\")\n",
    "# mg.gs.sheet._driver.update_acell(\"E1\", LM_From)\n",
    "# mg.gs.sheet._driver.update_acell(\"Z1\", LM___To)\n",
    "update_sheets_cells({\"先月\": {\"E1\": LM_From, \"Z1\": LM___To}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMqsfW9wXH0w"
   },
   "source": [
    "## 03. ニュースリリース"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YeniWAeV6AoM",
    "outputId": "bf85d15e-448e-4b1e-bfd8-dcc4e32e440d"
   },
   "outputs": [],
   "source": [
    "#@title 日別データをBigQueryで抽出\n",
    "\n",
    "# 上のドロップダウンメニューで選択したプロパティを活かす場合\n",
    "GA4_TABLE = f\"analytics_{mg.ga['4'].property.id}\"\n",
    "\n",
    "# LM_From = (now.replace(day=1) - timedelta(days=1)).replace(day=1).strftime('%Y-%m-%d')\n",
    "# LM___To = (now.replace(day=1) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "date_from = LM_From.replace(\"-\", \"\")\n",
    "date_to = LM___To.replace(\"-\", \"\")\n",
    "\n",
    "df_pd = bq.run(query=f\"\"\"--from Colab\n",
    "WITH PV AS (\n",
    "    --元データとなるGA4のevent\n",
    "    SELECT\n",
    "        FORMAT_DATE('%Y-%m-%d', PARSE_DATE('%Y%m%d', event_date)) AS date,\n",
    "        (SELECT value.string_value FROM UNNEST(event_params) WHERE key='page_location') AS page_url,\n",
    "        --user_pseudo_id,\n",
    "        user_pseudo_id || '_' || (SELECT value.int_value FROM UNNEST(event_params) WHERE key='ga_session_id') AS session_id,\n",
    "        SUM(1) AS pv,\n",
    "    FROM `{GA4_TABLE}.events_*`\n",
    "    WHERE\n",
    "        event_name = 'page_view'\n",
    "        AND user_pseudo_id IS NOT NULL\n",
    "        AND '{date_from}' <= _TABLE_SUFFIX AND _TABLE_SUFFIX <= '{date_to}'\n",
    "    GROUP BY 1,2,3--,4\n",
    "    HAVING\n",
    "        REGEXP_CONTAINS(page_url, r'^https://corp\\.shiseido\\.com/\\w\\w/news/detail\\.html\\?.+')\n",
    "), PG AS (\n",
    "    --ページ単位で前処理\n",
    "    SELECT *,\n",
    "        'n=' || REGEXP_EXTRACT(page_url, r'n=(\\d+)') AS art_id,\n",
    "        REGEXP_EXTRACT(page_url, r'com/(\\w\\w)/news/detail') AS lang,\n",
    "    FROM PV\n",
    ")\n",
    "SELECT date, art_id, lang,\n",
    "    SUM(pv) AS pv,\n",
    "    COUNT(DISTINCT session_id) AS sessions,\n",
    "    --COUNT(DISTINCT user_pseudo_id) AS uu,\n",
    "FROM PG\n",
    "GROUP BY 1,2,3\n",
    "ORDER BY date, pv DESC\n",
    "\"\"\", to_dataframe=True)\n",
    "\n",
    "print(f\"期間：{date_from}〜{date_to}\")\n",
    "\n",
    "#@markdown 結果を書き込むGoogle Sheets\n",
    "Sheet = \"_page-d\" #@param {type:\"string\"}\n",
    "\n",
    "if mg.open.sheet(URL4):\n",
    "    mg.append.to.sheet(df=df_pd[df_pd['pv']>9], sheet_name=Sheet)\n",
    "\n",
    "#mg.show.table(df_pv, rows=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gtll7565ySA6",
    "outputId": "1aa73cd1-c834-405f-8551-055d48500cc9"
   },
   "outputs": [],
   "source": [
    "#@title 月別に集計\n",
    "\n",
    "df_pd['month'] = pd.to_datetime(df_pd['date']).dt.strftime('%Y-%m-01')\n",
    "\n",
    "df_pm = df_pd.groupby(['month','art_id','lang']).agg({'pv':sum, 'sessions':sum}).reset_index().sort_values(by='pv', ascending=False)\n",
    "\n",
    "#@markdown 結果を書き込むGoogle Sheets\n",
    "Sheet = \"_page-m\" #@param {type:\"string\"}\n",
    "\n",
    "if mg.open.sheet(URL4):\n",
    "    mg.append.to.sheet(df=df_pm[df_pm['pv']>9], sheet_name=Sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7dEO8vaeotwB",
    "outputId": "a7dc7105-2849-45ea-9b0f-dd3aa5a4c32a"
   },
   "outputs": [],
   "source": [
    "#@title ページ属性をクロールで取得\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#@markdown クロールするページの最大数\n",
    "Crawl_Page_Num = 50  #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown 公開日を取得するCSSセレクタ\n",
    "CSS_date = 'p.c-txt--date'  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown 日付のフォーマット（変換したい場合のみ記入）\n",
    "date_format = '%Y年%m月%d日'  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown カテゴリを取得するCSSセレクタ\n",
    "CSS_cat = 'p.c-category'  #@param {type:\"string\"}\n",
    "\n",
    "\n",
    "#@markdown 属性を保存するGoogle Sheetsシート名\n",
    "Sheet = \"_meta\" #@param {type:\"string\"}\n",
    "\n",
    "mg.select.sheet(Sheet)\n",
    "meta_data = pd.DataFrame(mg.gs.sheet.data)\n",
    "\n",
    "# GA4で記録されたpageを対象とする\n",
    "df_p = df_pm.groupby(['art_id','lang']).agg({'pv': sum}).reset_index().sort_values(by='pv', ascending=False)\n",
    "\n",
    "# 属性を取得してGSに保存されたpageを除外\n",
    "if isinstance(meta_data, pd.DataFrame) and len(meta_data)>0:\n",
    "    _df = pd.merge(\n",
    "        df_p,\n",
    "        meta_data,\n",
    "        on=['art_id', 'lang'], how='left')\n",
    "    df_meta = _df[_df['title'].isna()].sort_values(by='pv', ascending=False)[:Crawl_Page_Num]\n",
    "else:\n",
    "    print(\"first time\")\n",
    "    df_meta = df_p.sort_values(by='pv', ascending=False)[:Crawl_Page_Num]\n",
    "\n",
    "def crawl_and_get_metadata(row):\n",
    "    art_id = row['art_id']\n",
    "    lang = row['lang']\n",
    "    url = f\"https://corp.shiseido.com/{lang}/news/detail.html?{art_id}\"\n",
    "\n",
    "    print(f\"{row['pv']}. {url} - \", end='')\n",
    "\n",
    "    # クロールしてHTMLを取得\n",
    "    r = requests.get(url)\n",
    "    content_type_encoding = r.encoding if r.encoding != 'ISO-8859-1' else None\n",
    "    soup = BeautifulSoup(r.content, 'html.parser', from_encoding=content_type_encoding)\n",
    "\n",
    "    # ページタイトル\n",
    "    title = soup.title.text\n",
    "    # print(f\",{title} \", end='')\n",
    "\n",
    "    # 公開日\n",
    "    date = ''\n",
    "    if CSS_date != '':\n",
    "        try:\n",
    "            date = soup.select_one(CSS_date).get_text(strip=True)\n",
    "            if date and date_format != '':\n",
    "                date = datetime.strptime(date, date_format).strftime('%Y-%m-%d')\n",
    "            print(f\"{date} \", end='')\n",
    "        except AttributeError:\n",
    "            print(f\"(公開日取得不能)\", end='')\n",
    "        except ValueError:\n",
    "            print(f\"(公開日のフォーマットが {date_format} と不一致)\", end='')\n",
    "\n",
    "    # カテゴリ\n",
    "    cat = ''\n",
    "    if CSS_cat != '':\n",
    "        try:\n",
    "            tmp = [a.get_text(strip=True) for a in soup.select(CSS_cat)]\n",
    "            cat = ','.join(tmp)\n",
    "            if cat:\n",
    "                print(cat)\n",
    "            else:\n",
    "                print(f\"(カテゴリnot found)\", end='')\n",
    "        except:\n",
    "            print(f\"(カテゴリ取得不能)\", end='')\n",
    "\n",
    "    return title, cat, date\n",
    "\n",
    "# クロール\n",
    "df_meta[['title','tag','published']] = df_meta.apply(crawl_and_get_metadata, axis=1, result_type='expand')\n",
    "\n",
    "\n",
    "# Google Sheetsへ追記\n",
    "mg.append.to.sheet(df=df_meta.drop('pv', axis=1), sheet_name=Sheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCMXbofKXK8O"
   },
   "source": [
    "## 04. Shiseido Talks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239
    },
    "id": "0fC2s_RgdNDY",
    "outputId": "85f38258-057e-4986-a3d6-2de753cdce91"
   },
   "outputs": [],
   "source": [
    "#@title BQのデータを集計\n",
    "# 入口とcontentGroupがセットされた行に絞り、不要カラム除外\n",
    "df = df_h[df_h['entrances']==1&df_h['page'].str.contains(\"/company/talk/\")][[\n",
    "    'ym','page','session_id','bounces','returns'\n",
    "]].groupby(['ym','page'], as_index=False).agg({'session_id': 'nunique', 'bounces': 'sum', 'returns': 'sum'})\n",
    "\n",
    "mg.show.table(df, rows=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oy2q7cAjpfic",
    "outputId": "76da8d50-2ea7-47ab-91f6-fc60e44c94a5"
   },
   "outputs": [],
   "source": [
    "#@title 結果をGoogle Sheetsへ\n",
    "\n",
    "#@markdown 結果を書き込むGoogle Sheets\n",
    "Sheet = \"_entry\" #@param {type:\"string\"}\n",
    "#markdown シートを消去して上書き保存\n",
    "Overwrite = True #param {type:\"boolean\"}\n",
    "\n",
    "if mg.open.sheet(URL3):\n",
    "    if Overwrite:\n",
    "        mg.save.to.sheet(sheet_name=Sheet, df=df)  # 上書きする\n",
    "    else:\n",
    "        mg.append.to.sheet(sheet_name=Sheet)  # 追記する"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title 新記事の属性をクロールで取得\n",
    "\n",
    "import sys, subprocess, pandas as pd\n",
    "\n",
    "# --- Playwright を準備（未インストールならインストール & ブラウザ展開） ---\n",
    "try:\n",
    "    from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError\n",
    "except Exception:\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"playwright\"], check=True)\n",
    "    subprocess.run([sys.executable, \"-m\", \"playwright\", \"install\", \"chromium\"], check=True)\n",
    "    clear_output()\n",
    "    from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError\n",
    "\n",
    "# Ensure playwright dependencies are installed\n",
    "!playwright install-deps\n",
    "clear_output()\n",
    "\n",
    "async def scrape_talks_details_df(\n",
    "    url: str,\n",
    "    headless: bool = True,\n",
    "    timeout_ms: int = 20000,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrape talks cards:\n",
    "      <div class=\"swiper-slide talksList_item\">\n",
    "        <a id=\"jp_talk_YYYYMMDD\" href=\"/jp/company/talk/....html\">\n",
    "          <p class=\"details_date\">2025年8月21日</p>\n",
    "          <p class=\"details_type\">ブランド</p>\n",
    "          <p class=\"details_title\">…</p>\n",
    "        </a>\n",
    "      </div>\n",
    "\n",
    "    Returns DataFrame columns (DOM order, first occurrence kept):\n",
    "      - id:    anchor id (e.g., \"jp_talk_20250821\")\n",
    "      - href:  href path (e.g., \"/jp/company/talk/20250821.html\")\n",
    "      - title: text content of .details_title (collapsed whitespace)\n",
    "      - date:  \"YYYY/MM/DD\"\n",
    "      - tags:  array of tags (from .details_type; 1 item in current markup)\n",
    "    \"\"\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=headless, args=[\"--no-sandbox\"])\n",
    "        context = await browser.new_context(locale=\"ja-JP\", timezone_id=\"Asia/Tokyo\")\n",
    "        page = await context.new_page()\n",
    "        await page.goto(url, wait_until=\"domcontentloaded\")\n",
    "\n",
    "        try:\n",
    "            await page.wait_for_selector(\".talksList_item a[id^='jp_talk_']\", timeout=timeout_ms)\n",
    "        except PlaywrightTimeoutError:  # keep running even if late\n",
    "            pass\n",
    "\n",
    "        data = await page.evaluate(r\"\"\"\n",
    "        () => {\n",
    "          const pad = n => String(n).padStart(2, '0');\n",
    "          const toPath = href => {\n",
    "            try { return new URL(href, location.origin).pathname || ''; } catch { return href || ''; }\n",
    "          };\n",
    "          const nodeList = document.querySelectorAll(\".talksList_item a[id^='jp_talk_']\");\n",
    "          const seen = new Set();\n",
    "          const rows = [];\n",
    "          for (const a of nodeList) {\n",
    "            const id = a.getAttribute('id') || '';\n",
    "            const href = toPath(a.getAttribute('href') || '');\n",
    "            // date -> YYYY/MM/DD\n",
    "            const rawD = (a.querySelector('.details_date')?.textContent || '').trim();\n",
    "            let date = '';\n",
    "            const m = rawD.match(/(\\d{4})\\D+(\\d{1,2})\\D+(\\d{1,2})/);\n",
    "            if (m) date = `${m[1]}/${pad(m[2])}/${pad(m[3])}`;\n",
    "\n",
    "            // title (collapse whitespace, remove stray line breaks)\n",
    "            let title = (a.querySelector('.details_title')?.textContent || a.textContent || '')\n",
    "              .replace(/\\s+/g, ' ')\n",
    "              .trim();\n",
    "\n",
    "            // tags (if multiple ever appear, collect them)\n",
    "            const t = (a.querySelector('.details_type')?.textContent || '').trim();\n",
    "            const tags = t ? [t] : [];\n",
    "\n",
    "            if (id && !seen.has(id)) {\n",
    "              seen.add(id);\n",
    "              rows.push({ id, href, title, date, tags });\n",
    "            }\n",
    "          }\n",
    "          return rows;\n",
    "        }\n",
    "        \"\"\")\n",
    "        await browser.close()\n",
    "\n",
    "    df_talks = pd.DataFrame(data, columns=[\"id\", \"href\", \"title\", \"date\", \"tags\"]) if data \\\n",
    "               else pd.DataFrame(columns=[\"id\",\"href\",\"title\",\"date\",\"tags\"])\n",
    "    # Keep DOM order; if duplicates ever exist, keep first\n",
    "    df_talks = df_talks.drop_duplicates(subset=[\"id\"], keep=\"first\").reset_index(drop=True)\n",
    "    return df_talks\n",
    "\n",
    "TALK_URL = \"https://corp.shiseido.com/jp/\"\n",
    "df_talks = await scrape_talks_details_df(TALK_URL)\n",
    "print(f\"talks found: {len(df_talks)}\")\n",
    "\n",
    "mg.open.sheet(URL3)\n",
    "mg.gs.sheet.select(\"_meta\")\n",
    "existing_raw = mg.gs.sheet.data\n",
    "\n",
    "def _normalize_meta(obj) -> pd.DataFrame:\n",
    "    \"\"\"Coerce to DataFrame with columns: URL, Title, Language, Tag, Date.\"\"\"\n",
    "    expected = [\"URL\", \"Title\", \"Language\", \"Tag\", \"Date\"]\n",
    "    if isinstance(obj, pd.DataFrame):\n",
    "        df = obj.copy()\n",
    "    else:\n",
    "        try:\n",
    "            df = pd.DataFrame(obj)\n",
    "        except Exception:\n",
    "            df = pd.DataFrame()\n",
    "    if set(expected).issubset(df.columns):\n",
    "        df = df[expected]\n",
    "    elif df.shape[1] >= 5:\n",
    "        df = df.iloc[:, :5].copy()\n",
    "        df.columns = expected\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=expected)\n",
    "\n",
    "    # basic typing/cleanup\n",
    "    for c in [\"URL\", \"Title\", \"Language\", \"Tag\"]:\n",
    "        df[c] = df[c].astype(str)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "    df = df.dropna(subset=[\"URL\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "existing = _normalize_meta(existing_raw)\n",
    "\n",
    "# --- Build new rows from df_talks (expected: href, title, date 'YYYY/MM/DD', tags list) ---\n",
    "def talks_to_meta(df_talks: pd.DataFrame) -> pd.DataFrame:\n",
    "    expected = [\"URL\", \"Title\", \"Language\", \"Tag\", \"Date\"]\n",
    "    if df_talks is None or df_talks.empty:\n",
    "        return pd.DataFrame(columns=expected)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"URL\": df_talks[\"href\"].astype(str),\n",
    "        \"Title\": df_talks[\"title\"].astype(str).str.strip(),\n",
    "        \"Language\": \"JP\",\n",
    "        \"Tag\": df_talks.get(\"tags\", []).apply(\n",
    "            lambda x: (x[0] if isinstance(x, (list, tuple)) and len(x) else \"\")\n",
    "        ).astype(str),\n",
    "        \"Date\": pd.to_datetime(df_talks[\"date\"], errors=\"coerce\", format=\"%Y/%m/%d\").dt.strftime(\"%Y-%m-%d\"),\n",
    "    })\n",
    "    # de-dupe within incoming set (keep first/DOM order)\n",
    "    out = out.drop_duplicates(subset=[\"URL\"], keep=\"first\").reset_index(drop=True)\n",
    "    return out[expected]\n",
    "\n",
    "incoming = talks_to_meta(df_talks)\n",
    "\n",
    "# --- Keep only new URLs (don't overwrite existing rows) ---\n",
    "existing_urls = set(existing[\"URL\"]) if not existing.empty else set()\n",
    "to_add = incoming[~incoming[\"URL\"].isin(existing_urls)].copy()\n",
    "\n",
    "# --- Merge and sort: Language desc, then URL desc ---\n",
    "merged = pd.concat([existing, to_add], ignore_index=True)\n",
    "sorted_meta = merged.sort_values([\"Language\", \"URL\"], ascending=[False, False], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "# --- Save back to _meta ---\n",
    "mg.save.to.sheet(sheet_name=\"_meta\", df=sorted_meta)\n",
    "\n",
    "print(f\"✅ _meta updated: existing={len(existing)} + new={len(to_add)} → total={len(sorted_meta)} (sorted by Language desc, URL desc)\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "cellView": "form",
    "id": "mc31nqd5Qfn9",
    "outputId": "f45ebc64-7a19-4951-a9de-7c6575dc945a",
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UFQPdLFoqxlB",
    "outputId": "5b9c4068-7e39-4390-f61e-464d2f1241b3"
   },
   "outputs": [],
   "source": [
    "#@title コンテンツ到達\n",
    "mg.report.set_dates(LM_From, LM___To)\n",
    "\n",
    "filter_d = \"eventName==page_view;landingPagePlusQueryString=~/company/talk/\\d+;contentGroup!@not set;pagePath!~/company/talk/\\d+\"\n",
    "mg.report.run(\n",
    "    d=[\"year\", \"month\", (\"landingPagePlusQueryString\", \"entry\"), \"contentGroup\"],\n",
    "    m=[\"sessions\"],\n",
    "    filter_d=filter_d\n",
    ")\n",
    "_df = mg.report.data\n",
    "_df[\"ym\"] = _df[\"year\"].astype(str) + \"/\" + _df[\"month\"].astype(str) + \"/1\"\n",
    "\n",
    "df = _df[['entry', 'contentGroup', 'ym', 'sessions']]\n",
    "df = df.sort_values(by=['ym', 'entry', 'contentGroup'])\n",
    "\n",
    "#@markdown 結果を書き込むGoogle Sheets\n",
    "Sheet = \"_content\" #@param {type:\"string\"}\n",
    "#markdown シートを消去して上書き保存\n",
    "Overwrite = True #param {type:\"boolean\"}\n",
    "\n",
    "if mg.open.sheet(URL3):\n",
    "    if Overwrite:\n",
    "        mg.save.to.sheet(sheet_name=Sheet, df=df)  # 上書きする\n",
    "    else:\n",
    "        mg.append.to.sheet(sheet_name=Sheet)  # 追記する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TWQGYkZB2zng",
    "outputId": "1f004e83-f38e-44f9-da64-8c4fab183215"
   },
   "outputs": [],
   "source": [
    "#@title 関連サイト到達\n",
    "mg.report.set_dates(LM_From, LM___To)\n",
    "\n",
    "filter_d = \"eventName==page_view;landingPagePlusQueryString=~/company/talk/\\d+\"\n",
    "mg.report.run(\n",
    "    d=[\"year\", \"month\", (\"landingPagePlusQueryString\", \"entry\"), (\"customEvent:site\", \"site\")],\n",
    "    m=[\"sessions\"],\n",
    "    filter_d=filter_d\n",
    ")\n",
    "_df = mg.report.data\n",
    "_df[\"ym\"] = _df[\"year\"].astype(str) + \"/\" + _df[\"month\"].astype(str) + \"/1\"\n",
    "df = _df[['entry', 'site', 'ym', 'sessions']]\n",
    "df = df.sort_values(by=['ym', 'entry', 'site'])\n",
    "\n",
    "#@markdown 結果を書き込むGoogle Sheets\n",
    "Sheet = \"_site\" #@param {type:\"string\"}\n",
    "#markdown シートを消去して上書き保存\n",
    "Overwrite = True #param {type:\"boolean\"}\n",
    "\n",
    "if mg.open.sheet(URL3):\n",
    "    if Overwrite:\n",
    "        mg.save.to.sheet(sheet_name=Sheet, df=df[df['site']!='Company'])  # 上書きする\n",
    "    else:\n",
    "        mg.append.to.sheet(sheet_name=Sheet)  # 追記する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oY9dhmYSKS7P",
    "outputId": "51cb8943-0b26-4ec3-8543-1458e37242b1"
   },
   "outputs": [],
   "source": [
    "#@title CVアクション\n",
    "#@markdown （CVの定義が増えたら追加が必要）\n",
    "\n",
    "mg.report.set_dates(LM_From, LM___To)\n",
    "\n",
    "filter_d = \"landingPagePlusQueryString=~/company/talk/\\d+;eventName=~click_www|click_brand|sns_click|pdf_click|contact_submit_click|job_apply_thanks|mail_subscribe_thanks|factory_reserve_thanks\"\n",
    "mg.report.run(\n",
    "    d=[\"year\", \"month\", (\"landingPagePlusQueryString\", \"entry\"), \"eventName\"],\n",
    "    m=[\"sessions\"],\n",
    "    filter_d=filter_d\n",
    ")\n",
    "_df = mg.report.data\n",
    "_df[\"ym\"] = _df[\"year\"].astype(str) + \"/\" + _df[\"month\"].astype(str) + \"/1\"\n",
    "df = _df[['entry', 'eventName', 'ym', 'sessions']]\n",
    "df = df.sort_values(by=['ym', 'entry', 'eventName'])\n",
    "\n",
    "#@markdown 結果を書き込むGoogle Sheets\n",
    "Sheet = \"_cv\" #@param {type:\"string\"}\n",
    "#markdown シートを消去して上書き保存\n",
    "Overwrite = True #param {type:\"boolean\"}\n",
    "\n",
    "if mg.open.sheet(URL3):\n",
    "    if Overwrite:\n",
    "        mg.save.to.sheet(sheet_name=Sheet, df=df)  # 上書きする\n",
    "    else:\n",
    "        mg.append.to.sheet(sheet_name=Sheet)  # 追記する\n",
    "\n",
    "# mg.gs.sheet.select(\"先月\")\n",
    "# mg.gs.sheet._driver.update_acell(\"F1\", LM_From)\n",
    "# mg.gs.sheet._driver.update_acell(\"H1\", LM___To)\n",
    "update_sheets_cells({\"先月\": {\"F1\": LM_From, \"H1\": LM___To}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GGhPy3qGXPU7",
    "outputId": "f3bcf441-e51c-4ec8-8087-08a9c67ea268"
   },
   "outputs": [],
   "source": [
    "#@title Shiseido Talk：月別基本指標\n",
    "mg.report.set_dates(LM_From, LM___To)\n",
    "\n",
    "filter_d = \"pagePath=~/company/talk/\\d+;eventName==page_view\"\n",
    "mg.report.run(\n",
    "    d=[\"year\", \"month\", \"pagePath\"],\n",
    "    m=[\"sessions\", (\"eventCount\", \"pv\"), \"averageSessionDuration\"],\n",
    "    filter_d=filter_d,\n",
    "    sort=\"year,month,pagePath\"\n",
    ")\n",
    "_df = mg.report.data\n",
    "_df[\"ym\"] = _df[\"year\"].astype(str) + \"/\" + _df[\"month\"].astype(str) + \"/1\"\n",
    "_df[\"averageSessionDuration\"] = _df[\"averageSessionDuration\"].astype(float).round().apply(pd.to_timedelta, unit='s').astype(str).str[-5:]\n",
    "df = _df[['pagePath', 'ym', 'sessions', 'pv', \"averageSessionDuration\"]]\n",
    "\n",
    "#精読\n",
    "filter_d = \"pagePath=~/company/talk/\\d+;eventName==footer_view\"\n",
    "mg.report.run(\n",
    "    d=[\"year\", \"month\", \"pagePath\"],\n",
    "    m=[(\"sessions\", \"footer_view\")],\n",
    "    filter_d=filter_d,\n",
    "    sort=\"year,month,pagePath\"\n",
    ")\n",
    "_df = mg.report.data\n",
    "_df[\"ym\"] = _df[\"year\"].astype(str) + \"/\" + _df[\"month\"].astype(str) + \"/1\"\n",
    "df = df.merge(_df[['pagePath', 'ym', 'footer_view']],\n",
    "         on=['pagePath', 'ym'],\n",
    "         how='left'\n",
    ")[['pagePath', 'ym', 'sessions', 'pv', 'footer_view', 'averageSessionDuration']]\n",
    "\n",
    "#@markdown 結果を書き込むGoogle Sheets\n",
    "Sheet = \"_talk\" #@param {type:\"string\"}\n",
    "#markdown シートを消去して上書き保存\n",
    "Overwrite = True #param {type:\"boolean\"}\n",
    "\n",
    "if mg.open.sheet(URL3):\n",
    "    if Overwrite:\n",
    "        mg.save.to.sheet(sheet_name=Sheet, df=df)  # 上書きする\n",
    "    else:\n",
    "        mg.append.to.sheet(sheet_name=Sheet)  # 追記する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NMec6YvzXJo8",
    "outputId": "983b7762-05ed-40d9-bf03-cba2ac211442"
   },
   "outputs": [],
   "source": [
    "#@title Shiseido Talk：日別 未使用\n",
    "mg.report.set_dates(YR_From, TM___To)\n",
    "mg.report.run(\n",
    "    d=[\"date\", \"pagePath\"],\n",
    "    m=[\"sessions\"],\n",
    "    filter_d=\"eventName==page_view;pagePath=~/company/talk/\\d+\",\n",
    "    filter_m=\"eventCount>0.0\",\n",
    "    sort=\"date\"\n",
    ")\n",
    "\n",
    "#@title Google Sheetsへ出力\n",
    "\n",
    "Sheet = \"_talk_m-d\" #@param {type:\"string\"}\n",
    "#markdown シートを消去して上書き保存\n",
    "Overwrite = True #param {type:\"boolean\"}\n",
    "\n",
    "if mg.open.sheet(URL3):\n",
    "    if Overwrite:\n",
    "        mg.save.to.sheet(sheet_name=Sheet)  # 上書きする\n",
    "    else:\n",
    "        mg.append.to.sheet(sheet_name=Sheet)  # 追記する\n",
    "# mg.gs.sheet.select(\"Corp直近\")\n",
    "# mg.gs.sheet._driver.update(\"E1\", TM_From, value_input_option=\"USER_ENTERED\")\n",
    "# mg.gs.sheet._driver.update(\"G1\", TM___To, value_input_option=\"USER_ENTERED\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IyDGQaIQpIr"
   },
   "source": [
    "## 05. Shiseido Talks 経年"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iXudyKdqXpGX",
    "outputId": "f9ca36cd-ba5b-4f84-eeae-2a77f8e86338"
   },
   "outputs": [],
   "source": [
    "#@title 日別データをBigQueryで抽出\n",
    "\n",
    "# 上のドロップダウンメニューで選択したプロパティを活かす\n",
    "GA4_TABLE = f\"analytics_{mg.ga['4'].property.id}\"\n",
    "\n",
    "date_from = YR_From.replace(\"-\", \"\")\n",
    "date_to = LM___To.replace(\"-\", \"\")\n",
    "\n",
    "df_t = bq.run(query=f\"\"\"--from Colab\n",
    "WITH PV AS (\n",
    "    --元データとなるGA4のevent\n",
    "    SELECT\n",
    "        FORMAT_DATE('%Y-%m-%d', PARSE_DATE('%Y%m%d', event_date)) AS date,\n",
    "        (SELECT value.string_value FROM UNNEST(event_params) WHERE key='page_location') AS page_url,\n",
    "        --user_pseudo_id,\n",
    "        user_pseudo_id || '_' || (SELECT value.int_value FROM UNNEST(event_params) WHERE key='ga_session_id') AS session_id,\n",
    "        SUM(1) AS pv,\n",
    "    FROM `{GA4_TABLE}.events_*`\n",
    "    WHERE\n",
    "        event_name = 'page_view'\n",
    "        AND user_pseudo_id IS NOT NULL\n",
    "        AND '{date_from}' <= _TABLE_SUFFIX AND _TABLE_SUFFIX <= '{date_to}'\n",
    "    GROUP BY 1,2,3--,4\n",
    "    HAVING\n",
    "        REGEXP_CONTAINS(page_url, r'^https://corp\\.shiseido\\.com/\\w\\w/company/talk/[^\\.]+\\.html')\n",
    "), PG AS (\n",
    "    --ページ単位で前処理\n",
    "    SELECT *,\n",
    "        '' || REGEXP_EXTRACT(page_url, r'^https://corp\\.shiseido\\.com([^\\?]+)') AS page,\n",
    "        REGEXP_EXTRACT(page_url, r'com/(\\w\\w)/company/') AS lang,\n",
    "    FROM PV\n",
    ")\n",
    "SELECT date, page, lang,\n",
    "    SUM(pv) AS pv,\n",
    "    COUNT(DISTINCT session_id) AS sessions,\n",
    "    --COUNT(DISTINCT user_pseudo_id) AS uu,\n",
    "FROM PG\n",
    "GROUP BY 1,2,3\n",
    "ORDER BY date, pv DESC\n",
    "\"\"\", to_dataframe=True)\n",
    "\n",
    "print(f\"期間：{date_from}〜{date_to}\")\n",
    "\n",
    "#@markdown 結果を書き込むGoogle Sheets\n",
    "Sheet = \"_page-d\" #@param {type:\"string\"}\n",
    "\n",
    "if mg.open.sheet(URL3):\n",
    "    mg.save.to.sheet(df=df_t, sheet_name=Sheet)\n",
    "\n",
    "mg.show.table(df_t, rows=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oFTx54IQXXHt",
    "outputId": "06480362-d88b-4ccd-a796-afee74e94a16"
   },
   "outputs": [],
   "source": [
    "#@title 月別に集計\n",
    "\n",
    "df_t['month'] = pd.to_datetime(df_t['date']).dt.strftime('%Y-%m-01')\n",
    "\n",
    "df_tm = df_t.groupby(['month','page','lang']).agg({'pv':sum, 'sessions':sum}).reset_index().sort_values(by='pv', ascending=False)\n",
    "\n",
    "#@markdown 結果を書き込むGoogle Sheets\n",
    "Sheet = \"_page-m\" #@param {type:\"string\"}\n",
    "\n",
    "if mg.open.sheet(URL3):\n",
    "    mg.save.to.sheet(df=df_tm, sheet_name=Sheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEG8q5Q49dPk"
   },
   "source": [
    "## 06. Topクリック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "XM7QITlEc5vL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "809a9ada-2eb9-4536-c0be-8f68b100c52a"
   },
   "outputs": [],
   "source": [
    "#@title Corp：先月分を追記\n",
    "\n",
    "mg.report.set_dates(LM_From, LM___To)\n",
    "mg.report.run(\n",
    "    d = [\"linkId\",\"yearMonth\"],\n",
    "    m = [\n",
    "        # \"eventCount\",\n",
    "        \"sessions\",\n",
    "    ],\n",
    "    filter_d = \"eventName==top_content_click;linkId=~(^top.+|^hjp|^hen|_reports|_talk_|/notice/)\",\n",
    "    sort = \"-sessions\")\n",
    "\n",
    "#@markdown 結果を書き込むGoogle Sheets\n",
    "Sheet = \"_top_click\" #@param {type:\"string\"}\n",
    "\n",
    "if mg.open.sheet(URL6):\n",
    "    mg.append.to.sheet(sheet_name=Sheet)  # 上書きする\n",
    "    mg.gs.sheet.select(\"内訳\")\n",
    "    mg.gs.sheet._driver.update_acell(\"R2\", LM_From)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title IDとTextをクロールで取得\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "try:\n",
    "    from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError\n",
    "except ModuleNotFoundError:\n",
    "    %pip -q install playwright\n",
    "    !playwright install --with-deps chromium\n",
    "    from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError\n",
    "    clear_output()\n",
    "\n",
    "async def scrape_tags_to_df_playwright(\n",
    "    url: str,\n",
    "    wait_selector: str = \".topicsList_main a, .swiper-slide-duplicate a\",\n",
    "    headless: bool = True,\n",
    "    timeout_ms: int = 20000,\n",
    "    scroll_rounds: int = 0,\n",
    "    scroll_pause_ms: int = 800,\n",
    "    dedupe_by: str = \"id\",  # \"id\" or \"href\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Open URL with Playwright (Chromium), run JS to collect unique {id|href, text},\n",
    "    and return a DataFrame. Matches your original logic, with optional href-based dedupe.\n",
    "    \"\"\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=headless, args=[\"--no-sandbox\"])\n",
    "        context = await browser.new_context(\n",
    "            viewport={\"width\": 1400, \"height\": 900},\n",
    "            user_agent=(\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
    "                        \"(KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\"),\n",
    "            locale=\"ja-JP\",\n",
    "            timezone_id=\"Asia/Tokyo\",\n",
    "        )\n",
    "        page = await context.new_page()\n",
    "        await page.goto(url, wait_until=\"domcontentloaded\")\n",
    "\n",
    "        # Wait (don’t fail hard if slow)\n",
    "        try:\n",
    "            await page.wait_for_selector(wait_selector, timeout=timeout_ms)\n",
    "        except PlaywrightTimeoutError:\n",
    "            pass\n",
    "\n",
    "        # Optional lazy-load support\n",
    "        for _ in range(max(0, int(scroll_rounds))):\n",
    "            await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            await page.wait_for_timeout(scroll_pause_ms)\n",
    "\n",
    "        # JS in the page: same filtering/removal as your snippet\n",
    "        js = f\"\"\"\n",
    "        () => {{\n",
    "            const tags = document.querySelectorAll(\"{wait_selector}\");\n",
    "            const unique = [];\n",
    "            const seen = new Set();\n",
    "            const dynamicPattern = /^\\\\d{{4}}年\\\\d{{1,2}}月\\\\d{{1,2}}日\\\\n\\\\n[^\\\\n]+\\\\n\\\\n/;\n",
    "            tags.forEach(tag => {{\n",
    "                const key = {\"tag.getAttribute('id')\" if dedupe_by=='id' else \"tag.getAttribute('href')\"};\n",
    "                let text = (tag.innerText || \"\").trim();\n",
    "                text = text.replace(dynamicPattern, '').trim();\n",
    "                if (key && !seen.has(key)) {{\n",
    "                    unique.push({{ key, text }});\n",
    "                    seen.add(key);\n",
    "                }}\n",
    "            }});\n",
    "            return unique;\n",
    "        }}\n",
    "        \"\"\"\n",
    "        data = await page.evaluate(js)\n",
    "        await browser.close()\n",
    "\n",
    "    if not data:\n",
    "        cols = [\"id\", \"text\"] if dedupe_by == \"id\" else [\"href\", \"text\"]\n",
    "        return pd.DataFrame(columns=cols)\n",
    "\n",
    "    df = pd.DataFrame(data)  # columns: ['key','text']\n",
    "    if dedupe_by == \"id\":\n",
    "        df = df.rename(columns={\"key\": \"id\"})[[\"id\", \"text\"]]\n",
    "    else:\n",
    "        df = df.rename(columns={\"key\": \"href\"})[[\"href\", \"text\"]]\n",
    "    return df\n",
    "\n",
    "async def scrape_notice_labels_df(\n",
    "    url: str,\n",
    "    headless: bool = True,\n",
    "    timeout_ms: int = 15000\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrape .section_body .item a.wrap:\n",
    "      <span class=\"date\">YYYY/MM/DD</span>\n",
    "      <span class=\"label\">LABEL</span>\n",
    "    Returns DataFrame with columns:\n",
    "      id   = href's path (e.g., \"/jp/notice/20240704.html\")\n",
    "      text = \"YYYY/MM/DD　LABEL\"  (full-width space between)\n",
    "    \"\"\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=headless, args=[\"--no-sandbox\"])\n",
    "        context = await browser.new_context(locale=\"ja-JP\", timezone_id=\"Asia/Tokyo\")\n",
    "        page = await context.new_page()\n",
    "        await page.goto(url, wait_until=\"domcontentloaded\")\n",
    "\n",
    "        try:\n",
    "            await page.wait_for_selector(\".section_body .item a.wrap\", timeout=timeout_ms)\n",
    "        except PlaywrightTimeoutError:\n",
    "            pass\n",
    "\n",
    "        items = await page.evaluate(\"\"\"\n",
    "        () => {\n",
    "          const as = Array.from(document.querySelectorAll('.section_body .item a.wrap'));\n",
    "          return as.map(a => ({\n",
    "            href: a.getAttribute('href') || '',\n",
    "            date: (a.querySelector('.date')?.textContent || '').trim(),\n",
    "            label: (a.querySelector('.label')?.textContent || a.textContent || '').trim()\n",
    "          }));\n",
    "        }\n",
    "        \"\"\")\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    def ensure_path(href: str) -> str:\n",
    "        p = urlparse(href)\n",
    "        path = p.path or \"\"\n",
    "        if not path and href:\n",
    "            # relative without leading slash (rare) -> treat as path\n",
    "            path = href if href.startswith(\"/\") else f\"/{href}\"\n",
    "        return path\n",
    "\n",
    "    def fmt_date(date_str: str, path: str) -> str:\n",
    "        ds = (date_str or \"\").strip()\n",
    "        # Accept exact YYYY/MM/DD\n",
    "        if re.fullmatch(r\"\\d{4}/\\d{2}/\\d{2}\", ds):\n",
    "            return ds\n",
    "        # Fallback from path like /jp/notice/20240704.html\n",
    "        m = re.search(r\"(\\d{4})(\\d{2})(\\d{2})\", path or \"\")\n",
    "        if m:\n",
    "            return f\"{m.group(1)}/{m.group(2)}/{m.group(3)}\"\n",
    "        return ds  # may be \"\"\n",
    "\n",
    "    records = []\n",
    "    for it in (items or []):\n",
    "        path = ensure_path(it.get(\"href\", \"\"))\n",
    "        if not path:\n",
    "            continue\n",
    "        date_part = fmt_date(it.get(\"date\", \"\"), path)\n",
    "        label = (it.get(\"label\", \"\") or \"\").strip()\n",
    "        # Build \"YYYY/MM/DD　LABEL\" (U+3000 full-width space). If date missing, just label.\n",
    "        text = f\"{date_part}　{label}\".strip() if date_part else label\n",
    "        if text:\n",
    "            records.append({\"id\": path, \"text\": text})\n",
    "\n",
    "    df_notice = pd.DataFrame(records, columns=[\"id\", \"text\"]).drop_duplicates(subset=[\"id\"], keep=\"last\")\n",
    "    return df_notice\n",
    "\n",
    "#@markdown 結果を書き込むGoogle Sheets\n",
    "Sheet = \"_link\" #@param {type:\"string\"}\n",
    "\n",
    "URL = \"https://corp.shiseido.com/jp/\"\n",
    "\n",
    "df_news = await scrape_tags_to_df_playwright(URL, scroll_rounds=2)\n",
    "df_notice = await scrape_notice_labels_df(URL)\n",
    "df = pd.concat([df_news, df_notice], ignore_index=True)\n",
    "# df = df.drop_duplicates(subset=[\"id\"], keep=\"first\")\n",
    "\n",
    "print(f\"Added notices: {len(df_notice)}  |  df total now: {len(df)}\")\n",
    "df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "cellView": "form",
    "id": "inE8ptDIqBhY",
    "outputId": "903fa5af-8102-49be-c6dd-918dcc5d2465"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title GSに差分を追記\n",
    "\n",
    "if mg.open.sheet(URL6):\n",
    "    mg.gs.sheet.select(Sheet)\n",
    "    existing_df = pd.DataFrame(mg.gs.sheet.data)\n",
    "\n",
    "def _norm2cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ensure exactly ['id','text']; strip; drop blank id; drop dup by id (keep last).\"\"\"\n",
    "    out = df[[\"id\", \"text\"]].copy()\n",
    "    out[\"id\"] = out[\"id\"].astype(str).str.strip()\n",
    "    out[\"text\"] = out[\"text\"].astype(str).str.strip()\n",
    "    out = out[out[\"id\"] != \"\"]\n",
    "    out = out.drop_duplicates(subset=[\"id\"], keep=\"last\").reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "new_df = _norm2cols(df)\n",
    "\n",
    "if existing_df is None or len(existing_df) == 0:\n",
    "    merged = new_df.copy()\n",
    "    n_new, n_updated = len(merged), 0\n",
    "else:\n",
    "    ex = _norm2cols(existing_df)\n",
    "\n",
    "    # 更新/新規のカウント用に既存マップ\n",
    "    ex_map = dict(zip(ex[\"id\"], ex[\"text\"]))\n",
    "\n",
    "    n_new = sum(k not in ex_map for k in new_df[\"id\"])\n",
    "    n_updated = sum((k in ex_map) and (ex_map[k] != t) for k, t in zip(new_df[\"id\"], new_df[\"text\"]))\n",
    "\n",
    "    # 既存 + 新規 を結合 → id 重複を keep='last' で解決（最後が勝つ）\n",
    "    merged = pd.concat([ex, new_df], ignore_index=True)\n",
    "    merged = merged.drop_duplicates(subset=[\"id\"], keep=\"last\").reset_index(drop=True)\n",
    "\n",
    "# ---- save back ----\n",
    "mg.save.to.sheet(df=merged.sort_values(by='id'), sheet_name=Sheet)\n",
    "\n",
    "print(f\"✅ Merge complete → new: {n_new}, updated: {n_updated}, total: {len(merged)}\")\n",
    "# merged.sort_values(by='id')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "cellView": "form",
    "id": "NT-bcvWousXN",
    "outputId": "b05e9b56-9b5c-49d6-81c7-a609dec550a4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title 内訳シートにWhat's NewのIDを記入\n",
    "\n",
    "# --- 1) hjp_* from df (keep original order, dedupe keep first) ---\n",
    "hjp_df = df[df[\"id\"].astype(str).str.startswith(\"hjp_\")].copy()\n",
    "ids_df_ordered = hjp_df[\"id\"].astype(str).drop_duplicates(keep=\"first\").tolist()\n",
    "ids_df_set = set(ids_df_ordered)\n",
    "\n",
    "# --- 2) hjp_* from merged (sort desc, then dedupe in that order), exclude ids already in df ---\n",
    "hjp_merged = merged[merged[\"id\"].astype(str).str.startswith(\"hjp_\")].copy()\n",
    "ids_merged_desc = (\n",
    "    hjp_merged[\"id\"].astype(str)\n",
    "    .sort_values(ascending=False)          # sort only merged side\n",
    "    .drop_duplicates(keep=\"first\")         # dedupe in the sorted order\n",
    "    .tolist()\n",
    ")\n",
    "ids_extra = [x for x in ids_merged_desc if x not in ids_df_set]\n",
    "\n",
    "\n",
    "# --- 3) Build final list for B5..B25 (21 rows) ---\n",
    "TOTAL = 21\n",
    "final_ids = (ids_df_ordered + ids_extra)[:TOTAL]\n",
    "final_ids += [\"\"] * (TOTAL - len(final_ids))\n",
    "\n",
    "# --- 4) Write to Google Sheets \"内訳\" B5:B25 ---\n",
    "mg.gs.sheet.select(\"内訳\")\n",
    "for idx, val in enumerate(final_ids, start=5):  # rows 5..25\n",
    "    mg.gs.sheet._driver.update_acell(f\"B{idx}\", val)\n",
    "\n",
    "print(f\"✅ Wrote hjp_* ids to 内訳!B5:B25 (df order kept, merged sorted desc). Rows: {len(final_ids)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "cellView": "form",
    "id": "K2s1QTWvC5Qw",
    "outputId": "6878c268-d9ae-40ce-b2c9-bb5a8426a794"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title 内訳シートにTalksのIDを記入\n",
    "\n",
    "# 1) Filter jp_talk rows and sort desc\n",
    "talk = df[df[\"id\"].astype(str).str.startswith(\"jp_talk\")].copy()\n",
    "\n",
    "talk = talk.sort_values([\"id\"], ascending=[False])\n",
    "top_ids = talk[\"id\"].drop_duplicates().head(5).tolist()\n",
    "\n",
    "# pad to 5 rows\n",
    "while len(top_ids) < 5:\n",
    "    top_ids.append(\"\")\n",
    "\n",
    "# 2) Write to Google Sheets \"内訳\" B30:B34\n",
    "mg.gs.sheet.select(\"内訳\")\n",
    "start_row = 30\n",
    "for i, val in enumerate(top_ids, start=0):\n",
    "    cell = f\"B{start_row + i}\"\n",
    "    mg.gs.sheet._driver.update_acell(cell, val)\n",
    "\n",
    "print(\"✅ Wrote jp_talk ids to 内訳!B30:B34:\", top_ids)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "cellView": "form",
    "id": "zvLUkT6XAFDA",
    "outputId": "c27ad859-a5dd-40a2-931b-014d40d827ec"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title 内訳シートにお知らせのIDを記入\n",
    "\n",
    "# 1) Filter /jp/notice/ rows and sort desc (by id as-is)\n",
    "notice = df[df[\"id\"].astype(str).str.startswith(\"/jp/notice/\")].copy()\n",
    "notice[\"text\"] = notice[\"text\"].astype(str)\n",
    "notice = notice.sort_values([\"text\", \"id\"], ascending=[False, False], kind=\"mergesort\")\n",
    "\n",
    "# keep first occurrence after sort (highest text), then take top 6 ids\n",
    "top_notice_ids = notice.drop_duplicates(subset=[\"id\"], keep=\"first\")[\"id\"].head(6).tolist()\n",
    "\n",
    "# pad to 6 rows\n",
    "while len(top_notice_ids) < 6:\n",
    "    top_notice_ids.append(\"\")\n",
    "\n",
    "# 2) Write to Google Sheets \"内訳\" B36:B41\n",
    "mg.gs.sheet.select(\"内訳\")\n",
    "start_row = 36\n",
    "for i, val in enumerate(top_notice_ids):\n",
    "    cell = f\"B{start_row + i}\"\n",
    "    mg.gs.sheet._driver.update_acell(cell, val)\n",
    "\n",
    "print(\"✅ Wrote notice ids to 内訳!B36:B41:\", top_notice_ids)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "cellView": "form",
    "id": "_mXUE2wcBWFz",
    "outputId": "65f69c1d-8469-4150-ca46-94f6b0443532"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RImFEt_rcFf0"
   },
   "source": [
    "# Search Consoleデータ抽出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S_OMHD3b1QN7",
    "outputId": "8041daf1-776b-49ed-ff9f-72d9fe613586"
   },
   "outputs": [],
   "source": [
    "#@title GSCからデータを取得して加工\n",
    "\n",
    "try:\n",
    "    import jaconv\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -q jaconv\n",
    "    import jaconv\n",
    "    from IPython.display import clear_output\n",
    "    clear_output()\n",
    "\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "import logging\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "# Enable asyncio for Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Connect to Google Search Console\n",
    "def connect_to_gsc(credentials_path, scopes=None):\n",
    "    \"\"\"\n",
    "    Establishes a connection to Google Search Console.\n",
    "    \"\"\"\n",
    "    scopes = scopes or ['https://www.googleapis.com/auth/webmasters']\n",
    "    if 'https://www.googleapis.com/auth/webmasters' not in scopes:\n",
    "        scopes.append('https://www.googleapis.com/auth/webmasters')\n",
    "    try:\n",
    "        credentials = service_account.Credentials.from_service_account_file(credentials_path, scopes=scopes)\n",
    "        return build('searchconsole', 'v1', credentials=credentials)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to connect to GSC: {e}\")\n",
    "        raise\n",
    "\n",
    "# Execute a GSC query\n",
    "def execute_gsc_query(service, site_url, payload):\n",
    "    \"\"\"\n",
    "    Executes a query to the GSC API and returns results as a DataFrame.\n",
    "    \"\"\"\n",
    "    results, start_row = [], 0\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            payload[\"startRow\"] = start_row\n",
    "            response = service.searchanalytics().query(siteUrl=site_url, body=payload).execute()\n",
    "            rows = response.get('rows', [])\n",
    "            if not rows:\n",
    "                break\n",
    "            results.extend(rows)\n",
    "            start_row += len(rows)\n",
    "\n",
    "        # Process rows into a DataFrame\n",
    "        dimensions = payload[\"dimensions\"]\n",
    "        data = [\n",
    "            {**{dim: row['keys'][i] for i, dim in enumerate(dimensions)},\n",
    "             \"clicks\": row.get(\"clicks\", 0),\n",
    "             \"impressions\": row.get(\"impressions\", 0)}\n",
    "            for row in results\n",
    "        ]\n",
    "        return pd.DataFrame(data)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during API call: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Group similar queries with normalization\n",
    "def group_similar_queries(df, query_column=\"query\", metric_columns=None, additional_dimensions=None, normalize_columns=None):\n",
    "    \"\"\"\n",
    "    Groups similar queries by normalizing and aggregating metrics.\n",
    "    Includes normalization of specified columns.\n",
    "    \"\"\"\n",
    "    if query_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{query_column}' not found in the DataFrame.\")\n",
    "\n",
    "    # Normalize specified columns\n",
    "    normalize_columns = normalize_columns or [query_column]\n",
    "    for column in normalize_columns:\n",
    "        if column in df.columns:\n",
    "            df[column] = df[column].apply(lambda x: jaconv.h2z(x, kana=True) if pd.notnull(x) else x)\n",
    "            df[column] = df[column].apply(lambda x: jaconv.z2h(x, kana=False, ascii=True, digit=True) if pd.notnull(x) else x)\n",
    "\n",
    "    # Metric and additional dimensions setup\n",
    "    metric_columns = metric_columns or df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    additional_dimensions = additional_dimensions or [col for col in df.columns if col not in metric_columns + [query_column]]\n",
    "\n",
    "    # Normalize queries for grouping\n",
    "    def normalize_query(query):\n",
    "        return \"\".join(sorted(query.replace(\" \", \"\").replace(\"　\", \"\"))) if pd.notnull(query) else query\n",
    "\n",
    "    df[\"normalized_query\"] = df[query_column].apply(normalize_query)\n",
    "\n",
    "    # Aggregate metrics\n",
    "    agg_rules = {query_column: \"first\", **{col: \"sum\" for col in metric_columns}}\n",
    "    grouped = df.groupby([\"normalized_query\"] + additional_dimensions, as_index=False).agg(agg_rules)\n",
    "    grouped_sorted = grouped.drop(columns=[\"normalized_query\"]).sort_values(by=metric_columns, ascending=False)\n",
    "    # return grouped.drop(columns=[\"normalized_query\"])\n",
    "    return grouped_sorted\n",
    "\n",
    "# Fetch GSC data with combined filter patterns and dimensions\n",
    "async def parallel_query_web_gsc(credentials_path, site_url, filters, start_date, end_date, dimensions, scopes=None):\n",
    "    \"\"\"\n",
    "    Fetches GSC data for multiple filters and dimensions in parallel.\n",
    "\n",
    "    Args:\n",
    "        filters (list of dict): Each filter contains \"pattern\" and \"dimension\".\n",
    "    \"\"\"\n",
    "    async def fetch_data(filter_config):\n",
    "        payload = {\n",
    "            'startDate': start_date,\n",
    "            'endDate': end_date,\n",
    "            'dimensions': dimensions,\n",
    "            'type': 'web',\n",
    "            \"dimensionFilterGroups\": [\n",
    "                {\"filters\": [{\"dimension\": filter_config[\"dimension\"], \"operator\": \"includingRegex\", \"expression\": filter_config[\"pattern\"]}]}\n",
    "            ],\n",
    "            \"rowLimit\": 25000\n",
    "        }\n",
    "        service = connect_to_gsc(credentials_path, scopes)\n",
    "        return execute_gsc_query(service, site_url, payload)\n",
    "\n",
    "    tasks = [fetch_data(filter_config) for filter_config in filters]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return pd.concat(results, ignore_index=True) if results else pd.DataFrame()\n",
    "\n",
    "# Main function\n",
    "async def main():\n",
    "    global df_jp, df_en, df_t\n",
    "\n",
    "    # Japanese data\n",
    "    filters_jp = [{\"pattern\": \"/jp/\", \"dimension\": \"page\"}, {\"pattern\": \"/sbs/\", \"dimension\": \"page\"}, {\"pattern\": \"/scp/\", \"dimension\": \"page\"}]\n",
    "    df_jp_raw = await parallel_query_web_gsc(CREDS_PATH, GSC_PROPERTY, filters_jp, LM_From, LM___To, [\"query\"], mg.creds.scopes)\n",
    "    df_jp = group_similar_queries(df_jp_raw, query_column=\"query\", metric_columns=[\"clicks\", \"impressions\"], normalize_columns=[\"query\"])\n",
    "    df_jp['ctr'] = df_jp['clicks'] / df_jp['impressions']\n",
    "    logger.info(f\"Japanese data contains {len(df_jp)} rows.\")\n",
    "\n",
    "    # English data\n",
    "    filters_en = [{\"pattern\": \"/en/\", \"dimension\": \"page\"}]\n",
    "    df_en_raw = await parallel_query_web_gsc(CREDS_PATH, GSC_PROPERTY, filters_en, LM_From, LM___To, [\"query\"], mg.creds.scopes)\n",
    "    df_en = group_similar_queries(df_en_raw, query_column=\"query\", metric_columns=[\"clicks\", \"impressions\"], normalize_columns=[\"query\"])\n",
    "    df_en['ctr'] = df_en['clicks'] / df_en['impressions']\n",
    "    logger.info(f\"English data contains {len(df_en)} rows.\")\n",
    "\n",
    "    # Shiseido Talks\n",
    "    filters_t = [{\"pattern\": \"/jp/company/talk/2\", \"dimension\": \"page\"}]\n",
    "    df_raw = await parallel_query_web_gsc(CREDS_PATH, GSC_PROPERTY, filters_t, LM_From, LM___To, ['query', 'page'], mg.creds.scopes)\n",
    "    df_t = group_similar_queries(df_raw, query_column=\"query\", metric_columns=[\"clicks\", \"impressions\"], normalize_columns=[\"query\"])\n",
    "    df_t['ctr'] = df_t['clicks'] / df_t['impressions']\n",
    "    logger.info(f\"Shiseido Talks data contains {len(df_t)} rows.\")\n",
    "\n",
    "print(f\"対象期間：{LM_From} - {LM___To}\")\n",
    "\n",
    "GSC_PROPERTY = \"https://corp.shiseido.com\"\n",
    "\n",
    "await main()\n",
    "\n",
    "# Display results\n",
    "# df_jp[df_jp['clicks'] > 0]\n",
    "# df_en[df_en['clicks'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NPN1Jblli-U7",
    "outputId": "91c0baf4-e2e2-4f64-81f9-60a0df757160"
   },
   "outputs": [],
   "source": [
    "#@title 結果をGoogle Sheetsへ保存\n",
    "LM_From_YYYYMM = get_past_date(n_months=1, first_or_last='first', date_format='%Y%m')\n",
    "LLM_From_YYYYMM = get_past_date(n_months=2, first_or_last='first', date_format='%Y%m')\n",
    "\n",
    "def duplicate_sheet___alreadyDefined(source_sheet_title, new_sheet_name, cell_update=None):\n",
    "    \"\"\"\n",
    "    Duplicates a Google Sheets worksheet and updates a specific cell in the new sheet.\n",
    "\n",
    "    Args:\n",
    "        source_sheet_title (str): The title of the worksheet to duplicate.\n",
    "        new_sheet_name (str): The title for the new duplicated worksheet.\n",
    "        cell_update (dict, optional): A dictionary specifying a cell to update, e.g., {\"cell\": \"B1\", \"value\": \"some_value\"}.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get the source worksheet\n",
    "        source_worksheet = mg.gs._driver.worksheet(source_sheet_title)\n",
    "\n",
    "        # Duplicate the worksheet\n",
    "        mg.gs._driver.duplicate_sheet(\n",
    "            source_worksheet.id,\n",
    "            new_sheet_name=new_sheet_name\n",
    "        )\n",
    "        logger.info(f\"Worksheet '{new_sheet_name}' duplicated from '{source_sheet_title}'.\")\n",
    "\n",
    "        # Update a specific cell in the duplicated sheet if specified\n",
    "        if cell_update:\n",
    "            mg.gs.sheet._driver.update_acell(cell_update[\"cell\"], cell_update[\"value\"])\n",
    "            logger.info(f\"Cell '{cell_update['cell']}' updated with value '{cell_update['value']}' in '{new_sheet_name}'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error duplicating worksheet '{source_sheet_title}' to '{new_sheet_name}': {e}\")\n",
    "\n",
    "def save_to_google_sheet_OLD(df, sheet_title, sort_by=\"clicks\"):\n",
    "    \"\"\"\n",
    "    Saves data to a Google Sheets worksheet. Creates the worksheet if it does not exist.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The data to save.\n",
    "        sheet_title (str): The title of the worksheet for saving data.\n",
    "        sort_by (str, optional): Column to sort the DataFrame by (default: None).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Sort the DataFrame if specified\n",
    "        if sort_by:\n",
    "            df = df.sort_values(by=sort_by, ascending=False)\n",
    "\n",
    "        # Check if worksheet exists\n",
    "        try:\n",
    "            mg.gs._driver.worksheet(sheet_title)\n",
    "            logger.info(f\"Worksheet '{sheet_title}' already exists. Skipping creation.\")\n",
    "        except Exception:\n",
    "            # Create worksheet if it does not exist\n",
    "            mg.gs._driver.add_worksheet(title=sheet_title, rows=10, cols=10)\n",
    "            logger.info(f\"Worksheet '{sheet_title}' created.\")\n",
    "\n",
    "        # Save data to the sheet\n",
    "        mg.save.to.sheet(df=df, sheet_name=sheet_title)\n",
    "        logger.info(f\"Data saved to worksheet '{sheet_title}'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving to Google Sheets: {e}\")\n",
    "\n",
    "def save_to_google_sheet___alreadyDefined(gs_url, sheet_name, df, sort_by=None):\n",
    "    \"\"\"\n",
    "    Saves a DataFrame to a Google Sheet. If the sheet exists, it clears the existing data;\n",
    "    otherwise, it creates a new sheet. Adjusts column widths based on the data and freezes the first row.\n",
    "\n",
    "    Args:\n",
    "        gs_url (str): The URL of the Google Sheets document.\n",
    "        sheet_name (str): The name of the sheet to save the data.\n",
    "        df (pd.DataFrame): The DataFrame to save.\n",
    "    \"\"\"\n",
    "    def calculate_pixel_size(value, single_byte_multiplier=7, multi_byte_multiplier=14):\n",
    "        \"\"\"\n",
    "        Calculates the pixel size for a given value, accounting for multi-byte characters.\n",
    "\n",
    "        Args:\n",
    "            value (str): The string value to calculate the size for.\n",
    "            single_byte_multiplier (int): Width multiplier for single-byte characters.\n",
    "            multi_byte_multiplier (int): Width multiplier for multi-byte characters.\n",
    "\n",
    "        Returns:\n",
    "            int: The calculated pixel size.\n",
    "        \"\"\"\n",
    "        total_width = 0\n",
    "        for char in str(value):\n",
    "            if ord(char) < 128:  # Single-byte character\n",
    "                total_width += single_byte_multiplier\n",
    "            else:  # Multi-byte character\n",
    "                total_width += multi_byte_multiplier\n",
    "        return total_width\n",
    "\n",
    "    # Sort the DataFrame if specified\n",
    "    if sort_by:\n",
    "        df = df.sort_values(by=sort_by, ascending=False)\n",
    "\n",
    "    if mg.open.sheet(gs_url):\n",
    "        try:\n",
    "            # Try to create a new sheet\n",
    "            mg.gs._driver.add_worksheet(title=sheet_name, rows=10, cols=10)\n",
    "        except Exception:\n",
    "            # If the sheet already exists, select it and clear the data\n",
    "            mg.gs.sheet.select(sheet_name)\n",
    "            mg.gs.sheet.clear()\n",
    "\n",
    "        # Save the DataFrame to the sheet\n",
    "        mg.save.to.sheet(df=df, sheet_name=sheet_name)\n",
    "\n",
    "        # Get the sheet object and its ID\n",
    "        sheet = mg.gs._driver.worksheet(sheet_name)\n",
    "        sheet_id = sheet.id\n",
    "\n",
    "        # Calculate column widths\n",
    "        column_widths = []\n",
    "        for col_name in df.columns:\n",
    "            max_length = max(\n",
    "                df[col_name].astype(str).map(\n",
    "                    lambda x: calculate_pixel_size(x)\n",
    "                ).max(),\n",
    "                calculate_pixel_size(col_name)\n",
    "            )\n",
    "            pixel_size = max(min(max_length, 500), 50)  # Minimum width: 50, Maximum width: 500\n",
    "            column_widths.append(pixel_size)\n",
    "\n",
    "        # Prepare batch update requests for column resizing and freezing the first row\n",
    "        requests = [\n",
    "            # Column resizing\n",
    "            {\n",
    "                \"updateDimensionProperties\": {\n",
    "                    \"range\": {\n",
    "                        \"sheetId\": sheet_id,\n",
    "                        \"dimension\": \"COLUMNS\",\n",
    "                        \"startIndex\": i,\n",
    "                        \"endIndex\": i + 1\n",
    "                    },\n",
    "                    \"properties\": {\"pixelSize\": column_widths[i]},\n",
    "                    \"fields\": \"pixelSize\"\n",
    "                }\n",
    "            }\n",
    "            for i in range(len(column_widths))\n",
    "        ]\n",
    "\n",
    "        # Add request to freeze the first row\n",
    "        requests.append({\n",
    "            \"updateSheetProperties\": {\n",
    "                \"properties\": {\n",
    "                    \"sheetId\": sheet_id,\n",
    "                    \"gridProperties\": {\"frozenRowCount\": 1}\n",
    "                },\n",
    "                \"fields\": \"gridProperties.frozenRowCount\"\n",
    "            }\n",
    "        })\n",
    "\n",
    "        # Execute batch update\n",
    "        mg.gs._driver.batch_update({\"requests\": requests})\n",
    "        print(f\"Data successfully saved to the sheet: {sheet_name}, column widths adjusted, and first row frozen.\")\n",
    "    else:\n",
    "        print(\"Failed to open the Google Sheets document.\")\n",
    "\n",
    "#if mg.open.sheet(URL5):\n",
    "# Japanese data\n",
    "save_to_google_sheet(\n",
    "    gs_url=URL5,\n",
    "    sheet_name=f'{LM_From_YYYYMM}jp',\n",
    "    df=df_jp[df_jp['clicks'] > 0],\n",
    "    sort_by=\"clicks\"\n",
    ")\n",
    "duplicate_sheet(\n",
    "    source_sheet_title=LLM_From_YYYYMM,\n",
    "    new_sheet_name=f'{LM_From_YYYYMM}',\n",
    "    cell_update={\"cell\": \"B1\", \"value\": LM_From_YYYYMM}\n",
    ")\n",
    "\n",
    "# English data\n",
    "save_to_google_sheet(\n",
    "    gs_url=URL5,\n",
    "    sheet_name=f'{LM_From_YYYYMM}en',\n",
    "    df=df_en[df_en['clicks'] > 0],\n",
    "    sort_by=\"clicks\"\n",
    ")\n",
    "\n",
    "# Japanese data without '資生堂' or 'shiseido'\n",
    "filtered_jp = df_jp[(df_jp['clicks'] > 0) & (~df_jp['query'].str.contains('資生堂|shiseido', case=False, na=False))]\n",
    "save_to_google_sheet(\n",
    "    gs_url=URL5,\n",
    "    sheet_name=f'{LM_From_YYYYMM}jp_nb',\n",
    "    df=filtered_jp,\n",
    "    sort_by=\"clicks\"\n",
    ")\n",
    "duplicate_sheet(\n",
    "    source_sheet_title=f\"{LLM_From_YYYYMM} nb\",\n",
    "    new_sheet_name=f\"{LM_From_YYYYMM} nb\",\n",
    "    cell_update={\"cell\": \"B1\", \"value\": f\"{LM_From_YYYYMM} nb\"}\n",
    ")\n",
    "\n",
    "# English data without '資生堂' or 'shiseido'\n",
    "filtered_en = df_en[(df_en['clicks'] > 0) & (~df_en['query'].str.contains('資生堂|shiseido', case=False, na=False))]\n",
    "save_to_google_sheet(\n",
    "    gs_url=URL5,\n",
    "    sheet_name=f'{LM_From_YYYYMM}en_nb',\n",
    "    df=filtered_en,\n",
    "    sort_by=\"clicks\"\n",
    ")\n",
    "\n",
    "# cell_updateが動いていない？\n",
    "if mg.open.sheet(URL5):\n",
    "    update_sheets_cells({\n",
    "        LM_From_YYYYMM: {\"B1\": LM_From_YYYYMM},\n",
    "        f\"{LM_From_YYYYMM} nb\": {\"B1\": LM_From_YYYYMM},\n",
    "    })\n",
    "\n",
    "# Shiseido Talks\n",
    "save_to_google_sheet(\n",
    "    gs_url=URL3,\n",
    "    sheet_name=f'SC_{LM_From_YYYYMM}',\n",
    "    df=df_t[df_t['clicks'] > 0],\n",
    "    sort_by=\"clicks\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 以下は不要"
   ],
   "metadata": {
    "id": "vySTE8jsKaHx"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 以下は分類を実験中"
   ],
   "metadata": {
    "id": "A16Jncy4zhL_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#@title ③ SERP共起グラフでクエリ自動クラスタ\n",
    "# 依存: Cell1, Cell2（df_jp, parallel_query_web_gsc, mg, LM_From/LM___To, GSC_PROPERTY など）\n",
    "# 出力: serp_nodes_df, serp_edges_df, serp_clusters_df, serp_graph（保存はしない）\n",
    "\n",
    "GS_PREVIEW_ROWS = 15  # 表示件数\n",
    "\n",
    "MIN_CO = 3           # 同一ページを共有する回数の最小値\n",
    "MIN_JACCARD = 0.08   # Jaccard の最小値\n",
    "MAX_Q_PER_PAGE = 30  # 1ページ当たり上位Nクエリで共起を作る（計算削減）\n",
    "\n",
    "try:\n",
    "    import networkx as nx\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -q networkx\n",
    "    import networkx as nx\n",
    "    clear_output()\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "async def build_serp_graph_only():\n",
    "    # 1) 同期間で query×page を取得（Cell2の関数を再利用）\n",
    "    filters_jp_qp = [\n",
    "        {\"pattern\": \"/jp/\", \"dimension\": \"page\"},\n",
    "        {\"pattern\": \"/sbs/\", \"dimension\": \"page\"},\n",
    "        {\"pattern\": \"/scp/\", \"dimension\": \"page\"},\n",
    "    ]\n",
    "    df_qp = await parallel_query_web_gsc(\n",
    "        CREDS_PATH, GSC_PROPERTY,\n",
    "        filters_jp_qp, LM_From, LM___To,\n",
    "        [\"query\", \"page\"], mg.creds.scopes\n",
    "    )\n",
    "    if df_qp.empty:\n",
    "        print(\"query×page データが空です。条件（期間/フィルタ）をご確認ください。\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # df_jp に存在するクエリに絞る（ノイズ削減）\n",
    "    if \"query\" in df_jp.columns:\n",
    "        df_qp = df_qp[df_qp[\"query\"].isin(df_jp[\"query\"])].copy()\n",
    "    if df_qp.empty:\n",
    "        print(\"df_jp と一致するクエリがありませんでした。\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # 型整形\n",
    "    for col in (\"clicks\", \"impressions\"):\n",
    "        if col in df_qp.columns:\n",
    "            df_qp[col] = pd.to_numeric(df_qp[col], errors=\"coerce\").fillna(0).astype(int)\n",
    "        else:\n",
    "            df_qp[col] = 0\n",
    "\n",
    "    # 2) ページごとの上位クエリ（clicks順）から共起ペアを作成\n",
    "    df_qp_sorted = df_qp.sort_values([\"page\", \"clicks\"], ascending=[True, False])\n",
    "    pairs = []\n",
    "    for page, sub in df_qp_sorted.groupby(\"page\"):\n",
    "        qs = sub[\"query\"].head(int(MAX_Q_PER_PAGE)).dropna().unique()\n",
    "        if len(qs) >= 2:\n",
    "            pairs.extend(itertools.combinations(qs, 2))\n",
    "\n",
    "    if not pairs:\n",
    "        print(\"共起ペアが生成されませんでした。MAX_Q_PER_PAGE やフィルタを見直してください。\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    df_pairs = pd.DataFrame(pairs, columns=[\"q1\", \"q2\"])\n",
    "    df_pairs[\"co\"] = 1\n",
    "    df_pairs = df_pairs.groupby([\"q1\", \"q2\"], as_index=False)[\"co\"].sum()\n",
    "\n",
    "    # 3) 各クエリの「ページ出現数」を計算 → Jaccard\n",
    "    deg = (df_qp.groupby(\"query\")[\"page\"]\n",
    "           .nunique().reset_index().rename(columns={\"page\": \"deg\"}))\n",
    "    df_pairs = (\n",
    "        df_pairs\n",
    "        .merge(deg.rename(columns={\"query\": \"q1\", \"deg\": \"deg1\"}), on=\"q1\", how=\"left\")\n",
    "        .merge(deg.rename(columns={\"query\": \"q2\", \"deg\": \"deg2\"}), on=\"q2\", how=\"left\")\n",
    "    )\n",
    "    df_pairs[\"jaccard\"] = df_pairs[\"co\"] / (df_pairs[\"deg1\"] + df_pairs[\"deg2\"] - df_pairs[\"co\"])\n",
    "\n",
    "    # しきい値でエッジをフィルタ\n",
    "    edges = df_pairs[(df_pairs[\"co\"] >= MIN_CO) & (df_pairs[\"jaccard\"] >= MIN_JACCARD)].copy()\n",
    "    if edges.empty:\n",
    "        print(\"しきい値によりエッジが残りませんでした。MIN_CO / MIN_JACCARD を下げて再実行してください。\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # 4) グラフ構築\n",
    "    G = nx.Graph()\n",
    "    nodes_set = set(edges[\"q1\"]).union(set(edges[\"q2\"]))\n",
    "\n",
    "    # ノード属性（df_jpのメトリクスを付与）\n",
    "    jp_metrics = df_jp.set_index(\"query\")[[\"clicks\", \"impressions\", \"ctr\"]]\n",
    "    for q in nodes_set:\n",
    "        if q in jp_metrics.index:\n",
    "            clicks = float(jp_metrics.at[q, \"clicks\"]) if not pd.isna(jp_metrics.at[q, \"clicks\"]) else None\n",
    "            imps   = float(jp_metrics.at[q, \"impressions\"]) if not pd.isna(jp_metrics.at[q, \"impressions\"]) else None\n",
    "            ctr    = float(jp_metrics.at[q, \"ctr\"]) if not pd.isna(jp_metrics.at[q, \"ctr\"]) else None\n",
    "        else:\n",
    "            clicks = imps = ctr = None\n",
    "        G.add_node(q, clicks=clicks, impressions=imps, ctr=ctr)\n",
    "\n",
    "    for _, r in edges.iterrows():\n",
    "        G.add_edge(r[\"q1\"], r[\"q2\"], co=int(r[\"co\"]), jaccard=float(r[\"jaccard\"]))\n",
    "\n",
    "    # 5) クラスタリング（Louvain優先、無ければGreedy）\n",
    "    try:\n",
    "        from networkx.algorithms.community import louvain_communities\n",
    "        comms = louvain_communities(G, weight=\"jaccard\", seed=42)\n",
    "    except Exception:\n",
    "        from networkx.algorithms.community import greedy_modularity_communities\n",
    "        comms = list(greedy_modularity_communities(G, weight=\"jaccard\"))\n",
    "\n",
    "    q2cid = {}\n",
    "    for cid, com in enumerate(comms):\n",
    "        for q in com:\n",
    "            q2cid[q] = cid\n",
    "\n",
    "    # 6) ディレクトリからクラスタ名を付与\n",
    "    def section_from_url(u: str) -> str:\n",
    "        try:\n",
    "            p = urlparse(u).path\n",
    "        except Exception:\n",
    "            p = str(u)\n",
    "        parts = [s for s in p.split(\"/\") if s]\n",
    "        if len(parts) >= 2:\n",
    "            return f\"/{parts[0]}/{parts[1]}/\"\n",
    "        elif len(parts) == 1:\n",
    "            return f\"/{parts[0]}/\"\n",
    "        return \"/\"\n",
    "\n",
    "    sub_qp = df_qp[df_qp[\"query\"].isin(nodes_set)].copy()\n",
    "    sub_qp[\"section\"] = sub_qp[\"page\"].map(section_from_url)\n",
    "\n",
    "    # 各クエリの代表セクション（最頻）\n",
    "    sec_map = (sub_qp.groupby([\"query\", \"section\"])[\"page\"]\n",
    "               .nunique().reset_index(name=\"cnt\")\n",
    "               .sort_values([\"query\", \"cnt\"], ascending=[True, False])\n",
    "               .drop_duplicates(\"query\")\n",
    "               .set_index(\"query\")[\"section\"].to_dict())\n",
    "\n",
    "    # ノード表\n",
    "    nodes_df = pd.DataFrame({\"query\": list(nodes_set)})\n",
    "    nodes_df[\"cluster\"] = nodes_df[\"query\"].map(q2cid).astype(\"Int64\")\n",
    "    nodes_df = nodes_df.merge(df_jp[[\"query\", \"clicks\", \"impressions\", \"ctr\"]], on=\"query\", how=\"left\")\n",
    "    nodes_df[\"degree\"] = nodes_df[\"query\"].map(dict(G.degree())).astype(\"Int64\")\n",
    "    nodes_df[\"section\"] = nodes_df[\"query\"].map(sec_map)\n",
    "    clabel = (nodes_df.groupby(\"cluster\")[\"section\"]\n",
    "              .agg(lambda s: pd.Series.mode(s.dropna()).iat[0] if s.dropna().size else None)\n",
    "              .to_dict())\n",
    "    nodes_df[\"cluster_label\"] = nodes_df[\"cluster\"].map(clabel)\n",
    "\n",
    "    # エッジ表\n",
    "    edges_df = pd.DataFrame(\n",
    "        [(u, v, d.get(\"co\", 0), d.get(\"jaccard\", 0.0)) for u, v, d in G.edges(data=True)],\n",
    "        columns=[\"q1\", \"q2\", \"co\", \"jaccard\"]\n",
    "    )\n",
    "    edges_df[\"cluster1\"] = edges_df[\"q1\"].map(q2cid).astype(\"Int64\")\n",
    "    edges_df[\"cluster2\"] = edges_df[\"q2\"].map(q2cid).astype(\"Int64\")\n",
    "\n",
    "    # クラスタ要約\n",
    "    clust_df = (nodes_df.groupby([\"cluster\", \"cluster_label\"])\n",
    "                .agg(n_queries=(\"query\", \"nunique\"),\n",
    "                     clicks=(\"clicks\", \"sum\"),\n",
    "                     impressions=(\"impressions\", \"sum\"),\n",
    "                     ctr=(\"ctr\", \"mean\"),\n",
    "                     degree_avg=(\"degree\", \"mean\"))\n",
    "                .reset_index()\n",
    "                .sort_values([\"clicks\", \"n_queries\"], ascending=[False, False]))\n",
    "\n",
    "    return nodes_df, edges_df, clust_df, G\n",
    "\n",
    "# 実行（計算のみ）\n",
    "serp_nodes_df, serp_edges_df, serp_clusters_df, serp_graph = await build_serp_graph_only()\n",
    "\n",
    "# プレビュー表示（保存なし）\n",
    "if serp_nodes_df is not None:\n",
    "    print(f\"Graph: {serp_graph.number_of_nodes()} nodes, {serp_graph.number_of_edges()} edges\")\n",
    "    display(serp_clusters_df.head(GS_PREVIEW_ROWS))\n",
    "    print(\"— nodes preview —\")\n",
    "    display(serp_nodes_df.sort_values([\"cluster\", \"degree\"], ascending=[True, False]).head(GS_PREVIEW_ROWS))\n",
    "    print(\"— edges preview —\")\n",
    "    display(serp_edges_df.sort_values([\"co\", \"jaccard\"], ascending=[False, False]).head(GS_PREVIEW_ROWS))\n",
    "else:\n",
    "    print(\"計算結果が空のため、保存対象はありません。\")"
   ],
   "metadata": {
    "cellView": "form",
    "id": "gdpPfJo8r4Ka"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title ④ SERPクラスタ結果をGoogle Sheetsへ保存\n",
    "\n",
    "GS_URL = URL8\n",
    "SHEET_NODES = \"_serp_nodes\"     #@param {type:\"string\"}\n",
    "SHEET_EDGES = \"_serp_edges\"     #@param {type:\"string\"}\n",
    "SHEET_CLUST = \"_serp_clusters\"  #@param {type:\"string\"}\n",
    "\n",
    "# 存在チェック\n",
    "for name in [\"serp_nodes_df\", \"serp_edges_df\", \"serp_clusters_df\"]:\n",
    "    if name not in globals() or globals()[name] is None or globals()[name].empty:\n",
    "        raise RuntimeError(f\"{name} が未生成または空です。Cell 3 を先に実行してください。\")\n",
    "\n",
    "# 保存\n",
    "mg.open.sheet(GS_URL)\n",
    "save_to_google_sheet(GS_URL, SHEET_NODES, serp_nodes_df.sort_values([\"cluster\", \"degree\"], ascending=[True, False]))\n",
    "save_to_google_sheet(GS_URL, SHEET_EDGES, serp_edges_df.sort_values([\"co\", \"jaccard\"], ascending=[False, False]))\n",
    "save_to_google_sheet(GS_URL, SHEET_CLUST, serp_clusters_df)\n",
    "\n",
    "print(\"✅ 保存完了:\", GS_URL, SHEET_NODES, SHEET_EDGES, SHEET_CLUST)\n"
   ],
   "metadata": {
    "cellView": "form",
    "id": "m3CtkwqU0l9g"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "B_syy1LSItEF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title ⑥ Need（クエリ起点）分類 & 指標化（保存なし）\n",
    "# 依存: Cell 1, 2, 3（parallel_query_web_gsc, LM_From/LM___To, GSC_PROPERTY など）\n",
    "# 出力: need_overview_df, need_priority_df, need_ctr_fixes_df, need_new_topics_df, need_link_plan_df\n",
    "\n",
    "# ========== パラメータ ==========\n",
    "# クラスタ（＝Need）作成\n",
    "MAX_QUERIES = 20000       # 対象クエリが多すぎる場合は imp 上位で打ち切り\n",
    "MIN_CLUSTER_SIZE = None   # 既定: データ件数から自動設定（下で決定）\n",
    "LABEL_TOPN = 3            # Need名として出す上位語数（c-TF-IDF）\n",
    "INCLUDE_NOISE = False     # HDBSCANのノイズ(-1)をNeedに含めるか\n",
    "\n",
    "# CTR改善／新規トピック／リンク案 関連\n",
    "CTR_GAP = 0.05            # Need平均CTRより5pp低いページを改善候補に\n",
    "MIN_IMP_QUANTILE = 0.70   # Need内 imp 上位30%を対象\n",
    "CLICK_LOW = 3             # クリックがこの値未満なら「Low Click」\n",
    "NEW_TOPIC_IMP_Q = 0.70    # 新規トピック候補: imp 上位30%\n",
    "\n",
    "# カバレッジ判定（任意）\n",
    "TH_QPERPAGE = 12\n",
    "TH_TOP3SHARE = 0.55\n",
    "TH_HHI_FRAGMENT = 0.18\n",
    "\n",
    "# 変更不可ディレクトリ（ページ提案時に除外）\n",
    "IMMUTABLE_SECTIONS = [\"/jp/ir/\", \"/jp/careers/\", \"/art-house/jp/nadoto/\"]\n",
    "\n",
    "# ========== 追加ライブラリ（未インストール時のみ） ==========\n",
    "try:\n",
    "    import hdbscan\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "except ModuleNotFoundError:\n",
    "    %pip -q install sentence-transformers hdbscan scikit-learn\n",
    "    import hdbscan\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from IPython.display import clear_output\n",
    "    clear_output()\n",
    "\n",
    "import pandas as pd, numpy as np, re, unicodedata\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# ========== ユーティリティ ==========\n",
    "def section_from_url(u: str) -> str:\n",
    "    try:\n",
    "        p = urlparse(u).path\n",
    "    except Exception:\n",
    "        p = str(u)\n",
    "    parts = [s for s in p.split(\"/\") if s]\n",
    "    if len(parts) >= 2:  return f\"/{parts[0]}/{parts[1]}/\"\n",
    "    if len(parts) == 1:  return f\"/{parts[0]}/\"\n",
    "    return \"/\"\n",
    "\n",
    "def is_immutable_page(url: str) -> bool:\n",
    "    if not isinstance(url, str): return False\n",
    "    sec = section_from_url(url)\n",
    "    return any(sec.startswith(x) for x in IMMUTABLE_SECTIONS if x)\n",
    "\n",
    "def normalize_query(q: str) -> str:\n",
    "    if pd.isna(q): return \"\"\n",
    "    s = unicodedata.normalize(\"NFKC\", str(q)).lower().strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "# ========== データ取得（query×page：現月と先月） ==========\n",
    "filters = [{\"pattern\": p, \"dimension\": \"page\"} for p in [\"/jp/\", \"/sbs/\", \"/scp/\"]]\n",
    "\n",
    "# 現月\n",
    "df_qp = await parallel_query_web_gsc(CREDS_PATH, GSC_PROPERTY, filters, LM_From, LM___To, [\"query\",\"page\"], mg.creds.scopes)\n",
    "# 先月（MoM用）\n",
    "PREV_From = get_past_date(n_months=2, first_or_last='first')\n",
    "PREV___To = get_past_date(n_months=2, first_or_last='last')\n",
    "df_qp_prev = await parallel_query_web_gsc(CREDS_PATH, GSC_PROPERTY, filters, PREV_From, PREV___To, [\"query\",\"page\"], mg.creds.scopes)\n",
    "\n",
    "for _df in [df_qp, df_qp_prev]:\n",
    "    for col in (\"clicks\",\"impressions\"):\n",
    "        _df[col] = pd.to_numeric(_df.get(col, 0), errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# ========== 1) クエリをクラスタリングして Need（=クエリ起点トピック）を作成 ==========\n",
    "# 対象クエリ（impの大きい順に抽出）\n",
    "q_agg = (df_qp.groupby(\"query\")[[\"clicks\",\"impressions\"]].sum()\n",
    "               .reset_index()\n",
    "               .sort_values(\"impressions\", ascending=False))\n",
    "if len(q_agg) > MAX_QUERIES:\n",
    "    q_agg = q_agg.head(MAX_QUERIES).copy()\n",
    "\n",
    "q_agg[\"q_norm\"] = q_agg[\"query\"].map(normalize_query)\n",
    "\n",
    "# 埋め込み → HDBSCAN\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "emb = model.encode(q_agg[\"q_norm\"].tolist(), normalize_embeddings=True, show_progress_bar=False)\n",
    "\n",
    "if MIN_CLUSTER_SIZE is None:\n",
    "    MIN_CLUSTER_SIZE = max(15, len(q_agg)//200)  # データ規模に合わせ自動調整\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=MIN_CLUSTER_SIZE, metric=\"euclidean\",\n",
    "                            cluster_selection_method=\"eom\")\n",
    "labels = clusterer.fit_predict(emb)\n",
    "q_agg[\"need_id\"] = labels\n",
    "\n",
    "# ノイズの扱い\n",
    "if not INCLUDE_NOISE:\n",
    "    q_agg = q_agg[q_agg[\"need_id\"] != -1].copy()\n",
    "\n",
    "# Need名（c-TF-IDFで代表語抽出：日本語は char n-gram が安定）\n",
    "def ctfi_labels(df, text_col, group_col, topn=LABEL_TOPN):\n",
    "    docs = (df.groupby(group_col)[text_col]\n",
    "              .apply(lambda s: \" \".join(s.tolist()))\n",
    "              .reset_index())\n",
    "    vec = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,5), min_df=2)\n",
    "    X = vec.fit_transform(docs[text_col])\n",
    "    vocab = np.array(vec.get_feature_names_out())\n",
    "    labels_map = {}\n",
    "    for i, need in enumerate(docs[group_col].tolist()):\n",
    "        row = X.getrow(i).toarray().ravel()\n",
    "        idx = row.argsort()[::-1][:topn]\n",
    "        terms = [vocab[j] for j in idx if row[j] > 0]\n",
    "        label = \"/\".join(terms) if terms else f\"Need_{need}\"\n",
    "        labels_map[need] = label\n",
    "    return labels_map\n",
    "\n",
    "need_labels = ctfi_labels(q_agg, \"q_norm\", \"need_id\", LABEL_TOPN)\n",
    "q_agg[\"need\"] = q_agg[\"need_id\"].map(need_labels)\n",
    "\n",
    "# 全クエリに Need を付与（現月・先月）\n",
    "q2need = dict(zip(q_agg[\"query\"], q_agg[\"need\"]))\n",
    "df_qp[\"need\"] = df_qp[\"query\"].map(q2need)\n",
    "df_qp_prev[\"need\"] = df_qp_prev[\"query\"].map(q2need)  # 現月のNeedに投影（未知クエリは欠損）\n",
    "\n",
    "# ========== 2) Need別のKPI/カバレッジ/HHIと MoM ==========\n",
    "def aggregate_need_from_qp(qp: pd.DataFrame) -> pd.DataFrame:\n",
    "    qp = qp.dropna(subset=[\"need\"]).copy()\n",
    "    clicks_p = qp.groupby([\"need\",\"page\"])[\"clicks\"].sum().reset_index()\n",
    "    clicks_q = qp.groupby([\"need\",\"query\"])[\"clicks\"].sum().reset_index()\n",
    "    need_clicks = clicks_p.groupby(\"need\")[\"clicks\"].sum()\n",
    "    need_imps   = qp.groupby(\"need\")[\"impressions\"].sum()\n",
    "    pages       = clicks_p[clicks_p[\"clicks\"]>0].groupby(\"need\")[\"page\"].nunique()\n",
    "    queries     = clicks_q.groupby(\"need\")[\"query\"].nunique()\n",
    "\n",
    "    # CTR\n",
    "    ctr = (need_clicks / need_imps.replace(0, np.nan)).fillna(0.0)\n",
    "    # top3 share\n",
    "    top3 = (clicks_p.sort_values([\"need\",\"clicks\"], ascending=[True, False])\n",
    "             .groupby(\"need\")[\"clicks\"].apply(lambda s: s.head(3).sum()))\n",
    "    top3_share = (top3 / need_clicks.replace(0, np.nan)).fillna(0.0)\n",
    "    # HHI\n",
    "    def hhi(s):\n",
    "        tot = s.sum()\n",
    "        if tot <= 0: return 0.0\n",
    "        p = s / tot\n",
    "        return float((p**2).sum())\n",
    "    hhi_series = clicks_p.groupby(\"need\")[\"clicks\"].apply(hhi)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"need\": need_clicks.index,\n",
    "        \"clicks\": need_clicks.values,\n",
    "        \"impressions\": need_imps.reindex(need_clicks.index).values,\n",
    "        \"ctr\": ctr.reindex(need_clicks.index).values,\n",
    "        \"pages\": pages.reindex(need_clicks.index).fillna(0).astype(int).values,\n",
    "        \"queries\": queries.reindex(need_clicks.index).fillna(0).astype(int).values,\n",
    "        \"top3_share\": top3_share.reindex(need_clicks.index).fillna(0.0).values,\n",
    "        \"hhi\": hhi_series.reindex(need_clicks.index).fillna(0.0).values\n",
    "    })\n",
    "    df[\"queries_per_page\"] = (df[\"queries\"] / df[\"pages\"].replace(0, np.nan)).fillna(df[\"queries\"])\n",
    "    return df\n",
    "\n",
    "need_curr = aggregate_need_from_qp(df_qp)\n",
    "need_prev = aggregate_need_from_qp(df_qp_prev)\n",
    "\n",
    "need_overview_df = (\n",
    "    need_curr.merge(need_prev.add_prefix(\"prev_\"), left_on=\"need\", right_on=\"prev_need\", how=\"left\")\n",
    "             .drop(columns=[\"prev_need\"])\n",
    ")\n",
    "\n",
    "for col in [\"clicks\",\"impressions\",\"ctr\",\"pages\",\"queries\",\"queries_per_page\",\"top3_share\",\"hhi\"]:\n",
    "    need_overview_df[f\"mom_{col}\"] = need_overview_df[col] - need_overview_df.get(f\"prev_{col}\", 0)\n",
    "\n",
    "# フラグ・優先度\n",
    "need_overview_df[\"coverage_flag\"] = np.where(\n",
    "    (need_overview_df[\"queries_per_page\"] > TH_QPERPAGE) | (need_overview_df[\"top3_share\"] > TH_TOP3SHARE),\n",
    "    \"ページ不足/一極集中の疑い\", \"\"\n",
    ")\n",
    "need_overview_df[\"cannibal_flag\"] = np.where(\n",
    "    (need_overview_df[\"hhi\"] < TH_HHI_FRAGMENT) & (need_overview_df[\"ctr\"] < (need_overview_df[\"prev_ctr\"].fillna(need_overview_df[\"ctr\"]).median())),\n",
    "    \"分散しすぎ/カニバリ懸念\", \"\"\n",
    ")\n",
    "\n",
    "need_overview_df[\"opportunity\"] = need_overview_df[\"impressions\"] * (1.0 - need_overview_df[\"ctr\"])\n",
    "need_overview_df[\"trend\"] = need_overview_df[\"mom_impressions\"].fillna(0.0)\n",
    "need_overview_df[\"coverage_penalty\"] = (\n",
    "    (need_overview_df[\"queries_per_page\"] > TH_QPERPAGE).astype(int)\n",
    "  + (need_overview_df[\"top3_share\"] > TH_TOP3SHARE).astype(int)\n",
    ")\n",
    "need_overview_df[\"priority_score\"] = (\n",
    "    need_overview_df[\"opportunity\"]*1.0 + need_overview_df[\"trend\"]*0.5 - need_overview_df[\"coverage_penalty\"]*1000\n",
    ")\n",
    "\n",
    "# ========== 3) CTR改善対象（ページ） ==========\n",
    "page_stats = (df_qp.dropna(subset=[\"need\"])\n",
    "                  .groupby([\"need\",\"page\"], as_index=False)[[\"clicks\",\"impressions\"]].sum())\n",
    "page_stats[\"ctr\"] = (page_stats[\"clicks\"] / page_stats[\"impressions\"].replace(0,np.nan)).fillna(0.0)\n",
    "need_ctr = page_stats.groupby(\"need\")[\"ctr\"].mean().to_dict()\n",
    "need_imp_q = page_stats.groupby(\"need\")[\"impressions\"].quantile(MIN_IMP_QUANTILE).to_dict()\n",
    "\n",
    "def ctr_fix_row(r):\n",
    "    n = r[\"need\"]\n",
    "    return (r[\"impressions\"] >= need_imp_q.get(n, 0)) and (r[\"ctr\"] <= max(0.0, need_ctr.get(n, 0) - CTR_GAP))\n",
    "\n",
    "need_ctr_fixes_df = (page_stats[page_stats.apply(ctr_fix_row, axis=1)]\n",
    "                     .assign(note=\"Title/Description改善・導入段落の検索意図明確化・FAQ補強\")\n",
    "                     .sort_values([\"need\",\"impressions\"], ascending=[True, False]))\n",
    "# 変更不可ディレクトリ除外\n",
    "need_ctr_fixes_df = need_ctr_fixes_df[~need_ctr_fixes_df[\"page\"].map(is_immutable_page)].reset_index(drop=True)\n",
    "\n",
    "# ========== 4) 新規トピック候補（High-IMP × Low-Click のクエリ） ==========\n",
    "q_need = (df_qp.dropna(subset=[\"need\"])\n",
    "              .groupby([\"need\",\"query\"], as_index=False)[[\"clicks\",\"impressions\"]].sum())\n",
    "imp_q_by_need = q_need.groupby(\"need\")[\"impressions\"].quantile(NEW_TOPIC_IMP_Q).to_dict()\n",
    "cand_new = q_need[(q_need[\"clicks\"] < CLICK_LOW) & (q_need[\"impressions\"] >= q_need[\"need\"].map(imp_q_by_need))]\n",
    "\n",
    "def intent_label(q: str):\n",
    "    if re.search(r\"(とは|とは何|とはなに|\\?$|？$)\", q): return \"定義/概要\"\n",
    "    if re.search(r\"(比較|違い)\", q): return \"比較/違い\"\n",
    "    if re.search(r\"(方法|手順|やり方|使い方)\", q): return \"HowTo/手順\"\n",
    "    if re.search(r\"(料金|費用|価格|値段)\", q): return \"料金/費用\"\n",
    "    if re.search(r\"(事例|ケース|サンプル)\", q): return \"事例/ケース\"\n",
    "    if re.search(r\"(FAQ|よくある質問|Q&A)\", q, flags=re.IGNORECASE): return \"FAQ\"\n",
    "    return \"一般/その他\"\n",
    "\n",
    "need_new_topics_df = (cand_new\n",
    "    .assign(intent=lambda d: d[\"query\"].map(intent_label))\n",
    "    .groupby([\"need\",\"intent\"], as_index=False)\n",
    "    .agg(top_queries=(\"query\", lambda s: s.head(10).tolist()),\n",
    "         impressions=(\"impressions\",\"sum\"))\n",
    "    .sort_values([\"impressions\"], ascending=False)\n",
    ")\n",
    "\n",
    "# ========== 5) 内部リンク案（Need内のハブへ集約） ==========\n",
    "need_page_clicks = (df_qp.dropna(subset=[\"need\"])\n",
    "                        .groupby([\"need\",\"page\"], as_index=False)[\"clicks\"].sum())\n",
    "hub_page_by_need = (need_page_clicks.sort_values([\"need\",\"clicks\"], ascending=[True, False])\n",
    "                                   .drop_duplicates(\"need\")\n",
    "                                   .set_index(\"need\")[\"page\"].to_dict())\n",
    "\n",
    "link_rows = []\n",
    "for need, hub in hub_page_by_need.items():\n",
    "    if not isinstance(hub, str) or not hub:\n",
    "        continue\n",
    "    if is_immutable_page(hub):\n",
    "        continue\n",
    "    sources = need_page_clicks[(need_page_clicks[\"need\"]==need) & (need_page_clicks[\"page\"]!=hub)]\n",
    "    if sources.empty:\n",
    "        continue\n",
    "    # アンカーテキスト候補：そのNeed内の上位クエリ\n",
    "    q_top = (df_qp[(df_qp[\"need\"]==need)]\n",
    "             .groupby(\"query\")[\"clicks\"].sum()\n",
    "             .sort_values(ascending=False).head(12).index.tolist())\n",
    "    for _, r in sources.iterrows():\n",
    "        if is_immutable_page(r[\"page\"]):\n",
    "            continue\n",
    "        link_rows.append({\n",
    "            \"need\": need,\n",
    "            \"source_page\": r[\"page\"],\n",
    "            \"target_page\": hub,\n",
    "            \"anchor_candidates\": q_top[:6],\n",
    "            \"reason\": \"同Need内のハブページへ評価集中（回遊増 & カニバリ緩和）\"\n",
    "        })\n",
    "need_link_plan_df = pd.DataFrame(link_rows)\n",
    "\n",
    "# ========== 6) Need優先順位 ==========\n",
    "need_priority_df = (need_overview_df\n",
    "    .sort_values([\"priority_score\",\"opportunity\",\"mom_impressions\"], ascending=[False, False, False])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ========== 7) 表示（列の定義・読み方・活用方法 付き） ==========\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"=== 出力テーブルと読み方（クエリ起点 Need 版） ===\\n\")\n",
    "\n",
    "print(\"1) need_overview_df ・・・ Need（= クエリから自動生成したトピック）別のKPIとMoM\")\n",
    "print(\"- need: クエリ埋め込み→HDBSCAN→c-TF-IDFで命名したトピック名（例：'採用/エントリー/中途'）\")\n",
    "print(\"- clicks, impressions, ctr: 今月(LM)のNeed集約KPI（ctrはclicks/impressions）\")\n",
    "print(\"- pages, queries: クリックのあるページ数 / クエリ数（このNeedに属するもの）\")\n",
    "print(\"- queries_per_page: クエリ数/ページ数（大きい→ページ不足、一極集中の疑い）\")\n",
    "print(\"- top3_share: 上位3ページのクリック集中度（大きい→一極集中）\")\n",
    "print(\"- hhi: クリック配分のHHI（小さい→分散しすぎ/カニバリ懸念）\")\n",
    "print(\"- mom_*: 直前月(LM-1)からの差分（needは“今月の分類”に投影）\")\n",
    "print(\"- opportunity: 機会損失規模（imp*(1-ctr)）\")\n",
    "print(\"- priority_score: 機会・トレンド・カバレッジを合算した当座の優先度\\n\")\n",
    "display(need_overview_df.sort_values(\"priority_score\", ascending=False).head(20))\n",
    "\n",
    "print(\"\\n2) need_priority_df ・・・ 優先すべきNeedのランキング\")\n",
    "print(\"- priority_scoreで降順。上位から施策を当てていくのが基本方針です。\\n\")\n",
    "display(need_priority_df.head(20)[[\"need\",\"priority_score\",\"opportunity\",\"trend\",\"coverage_flag\",\"cannibal_flag\"]])\n",
    "\n",
    "print(\"\\n3) need_ctr_fixes_df ・・・ CTR改善対象ページ（Need内でimp上位 & CTRが平均より低い）\")\n",
    "print(\"- page/ctr/impressions/clicks と“何を直すか”のメモを表示。\")\n",
    "print(\"- タイトル/description、導入文、FAQの強化での改善を想定。\\n\")\n",
    "display(need_ctr_fixes_df.head(30))\n",
    "\n",
    "print(\"\\n4) need_new_topics_df ・・・ 追加すべきトピック候補（High IMP × Low Clickのクエリ集合）\")\n",
    "print(\"- need, intent（定義/比較/HowTo/料金等）, top_queries（例示）を提示。\")\n",
    "print(\"- これをもとに特集/比較/FAQ/導入ガイド等の新規ページを企画。\\n\")\n",
    "display(need_new_topics_df.head(30))\n",
    "\n",
    "print(\"\\n5) need_link_plan_df ・・・ 内部リンク案（Need内でハブへ集約）\")\n",
    "print(\"- needごとに source_page → target_page（ハブ）、anchor_candidates（クエリ語）を提案。\")\n",
    "print(\"- 関連記事モジュール/本文内/パンくずなどでの導線強化に活用。\\n\")\n",
    "display(need_link_plan_df.head(30))\n",
    "\n",
    "print(\"\\n=== 活用のコツ（クエリ起点 Need 版） ===\")\n",
    "print(\"- 上位 Need（need_priority_df）を選定 → 以下を同Need内で実施：\")\n",
    "print(\"  (a) need_ctr_fixes_df で CTR 改善（タイトル/導入/FAQ）\")\n",
    "print(\"  (b) need_link_plan_df でハブへ内部リンク集約（回遊・評価伝播）\")\n",
    "print(\"  (c) need_new_topics_df で不足トピックを追加（需要>供給の穴）\")\n",
    "print(\"- MoMは“今月のNeed分類”に先月クエリを投影して算出。分類が変わった場合は参考値として解釈。\")\n",
    "print(\"- 変更不可ディレクトリ配下のページは、CTR改善/リンク案から自動除外しています。\")\n"
   ],
   "metadata": {
    "id": "8brBZ4EuBSus",
    "cellView": "form"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title ⑤ 編集ブリーフ / 内部リンク案 / 優先順位表 / リライト指示書 / IA変更提案\n",
    "# 依存: Cell 3（serp_nodes_df, serp_edges_df, serp_clusters_df, serp_graph）, Cell 2（parallel_query_web_gsc）, Cell 1 の期間/接続変数\n",
    "# 出力: serp_priority_df, serp_briefs_df, serp_linkplan_df, serp_rewrite_df, serp_ia_df（いずれも保存しない）\n",
    "\n",
    "# ===== パラメータ（必要に応じて調整）=====\n",
    "TOP_CLUSTERS = 20            # ブリーフ等を作る上位クラスタ数（優先度順）\n",
    "TOP_QUERIES_PER_CLUSTER = 12 # クラスタごとの上位クエリ数（ブリーフ/リンク案に使用）\n",
    "MAX_PAGES_PER_QUERY = 3      # クエリごとの例示ページ最大数（リンク案/ブリーフに使用）\n",
    "CTR_GAP = 0.05               # リライト候補: クラスタ平均CTRよりどれだけ低ければ対象か（5pp）\n",
    "IMP_QTL = 0.70               # リライト候補: インプレッションの分位（上位30%を対象）\n",
    "SECTION_DOMINANCE = 0.60     # IA提案: トップsectionの占有率しきい値\n",
    "\n",
    "# ===== 変更不可ディレクトリ（IA変更/リライト対象外にする） =====\n",
    "IMMUTABLE_SECTIONS = [\n",
    "    \"/jp/ir/\",\n",
    "    \"/jp/careers/\",\n",
    "    \"/art-house/jp/nadoto/\"\n",
    "]\n",
    "\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 存在チェック\n",
    "for name in [\"serp_nodes_df\", \"serp_edges_df\", \"serp_clusters_df\", \"serp_graph\"]:\n",
    "    if name not in globals() or globals()[name] is None or (hasattr(globals()[name], \"empty\") and globals()[name].empty):\n",
    "        raise RuntimeError(f\"{name} が未生成か空です。Cell 3 を先に実行してください。\")\n",
    "\n",
    "# 期間内の query×page を取得（Cell 2 の関数を再利用）\n",
    "filters_jp_qp = [\n",
    "    {\"pattern\": \"/jp/\", \"dimension\": \"page\"},\n",
    "    {\"pattern\": \"/sbs/\", \"dimension\": \"page\"},\n",
    "    {\"pattern\": \"/scp/\", \"dimension\": \"page\"},\n",
    "]\n",
    "df_qp = await parallel_query_web_gsc(\n",
    "    CREDS_PATH, GSC_PROPERTY,\n",
    "    filters_jp_qp, LM_From, LM___To,\n",
    "    [\"query\", \"page\"], mg.creds.scopes\n",
    ")\n",
    "if df_qp.empty:\n",
    "    raise RuntimeError(\"query×page データが空です。条件（期間/フィルタ）をご確認ください。\")\n",
    "\n",
    "# クラスタ基礎指標の再計算（優先順位用に機会スコアを追加）\n",
    "cl_base = serp_nodes_df.groupby(\"cluster\").agg(\n",
    "    clicks=(\"clicks\",\"sum\"),\n",
    "    impressions=(\"impressions\",\"sum\"),\n",
    "    ctr=(\"ctr\",\"mean\"),\n",
    "    n_queries=(\"query\",\"nunique\"),\n",
    "    degree_avg=(\"degree\",\"mean\")\n",
    ").reset_index()\n",
    "\n",
    "# 機会スコア（需要はあるが刺さってない）: impressions * (1 - ctr)\n",
    "cl_base[\"opportunity\"] = cl_base[\"impressions\"] * (1.0 - cl_base[\"ctr\"])\n",
    "# 参考スコア: クリック成長余地（素朴）：impressions*ctr*(1-ctr)\n",
    "cl_base[\"click_growth_room\"] = cl_base[\"impressions\"] * cl_base[\"ctr\"] * (1.0 - cl_base[\"ctr\"])\n",
    "\n",
    "# Cell3の要約と結合（cluster_label 等）\n",
    "serp_priority_df = (\n",
    "    serp_clusters_df\n",
    "    .merge(cl_base[[\"cluster\",\"opportunity\",\"click_growth_room\"]], on=\"cluster\", how=\"left\")\n",
    "    .sort_values([\"opportunity\",\"clicks\",\"n_queries\"], ascending=[False, False, False])\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# ===== クエリ→例示ページの辞書（クリック上位） =====\n",
    "df_qp[\"clicks\"] = pd.to_numeric(df_qp.get(\"clicks\", 0), errors=\"coerce\").fillna(0).astype(int)\n",
    "q2pages = (\n",
    "    df_qp.sort_values([\"query\",\"clicks\"], ascending=[True,False])\n",
    "         .groupby(\"query\")[\"page\"]\n",
    "         .apply(lambda s: s.head(MAX_PAGES_PER_QUERY).tolist())\n",
    "         .to_dict()\n",
    ")\n",
    "\n",
    "def section_from_url(u: str) -> str:\n",
    "    try:\n",
    "        p = urlparse(u).path\n",
    "    except Exception:\n",
    "        p = str(u)\n",
    "    parts = [s for s in p.split(\"/\") if s]\n",
    "    if len(parts) >= 2:\n",
    "        return f\"/{parts[0]}/{parts[1]}/\"\n",
    "    elif len(parts) == 1:\n",
    "        return f\"/{parts[0]}/\"\n",
    "    return \"/\"\n",
    "\n",
    "# ===== 編集ブリーフ（クラスタごとの推奨構成） =====\n",
    "brief_rows = []\n",
    "top_clusters = serp_priority_df.head(TOP_CLUSTERS)[\"cluster\"].tolist()\n",
    "\n",
    "for cid in top_clusters:\n",
    "    nodes_c = serp_nodes_df[serp_nodes_df[\"cluster\"] == cid].copy()\n",
    "    if nodes_c.empty:\n",
    "        continue\n",
    "\n",
    "    # 代表ディレクトリ（Cell3で作成済み）\n",
    "    clabel = nodes_c[\"cluster_label\"].dropna().mode().iat[0] if nodes_c[\"cluster_label\"].dropna().size else None\n",
    "\n",
    "    # 変更不可ディレクトリ → ブリーフは作らずスキップ\n",
    "    if clabel in IMMUTABLE_SECTIONS:\n",
    "        continue\n",
    "\n",
    "    # 上位クエリ（clicks→degree）\n",
    "    nodes_c = nodes_c.sort_values([\"clicks\",\"degree\"], ascending=[False, False])\n",
    "    top_q = nodes_c[\"query\"].head(TOP_QUERIES_PER_CLUSTER).tolist()\n",
    "\n",
    "    # ハブ候補（最上位クエリ1-3）\n",
    "    hub_queries = top_q[:3]\n",
    "    # スポーク候補（次点クエリ）\n",
    "    spoke_queries = top_q[3:TOP_QUERIES_PER_CLUSTER]\n",
    "\n",
    "    # ハブURL候補（クラスタ内で最もクリックの多いページ：例示）\n",
    "    df_c_qp = df_qp[df_qp[\"query\"].isin(nodes_c[\"query\"])]\n",
    "    hub_page = (df_c_qp.groupby(\"page\")[\"clicks\"].sum()\n",
    "                .sort_values(ascending=False).head(1).index.tolist())\n",
    "    hub_page = hub_page[0] if hub_page else None\n",
    "\n",
    "    # 見出し・FAQ候補（疑問意図を抽出）\n",
    "    def is_question(q):\n",
    "        return bool(re.search(r\"(とは|とは何|とはなに|とは\\?|とは？|どう|なぜ|理由|原因|方法|できる|比較|違い)\", q))\n",
    "    faq_candidates = [q for q in top_q if is_question(q)][:6]\n",
    "\n",
    "    brief_rows.append({\n",
    "        \"cluster\": cid,\n",
    "        \"cluster_label\": clabel,\n",
    "        \"hub_url_candidate\": hub_page or clabel or \"\",\n",
    "        \"representative_queries\": hub_queries,\n",
    "        \"supporting_queries\": spoke_queries,\n",
    "        \"h2_suggestions\": hub_queries[:1] + spoke_queries[:4],\n",
    "        \"faq_candidates\": faq_candidates,\n",
    "        \"example_pages_for_top_queries\": {q: q2pages.get(q, []) for q in hub_queries}\n",
    "    })\n",
    "\n",
    "serp_briefs_df = pd.DataFrame(brief_rows)\n",
    "\n",
    "# ===== 内部リンク案（同クラスタ内の代表ページへ集約） =====\n",
    "link_rows = []\n",
    "for cid in top_clusters:\n",
    "    nodes_c = serp_nodes_df[serp_nodes_df[\"cluster\"] == cid]\n",
    "    if nodes_c.empty:\n",
    "        continue\n",
    "    # ターゲット（クラスタ内でクリック最大のページ）\n",
    "    df_c_qp = df_qp[df_qp[\"query\"].isin(nodes_c[\"query\"])]\n",
    "    if df_c_qp.empty:\n",
    "        continue\n",
    "    target_page = (df_c_qp.groupby(\"page\")[\"clicks\"].sum()\n",
    "                   .sort_values(ascending=False).head(1).index.tolist())\n",
    "    target_page = target_page[0] if target_page else None\n",
    "    if not target_page:\n",
    "        continue\n",
    "\n",
    "    # アンカーテキスト候補 = 上位クエリ（ターゲットに重複しないよう一部除外）\n",
    "    nodes_c_sorted = nodes_c.sort_values([\"clicks\",\"degree\"], ascending=[False, False])\n",
    "    anchor_candidates = nodes_c_sorted[\"query\"].head(TOP_QUERIES_PER_CLUSTER).tolist()\n",
    "\n",
    "    # 各クエリの代表ページ（source）→ ターゲット（target_page）にリンク案\n",
    "    for q in anchor_candidates:\n",
    "        src_pages = q2pages.get(q, [])  # 例示ページ（最大 MAX_PAGES_PER_QUERY 件）\n",
    "        for sp in src_pages:\n",
    "            if sp == target_page:\n",
    "                continue\n",
    "            link_rows.append({\n",
    "                \"cluster\": cid,\n",
    "                \"anchor_text\": q,\n",
    "                \"source_page\": sp,\n",
    "                \"target_page\": target_page,\n",
    "                \"reason\": \"同クラスタ内のハブページへ評価集中（共起/Jaccardベース）\"\n",
    "            })\n",
    "\n",
    "serp_linkplan_df = pd.DataFrame(link_rows)\n",
    "\n",
    "# ===== リライト指示書（低CTR×高Impression の抽出と提案） =====\n",
    "rewrite_rows = []\n",
    "# クラスタごと平均CTR/IMP分位\n",
    "cl_stats = serp_nodes_df.groupby(\"cluster\").agg(\n",
    "    ctr_avg=(\"ctr\",\"mean\"),\n",
    "    imp_q=(\"impressions\", lambda s: float(np.quantile(pd.to_numeric(s, errors=\"coerce\").fillna(0), IMP_QTL)))\n",
    ").to_dict()\n",
    "\n",
    "for cid in top_clusters:\n",
    "    nodes_c = serp_nodes_df[serp_nodes_df[\"cluster\"] == cid].copy()\n",
    "    if nodes_c.empty:\n",
    "        continue\n",
    "    ctr_avg = cl_stats[\"ctr_avg\"].get(cid, nodes_c[\"ctr\"].mean())\n",
    "    imp_q = cl_stats[\"imp_q\"].get(cid, nodes_c[\"impressions\"].quantile(IMP_QTL))\n",
    "\n",
    "    # 候補: CTRが低い & インプレッションが高い\n",
    "    cand = nodes_c[\n",
    "        (pd.to_numeric(nodes_c[\"impressions\"], errors=\"coerce\") >= imp_q) &\n",
    "        (pd.to_numeric(nodes_c[\"ctr\"], errors=\"coerce\") <= max(0.0, ctr_avg - CTR_GAP))\n",
    "    ].copy()\n",
    "\n",
    "    if cand.empty:\n",
    "        continue\n",
    "\n",
    "    # タイトル/見出し案（素朴テンプレ）\n",
    "    clabel = nodes_c[\"cluster_label\"].dropna().mode().iat[0] if nodes_c[\"cluster_label\"].dropna().size else \"\"\n",
    "    top_support = (nodes_c.sort_values([\"clicks\",\"degree\"], ascending=[False, False])[\"query\"]\n",
    "                   .head(6).tolist())\n",
    "\n",
    "    for _, r in cand.sort_values([\"impressions\"], ascending=False).iterrows():\n",
    "        q = r[\"query\"]\n",
    "        title1 = f\"{q}｜{clabel.strip('/') if clabel else '特集'}\"\n",
    "        title2 = f\"{q}とは？基本・比較・選び方を解説\"\n",
    "        h2s = [q] + [x for x in top_support if x != q][:4]\n",
    "        rewrite_rows.append({\n",
    "            \"cluster\": cid,\n",
    "            \"query\": q,\n",
    "            \"impressions\": r[\"impressions\"],\n",
    "            \"ctr\": r[\"ctr\"],\n",
    "            \"cluster_ctr_avg\": ctr_avg,\n",
    "            \"suggested_title_1\": title1,\n",
    "            \"suggested_title_2\": title2,\n",
    "            \"suggested_h2\": h2s,\n",
    "            \"notes\": \"スニペット想定語を導入段落に配置（用語定義/比較/FAQ）。内部リンクはクラスタのハブへ。\"\n",
    "        })\n",
    "\n",
    "serp_rewrite_df = pd.DataFrame(rewrite_rows)\n",
    "\n",
    "# ===== IA変更提案（セクションの分散・カニバリ兆候） =====\n",
    "ia_rows = []\n",
    "for cid in top_clusters:\n",
    "    nodes_c = serp_nodes_df[serp_nodes_df[\"cluster\"] == cid]\n",
    "    if nodes_c.empty:\n",
    "        continue\n",
    "\n",
    "    # sectionの分布\n",
    "    sec_counts = nodes_c[\"section\"].fillna(\"\").value_counts()\n",
    "    if sec_counts.empty:\n",
    "        continue\n",
    "    top_sec, top_cnt = sec_counts.index[0], int(sec_counts.iloc[0])\n",
    "    if top_sec in IMMUTABLE_SECTIONS:\n",
    "        continue  # immutable セクションはIA提案対象外\n",
    "    total = int(sec_counts.sum())\n",
    "    dominance = top_cnt / max(1,total)\n",
    "\n",
    "    issue = []\n",
    "    suggest = []\n",
    "\n",
    "    # 支配的セクションが弱い → IAで集約/新カテゴリ\n",
    "    if dominance < SECTION_DOMINANCE:\n",
    "        issue.append(f\"クラスタ内のsection分散（トップが {dominance:.0%}）\")\n",
    "        suggest.append(f\"カテゴリ統合またはテーマランディング新設（例: {top_sec or '該当なし'} 配下に集約）\")\n",
    "\n",
    "    # カニバリ兆候（強いエッジ多 & sectionが複数）\n",
    "    multi_sec = (sec_counts.index.nunique() >= 2)\n",
    "    if multi_sec and not sec_counts.empty and dominance < 0.8:\n",
    "        issue.append(\"同テーマを複数セクションで展開（カニバリ懸念）\")\n",
    "        suggest.append(\"重複ページの統合/301、役割分担（入門/詳細/比較/FAQ）を明確化\")\n",
    "\n",
    "    if issue:\n",
    "        ia_rows.append({\n",
    "            \"cluster\": cid,\n",
    "            \"top_section\": top_sec,\n",
    "            \"section_dominance\": round(dominance, 3),\n",
    "            \"issues\": \" / \".join(issue),\n",
    "            \"proposal\": \" / \".join(suggest)\n",
    "        })\n",
    "\n",
    "serp_ia_df = pd.DataFrame(ia_rows).sort_values([\"section_dominance\"], ascending=[True]).reset_index(drop=True)\n",
    "\n",
    "# ===== 表示（保存はしない）=====\n",
    "from IPython.display import display\n",
    "print(\"=== 優先順位表（クラスタ）: opportunity desc ===\")\n",
    "display(serp_priority_df.head(20))\n",
    "\n",
    "print(\"\\n=== 編集ブリーフ（上位クラスタ） ===\")\n",
    "display(serp_briefs_df.head(10))\n",
    "\n",
    "print(\"\\n=== 内部リンク案（例示） ===\")\n",
    "display(serp_linkplan_df.head(20))\n",
    "\n",
    "print(\"\\n=== リライト指示書（低CTR×高IMP） ===\")\n",
    "display(serp_rewrite_df.head(20))\n",
    "\n",
    "print(\"\\n=== IA変更提案（セクション分散/カニバリ兆候） ===\")\n",
    "display(serp_ia_df.head(20))\n",
    "\n",
    "print(\"\\n（必要なら Cell 4 を参考に、上記 DataFrame を任意のシート名で保存してください）\")\n"
   ],
   "metadata": {
    "cellView": "form",
    "id": "Vw1zt0PN4CpG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title GSC分類とプレビュー（AI.GENERATE） 実験中\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# ===== 設定（必要に応じて変更） =====\n",
    "PROJECT_ID   = GCP_PROJECT          # ← Cell 1で設定済み\n",
    "DATASET      = \"searchconsole_corp\"\n",
    "TMP_TABLE    = \"_tmp_gsc_queries_for_ai\"  # 一時テーブル（毎回TRUNCATE）\n",
    "\n",
    "# Vertex 接続情報（AI.GENERATE）\n",
    "# CONNECTION_ID = \"asia-northeast1.gsc_vertex\"\n",
    "CONNECTION_PROJECT = PROJECT_ID\n",
    "CONNECTION_LOCATION = \"asia-northeast1\"\n",
    "CONNECTION_NAME = \"gsc_vertex\"\n",
    "CONNECTION_ID_LITERAL = f\"{CONNECTION_PROJECT}.{CONNECTION_LOCATION}.{CONNECTION_NAME}\"\n",
    "\n",
    "ENDPOINT_LITERAL = \"gemini-1.5-flash-002\"    # 例: gemini-2.0-flash / gemini-2.5-flash\n",
    "\n",
    "# 分類プロンプト（自由に編集）\n",
    "CLASSIFY_PROMPT = (\n",
    "    \"あなたは資生堂 企業サイト向けの検索クエリ分類器です。\"\n",
    "    \"与えられたクエリに対して、以下を日本語で返してください：\"\n",
    "    \"category（次から1つ: ブランド・企業情報, IR・投資家, 採用・キャリア, サステナビリティ/ESG, 製品情報, ニュース/プレス, 研究開発, 店舗・アクセス, お問い合わせ, その他）, \"\n",
    "    \"intent（navigational / informational / transactional / job / investor / other）, \"\n",
    "    \"confidence（0〜1の少数）, \"\n",
    "    \"reason（短い根拠）。\"\n",
    "    \"出力は上記の4項目だけ。\"\n",
    ")\n",
    "\n",
    "# ===== Cell 2 の結果（df_jp / df_en / df_t）を結合 =====\n",
    "dfs = []\n",
    "for name in [\"df_jp\", \"df_en\"]: # [\"df_jp\", \"df_en\", \"df_t\"]:\n",
    "    if name in globals() and isinstance(globals()[name], pd.DataFrame) and not globals()[name].empty:\n",
    "        d = globals()[name].copy()\n",
    "        d[\"lang_group\"] = name.replace(\"df_\", \"\")  # jp / en / t\n",
    "        dfs.append(d[[\"query\", \"clicks\", \"impressions\", \"ctr\", \"lang_group\"]])\n",
    "\n",
    "if not dfs:\n",
    "    raise RuntimeError(\"df_jp / df_en / df_t が見つかりません。先に Cell 2 を実行してください。\")\n",
    "\n",
    "df_all = pd.concat(dfs, ignore_index=True).dropna(subset=[\"query\"]).copy()\n",
    "df_all[\"period_from\"] = LM_From\n",
    "df_all[\"period_to\"]   = LM___To\n",
    "df_all[\"run_ts\"]      = datetime.now(timezone(timedelta(hours=9)))  # JST\n",
    "\n",
    "# ===== BigQuery に一時投入 → AI.GENERATE で分類 =====\n",
    "SA_SCOPES = [\n",
    "    \"https://www.googleapis.com/auth/cloud-platform\",\n",
    "    \"https://www.googleapis.com/auth/bigquery\",\n",
    "]\n",
    "SA_CREDS = service_account.Credentials.from_service_account_file(CREDS_PATH, scopes=SA_SCOPES)\n",
    "bq_client = bigquery.Client(project=PROJECT_ID, credentials=SA_CREDS)\n",
    "# bq_client = bq\n",
    "\n",
    "tmp_table_id = f\"{PROJECT_ID}.{DATASET}.{TMP_TABLE}\"\n",
    "load_cfg = bigquery.LoadJobConfig(write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE)\n",
    "bq_client.load_table_from_dataframe(\n",
    "    df_all[[\"query\",\"clicks\",\"impressions\",\"ctr\",\"lang_group\",\"period_from\",\"period_to\",\"run_ts\"]],\n",
    "    tmp_table_id, job_config=load_cfg\n",
    ").result()\n",
    "\n",
    "sql = f\"\"\"\n",
    "WITH gen AS (\n",
    "  SELECT\n",
    "    i.query, i.clicks, i.impressions, i.ctr, i.lang_group, i.period_from, i.period_to, i.run_ts,\n",
    "    AI.GENERATE(\n",
    "      (@prompt || ' クエリ: ' || i.query),\n",
    "      connection_id => '{CONNECTION_ID_LITERAL}',\n",
    "      endpoint      => '{ENDPOINT_LITERAL}',\n",
    "      output_schema => 'category STRING, intent STRING, confidence FLOAT64, reason STRING'\n",
    "    ) AS out\n",
    "  FROM `{PROJECT_ID}.{DATASET}.{TMP_TABLE}` AS i\n",
    ")\n",
    "SELECT\n",
    "  query, clicks, impressions, ctr, lang_group, period_from, period_to, run_ts,\n",
    "  out.category   AS category,\n",
    "  out.intent     AS intent,\n",
    "  out.confidence AS confidence,\n",
    "  out.reason     AS reason\n",
    "FROM gen\n",
    "\"\"\"\n",
    "\n",
    "params = [bigquery.ScalarQueryParameter(\"prompt\", \"STRING\", CLASSIFY_PROMPT)]\n",
    "\n",
    "df_classified = bq_client.query(\n",
    "    sql, job_config=bigquery.QueryJobConfig(query_parameters=params)\n",
    ").result().to_dataframe()\n",
    "\n",
    "# ===== プレビュー =====\n",
    "print(\"=== 分類プレビュー（先頭20行） ===\")\n",
    "display(df_classified.head(20))\n",
    "print(f\"\\n(using connection_id: {CONNECTION_ID_LITERAL}, endpoint: {ENDPOINT_LITERAL})\")\n",
    "\n",
    "pivot = (\n",
    "    df_classified.groupby([\"category\", \"intent\"])\n",
    "    .size().reset_index(name=\"rows\")\n",
    "    .sort_values([\"category\",\"rows\"], ascending=[True, False])\n",
    ")\n",
    "print(\"\\n=== 件数サマリ（category × intent） ===\")\n",
    "display(pivot)\n"
   ],
   "metadata": {
    "cellView": "form",
    "id": "WIz3iQlYlzvb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "dcgyPo7oxkSj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title _gsc に重複なしでマージ保存（URL8）実験中\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ===== 設定（必要に応じて変更） =====\n",
    "GS_URL        = URL8             # ← Cell 1 で定義済み\n",
    "GS_SHEET_NAME = \"_gsc\"\n",
    "KEY_COLS      = [\"query\", \"lang_group\"]  # 重複判定キー（必要なら列を追加）\n",
    "\n",
    "# 必須列チェック\n",
    "required_cols = [\"query\",\"clicks\",\"impressions\",\"ctr\",\"lang_group\",\n",
    "                 \"category\",\"intent\",\"confidence\",\"reason\",\n",
    "                 \"period_from\",\"period_to\",\"run_ts\"]\n",
    "if \"df_classified\" not in globals() or not isinstance(df_classified, pd.DataFrame) or df_classified.empty:\n",
    "    raise RuntimeError(\"df_classified が見つかりません。先に Cell 3 を実行してください。\")\n",
    "\n",
    "missing = [c for c in required_cols if c not in df_classified.columns]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"df_classified に欠落カラム: {missing}\")\n",
    "\n",
    "# 既存シート → DataFrame\n",
    "def _load_sheet_as_df(gs_url: str, sheet_name: str) -> pd.DataFrame:\n",
    "    if not mg.open.sheet(gs_url):\n",
    "        raise RuntimeError(\"Google Sheets を開けませんでした。URL/権限を確認してください。\")\n",
    "    try:\n",
    "        ws = mg.gs._driver.worksheet(sheet_name)\n",
    "    except Exception:\n",
    "        mg.gs._driver.add_worksheet(title=sheet_name, rows=10, cols=10)\n",
    "        return pd.DataFrame()\n",
    "    values = ws.get_all_values()\n",
    "    if not values:\n",
    "        return pd.DataFrame()\n",
    "    header, rows = values[0], values[1:]\n",
    "    return pd.DataFrame(rows, columns=header) if rows else pd.DataFrame(columns=header)\n",
    "\n",
    "# 型整形（比較用）\n",
    "def _coerce_types(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for c in [\"clicks\",\"impressions\",\"ctr\",\"confidence\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    if \"run_ts\" in df.columns:\n",
    "        df[\"run_ts_dt\"] = pd.to_datetime(df[\"run_ts\"], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "# マージ保存\n",
    "def upsert_to_google_sheet(gs_url: str, sheet_name: str, df_new: pd.DataFrame, key_cols: list, sort_by: str = \"clicks\"):\n",
    "    columns_out = [\"query\",\"lang_group\",\"clicks\",\"impressions\",\"ctr\",\n",
    "                   \"category\",\"intent\",\"confidence\",\"reason\",\n",
    "                   \"period_from\",\"period_to\",\"run_ts\"]\n",
    "\n",
    "    # 列を揃える\n",
    "    for c in columns_out:\n",
    "        if c not in df_new.columns:\n",
    "            df_new[c] = None\n",
    "    df_new = df_new[columns_out].copy()\n",
    "\n",
    "    # 既存読み込み\n",
    "    df_old = _load_sheet_as_df(gs_url, sheet_name)\n",
    "    if not df_old.empty:\n",
    "        for c in columns_out:\n",
    "            if c not in df_old.columns:\n",
    "                df_old[c] = None\n",
    "        df_old = df_old[columns_out].copy()\n",
    "    else:\n",
    "        df_old = pd.DataFrame(columns=columns_out)\n",
    "\n",
    "    # 新旧結合 → 新しい方を優先して重複除去\n",
    "    df_old = _coerce_types(df_old)\n",
    "    df_new = _coerce_types(df_new)\n",
    "\n",
    "    df_new[\"__is_new\"] = True\n",
    "    df_old[\"__is_new\"] = False\n",
    "    merged = pd.concat([df_old, df_new], ignore_index=True)\n",
    "\n",
    "    # 新規優先 → run_ts があれば新しい日時を優先\n",
    "    sort_cols = [\"__is_new\"]\n",
    "    ascending = [False]\n",
    "    if \"run_ts_dt\" in merged.columns:\n",
    "        sort_cols.append(\"run_ts_dt\")\n",
    "        ascending.append(False)  # 新しい日時を先に\n",
    "    deduped = (\n",
    "        merged.sort_values(sort_cols, ascending=ascending)\n",
    "              .drop_duplicates(subset=key_cols, keep=\"first\")\n",
    "              .drop(columns=[\"__is_new\"])\n",
    "    )\n",
    "\n",
    "    # 表示優先で並べ替え\n",
    "    deduped = deduped.sort_values(by=sort_by, ascending=False)\n",
    "\n",
    "    # 書き込み（Cell 1のユーティリティを流用）\n",
    "    save_to_google_sheet(gs_url, sheet_name, deduped, sort_by=None)\n",
    "    print(f\"✅ マージ保存完了: {len(deduped)} 行（キー: {key_cols}） → {sheet_name}\")\n",
    "\n",
    "# 実行\n",
    "upsert_to_google_sheet(GS_URL, GS_SHEET_NAME, df_classified, KEY_COLS, sort_by=\"clicks\")\n"
   ],
   "metadata": {
    "id": "wta7V6AumG9W",
    "cellView": "form"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title 2025.10抽出依頼\n",
    "\n"
   ],
   "metadata": {
    "id": "fPSWo6srX19Y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "zOjKKiCzyxlt"
   },
   "outputs": [],
   "source": [
    "#@title GSCからデータを取得して加工　リファクタリング前\n",
    "\n",
    "try:\n",
    "    import jaconv\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -q jaconv\n",
    "    import jaconv\n",
    "    from IPython.display import clear_output\n",
    "    clear_output()\n",
    "\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "import logging\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from collections import defaultdict\n",
    "\n",
    "# Apply nest_asyncio for environments like Jupyter Notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Connect Google Search Console\n",
    "def connect_to_gsc(credentials_path, scopes=None):\n",
    "    \"\"\"\n",
    "    Connects to the Google Search Console API using a service account key.\n",
    "\n",
    "    Args:\n",
    "        credentials_path (str): Path to the service account key file.\n",
    "        scopes (list, optional): List of scopes for the connection. Defaults to ['https://www.googleapis.com/auth/webmasters'].\n",
    "\n",
    "    Returns:\n",
    "        object: Google Search Console service object.\n",
    "    \"\"\"\n",
    "    if scopes is None:\n",
    "        scopes = ['https://www.googleapis.com/auth/webmasters']\n",
    "    elif 'https://www.googleapis.com/auth/webmasters' not in scopes:\n",
    "        scopes.append('https://www.googleapis.com/auth/webmasters')\n",
    "\n",
    "    try:\n",
    "        credentials = service_account.Credentials.from_service_account_file(\n",
    "            credentials_path, scopes=scopes\n",
    "        )\n",
    "        return build('searchconsole', 'v1', credentials=credentials)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to connect to GSC: {e}\")\n",
    "        raise\n",
    "\n",
    "def execute_gsc_query(service, site_url, payload, dimensions):\n",
    "    \"\"\"\n",
    "    Executes a query to the Google Search Console API and processes the results.\n",
    "\n",
    "    Args:\n",
    "        service (object): GSC service object.\n",
    "        site_url (str): The site property URL in Google Search Console.\n",
    "        payload (dict): The query payload.\n",
    "        dimensions (list): List of dimensions to include in the query.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing query results.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    start_row = 0\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            payload[\"startRow\"] = start_row\n",
    "            payload[\"dimensions\"] = dimensions  # Set dimensions dynamically\n",
    "            response = service.searchanalytics().query(siteUrl=site_url, body=payload).execute()\n",
    "            rows = response.get('rows', [])\n",
    "            if not rows:\n",
    "                break\n",
    "            all_results.extend(rows)\n",
    "            start_row += len(rows)\n",
    "\n",
    "        results = []\n",
    "        for row in all_results:\n",
    "            data = {\n",
    "                dimension: row['keys'][i] for i, dimension in enumerate(dimensions)\n",
    "            }\n",
    "            data['clicks'] = row.get('clicks', 0)\n",
    "            data['impressions'] = row.get('impressions', 0)\n",
    "            results.append(data)\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during API call: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def execute_gsc_query_OLD(service, site_url, payload):\n",
    "    \"\"\"\n",
    "    Executes a query to the Google Search Console API and processes the results.\n",
    "\n",
    "    Args:\n",
    "        service (object): GSC service object.\n",
    "        site_url (str): The site property URL in Google Search Console.\n",
    "        payload (dict): The query payload.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing query results.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    start_row = 0\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            payload[\"startRow\"] = start_row\n",
    "            response = service.searchanalytics().query(siteUrl=site_url, body=payload).execute()\n",
    "            rows = response.get('rows', [])\n",
    "            if not rows:\n",
    "                break\n",
    "            all_results.extend(rows)\n",
    "            start_row += len(rows)\n",
    "\n",
    "        results = []\n",
    "        for row in all_results:\n",
    "            data = {\n",
    "                dimension: row['keys'][i] for i, dimension in enumerate(payload['dimensions'])\n",
    "            }\n",
    "            data['clicks'] = row['clicks']\n",
    "            data['impressions'] = row['impressions']\n",
    "            results.append(data)\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during API call: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def normalize_query_column(df, query_columns):\n",
    "    \"\"\"\n",
    "    Normalizes the 'query' column by 半角カナを全角カナへ、英数字を半角へ\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        query_columns (list): The names of the columns to normalize.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with normalized query column.\n",
    "    \"\"\"\n",
    "    for column in query_columns:\n",
    "        if column in df.columns:\n",
    "            df[column] = df[column].apply(lambda x: jaconv.h2z(x, kana=True) if pd.notnull(x) else x)  # 半角カナを全角カナへ\n",
    "            df[column] = df[column].apply(lambda x: jaconv.z2h(x, kana=False, ascii=True, digit=True) if pd.notnull(x) else x)  # 英数字を半角へ\n",
    "    return df\n",
    "\n",
    "def group_similar_queries(df, query_column=\"query\", metric_columns=None, additional_dimensions=None):\n",
    "    \"\"\"\n",
    "    Groups similar queries by normalizing spaces, reordering characters,\n",
    "    and aggregating metrics.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        query_column (str): The name of the query column to process.\n",
    "        metric_columns (list, optional): List of columns to aggregate (e.g., clicks, impressions).\n",
    "        additional_dimensions (list, optional): Additional columns to include in grouping.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with grouped queries.\n",
    "    \"\"\"\n",
    "    if query_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{query_column}' not found in the DataFrame.\")\n",
    "\n",
    "    if metric_columns is None:\n",
    "        metric_columns = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "        if not metric_columns:\n",
    "            raise ValueError(\"No metric columns found for aggregation.\")\n",
    "\n",
    "    if additional_dimensions is None:\n",
    "        additional_dimensions = [\n",
    "            col for col in df.columns\n",
    "            if col not in metric_columns + [query_column, \"normalized_query\"]\n",
    "        ]\n",
    "\n",
    "    def normalize_query(query):\n",
    "        if pd.isnull(query):\n",
    "            return query\n",
    "        return \"\".join(sorted(query.replace(\" \", \"\").replace(\"　\", \"\")))\n",
    "\n",
    "    df[\"normalized_query\"] = df[query_column].apply(normalize_query)\n",
    "\n",
    "    agg_rules = {query_column: \"first\"}\n",
    "    for col in metric_columns:\n",
    "        agg_rules[col] = \"sum\"\n",
    "\n",
    "    grouped = (\n",
    "        df.groupby([\"normalized_query\"] + additional_dimensions, as_index=False)\n",
    "        .agg(agg_rules)\n",
    "        .sort_values(by=metric_columns, ascending=[False] * len(metric_columns))\n",
    "    )\n",
    "\n",
    "    return grouped.drop(columns=[\"normalized_query\"])\n",
    "\n",
    "def query_web_gsc(credentials_path, site_url, filter_pattern, start_date, end_date, dimensions=(\"query\", \"page\"), filter_dimension=\"page\", scopes=None):\n",
    "    \"\"\"\n",
    "    Queries Google Search Console data with a filter and processes the results.\n",
    "\n",
    "    Args:\n",
    "        credentials_path (str): Path to the service account key file.\n",
    "        site_url (str): The site property URL in Google Search Console.\n",
    "        filter_pattern (str): The regex or filter expression for filtering data.\n",
    "        start_date (str): Query start date (YYYY-MM-DD).\n",
    "        end_date (str): Query end date (YYYY-MM-DD).\n",
    "        dimensions (tuple): Dimensions to include in the query (default: ('query', 'page')).\n",
    "        filter_dimension (str): Dimension to apply the filter (e.g., \"page\", \"query\").\n",
    "        scopes (list, optional): Scopes for the connection.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame containing query and metrics.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Executing GSC query for site {site_url}, filter {filter_pattern} on dimension '{filter_dimension}'\")\n",
    "    service = connect_to_gsc(credentials_path, scopes)\n",
    "\n",
    "    # Ensure the filter_dimension is valid\n",
    "    if filter_dimension not in dimensions:\n",
    "        raise ValueError(f\"filter_dimension '{filter_dimension}' must be one of the specified dimensions: {dimensions}\")\n",
    "\n",
    "    # Prepare the payload\n",
    "    payload = {\n",
    "        'startDate': start_date,\n",
    "        'endDate': end_date,\n",
    "        'dimensions': list(dimensions),  # Convert tuple to list for API compatibility\n",
    "        'type': 'web',\n",
    "        \"dimensionFilterGroups\": [\n",
    "            {\"filters\": [{\n",
    "                \"dimension\": filter_dimension,  # Use the dynamic filter dimension\n",
    "                \"operator\": \"includingRegex\",  # Example: using regex filter\n",
    "                \"expression\": filter_pattern\n",
    "            }]}\n",
    "        ],\n",
    "        \"rowLimit\": 25000\n",
    "    }\n",
    "\n",
    "    df = execute_gsc_query(service, site_url, payload, list(dimensions))\n",
    "    if df.empty:\n",
    "        logger.warning(f\"No data returned for filter '{filter_pattern}' on dimension '{filter_dimension}'\")\n",
    "    return df\n",
    "\n",
    "def query_web_gsc_OLD(credentials_path, site_url, filter_pattern, start_date, end_date, dimensions=(\"query\", \"page\"), scopes=None):\n",
    "    \"\"\"\n",
    "    Queries Google Search Console data with a filter and processes the results.\n",
    "\n",
    "    Args:\n",
    "        credentials_path (str): Path to the service account key file.\n",
    "        site_url (str): The site property URL in Google Search Console.\n",
    "        filter_pattern (str): The regex or filter expression for the page.\n",
    "        start_date (str): Query start date (YYYY-MM-DD).\n",
    "        end_date (str): Query end date (YYYY-MM-DD).\n",
    "        dimensions (tuple): Dimensions to include in the query.\n",
    "        scopes (list, optional): Scopes for the connection.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame containing query and metrics.\n",
    "    \"\"\"\n",
    "    if not isinstance(dimensions, (list, tuple)):\n",
    "        raise ValueError(\"The 'dimensions' parameter must be a list or tuple of dimension names.\")\n",
    "\n",
    "    service = connect_to_gsc(credentials_path, scopes)\n",
    "\n",
    "    payload = {\n",
    "        'startDate': start_date,\n",
    "        'endDate': end_date,\n",
    "        'dimensions': list(dimensions),  # Ensure dimensions are a list\n",
    "        'type': 'web',\n",
    "        \"dimensionFilterGroups\": [\n",
    "            {\"filters\": [{\n",
    "                \"dimension\": \"page\",\n",
    "                \"operator\": \"includingRegex\",\n",
    "                \"expression\": filter_pattern\n",
    "            }]}\n",
    "        ],\n",
    "        \"rowLimit\": 25000\n",
    "    }\n",
    "\n",
    "    df = execute_gsc_query(service, site_url, payload)\n",
    "    if df.empty:\n",
    "        logger.warning(f\"No data returned for filter {filter_pattern}\")\n",
    "    return df\n",
    "\n",
    "async def async_query_web_gsc(credentials_path, site_url, filter_pattern, start_date, end_date, dimensions=(\"query\", \"page\"), filter_dimension=\"page\", scopes=None):\n",
    "    \"\"\"\n",
    "    Asynchronously queries Google Search Console data with a filter and processes the results.\n",
    "\n",
    "    Args:\n",
    "        credentials_path (str): Path to the service account key file.\n",
    "        site_url (str): The site property URL in Google Search Console.\n",
    "        filter_pattern (str): The regex or filter expression for filtering data.\n",
    "        start_date (str): Query start date (YYYY-MM-DD).\n",
    "        end_date (str): Query end date (YYYY-MM-DD).\n",
    "        dimensions (tuple): Dimensions to include in the query (default: ('query', 'page')).\n",
    "        filter_dimension (str): Dimension to apply the filter (e.g., \"page\", \"query\").\n",
    "        scopes (list, optional): Scopes for the connection.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame containing query and metrics.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting async query with filter '{filter_pattern}' on dimension '{filter_dimension}'\")\n",
    "    df = query_web_gsc(\n",
    "        credentials_path=credentials_path,\n",
    "        site_url=site_url,\n",
    "        filter_pattern=filter_pattern,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        dimensions=dimensions,\n",
    "        filter_dimension=filter_dimension,\n",
    "        scopes=scopes\n",
    "    )\n",
    "    return df\n",
    "\n",
    "async def async_query_web_gsc_OLD(credentials_path, site_url, filter_pattern, start_date, end_date, dimensions=(\"query\", \"page\"), scopes=None):\n",
    "    \"\"\"\n",
    "    Asynchronously queries Google Search Console data with a filter and processes the results.\n",
    "\n",
    "    Args:\n",
    "        credentials_path (str): Path to the service account key file.\n",
    "        site_url (str): The site property URL in Google Search Console.\n",
    "        filter_pattern (str): The regex or filter expression for the page.\n",
    "        start_date (str): Query start date (YYYY-MM-DD).\n",
    "        end_date (str): Query end date (YYYY-MM-DD).\n",
    "        dimensions (tuple): Dimensions to include in the query (default: ('query', 'page')).\n",
    "        scopes (list, optional): List of scopes for the connection. Defaults to mg.creds.scopes + ['https://www.googleapis.com/auth/webmasters'].\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame containing query and page metrics.\n",
    "    \"\"\"\n",
    "    if scopes is None:\n",
    "        scopes = mg.creds.scopes + ['https://www.googleapis.com/auth/webmasters']\n",
    "    elif 'https://www.googleapis.com/auth/webmasters' not in scopes:\n",
    "        scopes.append('https://www.googleapis.com/auth/webmasters')\n",
    "\n",
    "    # Step 1: Connect to GSC\n",
    "    logger.info(f\"Connecting to GSC for site: {site_url}\")\n",
    "    service = connect_to_gsc(credentials_path, scopes)\n",
    "\n",
    "    # Step 2: Prepare payload\n",
    "    payload = {\n",
    "        'startDate': start_date,\n",
    "        'endDate': end_date,\n",
    "        'dimensions': list(dimensions),\n",
    "        'type': 'web',\n",
    "        \"dimensionFilterGroups\": [\n",
    "            {\"filters\": [{\n",
    "                \"dimension\": \"page\",\n",
    "                \"operator\": \"includingRegex\", #contains/equals/notContains/notEquals/includingRegex/excludingRegex\n",
    "                \"expression\": filter_pattern\n",
    "            }]}\n",
    "        ],\n",
    "        \"rowLimit\": 25000,\n",
    "        \"startRow\": 0,\n",
    "    }\n",
    "\n",
    "    # Step 3: Execute the query and process results\n",
    "    logger.info(f\"Executing query for date range {start_date} to {end_date}\")\n",
    "    df = execute_gsc_query(service, site_url, payload)\n",
    "\n",
    "    # Step 4: Normalize the 'query' column\n",
    "    if not df.empty:\n",
    "        logger.info(\"Normalizing query and page columns\")\n",
    "        normalize_columns = [\"query\", \"page\"]  # Specify the columns to normalize\n",
    "        df = normalize_query_column(df, normalize_columns)\n",
    "\n",
    "        # Step 5: Aggregate and sort results\n",
    "        # logger.info(\"Aggregating and sorting results\")\n",
    "        # df = (\n",
    "        #     df.groupby(list(dimensions), as_index=False)\n",
    "        #     .agg({'clicks': 'sum', 'impressions': 'sum'})\n",
    "        #     .sort_values(by=['clicks', 'impressions'], ascending=[False, False])\n",
    "        # )\n",
    "        # Step 5: Group similar queries\n",
    "        logger.info(\"Grouping similar queries\")\n",
    "        df = group_similar_queries(df, query_column=\"query\", metric_columns=[\"clicks\", \"impressions\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "async def parallel_query_web_gsc(credentials_path, site_url, filter_patterns, start_date, end_date, dimensions=(\"query\", \"page\"), filter_dimension=\"page\", scopes=None):\n",
    "    \"\"\"\n",
    "    Queries Google Search Console data with multiple filter patterns in parallel.\n",
    "\n",
    "    Args:\n",
    "        credentials_path (str): Path to the service account key file.\n",
    "        site_url (str): The site property URL in Google Search Console.\n",
    "        filter_patterns (list): List of regex or filter expressions for filtering data.\n",
    "        start_date (str): Query start date (YYYY-MM-DD).\n",
    "        end_date (str): Query end date (YYYY-MM-DD).\n",
    "        dimensions (tuple): Dimensions to include in the query (default: ('query', 'page')).\n",
    "        filter_dimension (str): Dimension to apply the filter (e.g., \"page\", \"query\").\n",
    "        scopes (list, optional): Scopes for the connection.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame with results for all patterns.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting parallel queries for {len(filter_patterns)} patterns on dimension '{filter_dimension}'\")\n",
    "\n",
    "    tasks = [\n",
    "        async_query_web_gsc(\n",
    "            credentials_path=credentials_path,\n",
    "            site_url=site_url,\n",
    "            filter_pattern=pattern,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            dimensions=dimensions,\n",
    "            filter_dimension=filter_dimension,\n",
    "            scopes=scopes\n",
    "        )\n",
    "        for pattern in filter_patterns\n",
    "    ]\n",
    "\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    combined_df = pd.concat(results, ignore_index=True) if results else pd.DataFrame()\n",
    "    return combined_df\n",
    "\n",
    "async def parallel_query_web_gsc_OLD(credentials_path, site_url, filter_patterns, start_date, end_date, dimensions=(\"query\", \"page\"), scopes=None):\n",
    "    \"\"\"\n",
    "    Queries Google Search Console data with multiple filter patterns in parallel.\n",
    "\n",
    "    Args:\n",
    "        credentials_path (str): Path to the service account key file.\n",
    "        site_url (str): The site property URL in Google Search Console.\n",
    "        filter_patterns (list): List of regex or filter expressions for pages.\n",
    "        start_date (str): Query start date (YYYY-MM-DD).\n",
    "        end_date (str): Query end date (YYYY-MM-DD).\n",
    "        dimensions (tuple): Dimensions to include in the query.\n",
    "        scopes (list, optional): Scopes for the connection.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame with results for all patterns.\n",
    "    \"\"\"\n",
    "    if not isinstance(dimensions, (list, tuple)):\n",
    "        raise ValueError(\"The 'dimensions' parameter must be a list or tuple of dimension names.\")\n",
    "\n",
    "    tasks = [\n",
    "        async_query_web_gsc(\n",
    "            credentials_path,\n",
    "            site_url,\n",
    "            pattern,\n",
    "            start_date,\n",
    "            end_date,\n",
    "            dimensions=dimensions,\n",
    "            scopes=scopes\n",
    "        )\n",
    "        for pattern in filter_patterns\n",
    "    ]\n",
    "\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    combined_df = pd.concat(results, ignore_index=True) if results else pd.DataFrame()\n",
    "    return combined_df\n",
    "\n",
    "logger.info(f\"対象期間：{LM_From} - {LM___To}\")\n",
    "\n",
    "GSC_PROPERTY = \"https://corp.shiseido.com\"\n",
    "\n",
    "async def main():\n",
    "    global df_jp, df_en\n",
    "    filter_patterns = [\"/jp/\", \"/sbs/\", \"/scp/\"]  # Split your filter patterns logically\n",
    "    _df = await parallel_query_web_gsc(CREDS_PATH, GSC_PROPERTY, filter_patterns, LM_From, LM___To, [\"query\"], \"page\", mg.creds.scopes)\n",
    "    df_jp = group_similar_queries(_df, query_column=\"query\", metric_columns=[\"clicks\", \"impressions\"])\n",
    "    df_jp['ctr'] = df_jp['clicks'] / df_jp['impressions']\n",
    "    logger.info(f\"Japanese data contains {len(df_jp)} rows.\")\n",
    "\n",
    "    filter_patterns = [\"/en/\"]\n",
    "    _df = await parallel_query_web_gsc(CREDS_PATH, GSC_PROPERTY, filter_patterns, LM_From, LM___To, [\"query\"], \"page\", mg.creds.scopes)\n",
    "    df_en = group_similar_queries(_df, query_column=\"query\", metric_columns=[\"clicks\", \"impressions\"])\n",
    "    df_en['ctr'] = df_en['clicks'] / df_en['impressions']\n",
    "    logger.info(f\"English data contains {len(df_en)} rows.\")\n",
    "\n",
    "await main()\n",
    "\n",
    "df_jp[df_jp['clicks']>0]\n",
    "# df_en[df_en['clicks']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "vrn0B-3Qrv9W"
   },
   "outputs": [],
   "source": [
    "#@title GSCからデータを取得して加工　 改善前\n",
    "\n",
    "# Import necessary libraries\n",
    "try:\n",
    "    import jaconv\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -q jaconv\n",
    "    import jaconv\n",
    "    from IPython.display import clear_output\n",
    "    clear_output()\n",
    "\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "import logging\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "\n",
    "# Enable asyncio for Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Connect to Google Search Console\n",
    "def connect_to_gsc(credentials_path, scopes=None):\n",
    "    scopes = scopes or ['https://www.googleapis.com/auth/webmasters']\n",
    "    if 'https://www.googleapis.com/auth/webmasters' not in scopes:\n",
    "        scopes.append('https://www.googleapis.com/auth/webmasters')\n",
    "    try:\n",
    "        credentials = service_account.Credentials.from_service_account_file(\n",
    "            credentials_path, scopes=scopes\n",
    "        )\n",
    "        return build('searchconsole', 'v1', credentials=credentials)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to connect to GSC: {e}\")\n",
    "        raise\n",
    "\n",
    "# Execute a GSC query\n",
    "def execute_gsc_query(service, site_url, payload):\n",
    "    all_results = []\n",
    "    start_row = 0\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            payload[\"startRow\"] = start_row\n",
    "            response = service.searchanalytics().query(siteUrl=site_url, body=payload).execute()\n",
    "            rows = response.get('rows', [])\n",
    "            if not rows:\n",
    "                break\n",
    "            all_results.extend(rows)\n",
    "            start_row += len(rows)\n",
    "\n",
    "        # Process the results\n",
    "        results = []\n",
    "        dimensions = payload[\"dimensions\"]\n",
    "        for row in all_results:\n",
    "            data = {dim: row['keys'][i] for i, dim in enumerate(dimensions)}\n",
    "            data.update({\"clicks\": row.get('clicks', 0), \"impressions\": row.get('impressions', 0)})\n",
    "            results.append(data)\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during API call: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Normalize query column\n",
    "def normalize_query_column(df, query_columns):\n",
    "    for column in query_columns:\n",
    "        if column in df.columns:\n",
    "            df[column] = df[column].apply(lambda x: jaconv.h2z(x, kana=True) if pd.notnull(x) else x)\n",
    "            df[column] = df[column].apply(lambda x: jaconv.z2h(x, kana=False, ascii=True, digit=True) if pd.notnull(x) else x)\n",
    "    return df\n",
    "\n",
    "# Group similar queries\n",
    "def group_similar_queries(df, query_column=\"query\", metric_columns=None, additional_dimensions=None):\n",
    "    if query_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{query_column}' not found in the DataFrame.\")\n",
    "\n",
    "    metric_columns = metric_columns or df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    additional_dimensions = additional_dimensions or [col for col in df.columns if col not in metric_columns + [query_column]]\n",
    "\n",
    "    def normalize_query(query):\n",
    "        return \"\".join(sorted(query.replace(\" \", \"\").replace(\"　\", \"\"))) if pd.notnull(query) else query\n",
    "\n",
    "    df[\"normalized_query\"] = df[query_column].apply(normalize_query)\n",
    "\n",
    "    agg_rules = {query_column: \"first\", **{col: \"sum\" for col in metric_columns}}\n",
    "    grouped = df.groupby([\"normalized_query\"] + additional_dimensions, as_index=False).agg(agg_rules)\n",
    "    return grouped.drop(columns=[\"normalized_query\"])\n",
    "\n",
    "# Query GSC with flexibility\n",
    "def query_web_gsc(credentials_path, site_url, filter_pattern, start_date, end_date, dimensions, filter_dimension=\"page\", scopes=None):\n",
    "    logger.info(f\"Querying GSC for {filter_dimension}: {filter_pattern}\")\n",
    "    service = connect_to_gsc(credentials_path, scopes)\n",
    "\n",
    "    # if filter_dimension not in dimensions:\n",
    "    #     raise ValueError(f\"filter_dimension '{filter_dimension}' must be one of the specified dimensions: {dimensions}\")\n",
    "\n",
    "    payload = {\n",
    "        'startDate': start_date,\n",
    "        'endDate': end_date,\n",
    "        'dimensions': dimensions,\n",
    "        'type': 'web',\n",
    "        \"dimensionFilterGroups\": [\n",
    "            {\"filters\": [{\n",
    "                \"dimension\": filter_dimension,\n",
    "                \"operator\": \"includingRegex\",\n",
    "                \"expression\": filter_pattern\n",
    "            }]}\n",
    "        ],\n",
    "        \"rowLimit\": 25000\n",
    "    }\n",
    "\n",
    "    return execute_gsc_query(service, site_url, payload)\n",
    "\n",
    "# Asynchronous GSC query\n",
    "async def async_query_web_gsc(credentials_path, site_url, filter_pattern, start_date, end_date, dimensions, filter_dimension=\"page\", scopes=None):\n",
    "    return query_web_gsc(credentials_path, site_url, filter_pattern, start_date, end_date, dimensions, filter_dimension, scopes)\n",
    "\n",
    "# Parallel GSC queries\n",
    "async def parallel_query_web_gsc(credentials_path, site_url, filter_patterns, start_date, end_date, dimensions, filter_dimension=\"page\", scopes=None):\n",
    "    tasks = [\n",
    "        async_query_web_gsc(credentials_path, site_url, pattern, start_date, end_date, dimensions, filter_dimension, scopes)\n",
    "        for pattern in filter_patterns\n",
    "    ]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return pd.concat(results, ignore_index=True) if results else pd.DataFrame()\n",
    "\n",
    "# Main function\n",
    "async def main():\n",
    "    global df_jp, df_en\n",
    "    filter_patterns_jp = [\"/jp/\", \"/sbs/\", \"/scp/\"]\n",
    "    df_jp_raw = await parallel_query_web_gsc(CREDS_PATH, GSC_PROPERTY, filter_patterns_jp, LM_From, LM___To, [\"query\"], \"page\", mg.creds.scopes)\n",
    "    df_jp = group_similar_queries(df_jp_raw, query_column=\"query\", metric_columns=[\"clicks\", \"impressions\"])\n",
    "    df_jp['ctr'] = df_jp['clicks'] / df_jp['impressions']\n",
    "    logger.info(f\"Japanese data contains {len(df_jp)} rows.\")\n",
    "\n",
    "    filter_patterns_en = [\"/en/\"]\n",
    "    df_en_raw = await parallel_query_web_gsc(CREDS_PATH, GSC_PROPERTY, filter_patterns_en, LM_From, LM___To, [\"query\"], \"page\", mg.creds.scopes)\n",
    "    df_en = group_similar_queries(df_en_raw, query_column=\"query\", metric_columns=[\"clicks\", \"impressions\"])\n",
    "    df_en['ctr'] = df_en['clicks'] / df_en['impressions']\n",
    "    logger.info(f\"English data contains {len(df_en)} rows.\")\n",
    "\n",
    "logger.info(f\"対象期間：{LM_From} - {LM___To}\")\n",
    "\n",
    "GSC_PROPERTY = \"https://corp.shiseido.com\"\n",
    "\n",
    "await main()\n",
    "\n",
    "# Filtered results\n",
    "df_jp[df_jp['clicks'] > 0]\n",
    "# df_en[df_en['clicks'] > 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGqbtK1J-IX-"
   },
   "source": [
    "# Visualizeテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zwqsbtslmWV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "odKSEbijwXFW",
    "outputId": "ed36ff58-b6ec-4c47-dc4a-6767de9e7dc2"
   },
   "outputs": [],
   "source": [
    "# Configure Japanese font\n",
    "!apt-get update\n",
    "!apt-get install -y fonts-ipafont-gothic\n",
    "%pip install -q japanize-matplotlib\n",
    "clear_output()\n",
    "\n",
    "# キャッシュを削除してから\n",
    "!rm /root/.cache/matplotlib/fontlist-v330.json\n",
    "# ランタイムを再起動"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "W_rbYW9JNVD_",
    "outputId": "0a8a1335-de1e-4687-9c0a-d8aad1342c6d"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib  # Import japanize_matplotlib after installing matplotlib\n",
    "from matplotlib import font_manager\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "\n",
    "plt.rcParams['font.family'] = 'IPAPGothic'\n",
    "japanize_matplotlib.japanize()\n",
    "\n",
    "def create_network_diagram(df, query_column=\"query\", top_n_words=50):\n",
    "    \"\"\"\n",
    "    Creates a network diagram of words used in queries.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing queries.\n",
    "        query_column (str): The column name containing the queries.\n",
    "        top_n_words (int): Number of top words to include based on co-occurrence.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the network diagram.\n",
    "    \"\"\"\n",
    "    # Extract and split words from queries\n",
    "    all_words = df[query_column].dropna().str.split()\n",
    "    word_pairs = []\n",
    "\n",
    "    # Generate co-occurrences for each query\n",
    "    for words in all_words:\n",
    "        word_pairs.extend(combinations(sorted(set(words)), 2))  # Unique pairs of words\n",
    "\n",
    "    # Count co-occurrences\n",
    "    pair_counts = Counter(word_pairs)\n",
    "\n",
    "    # Create a graph\n",
    "    G = nx.Graph()\n",
    "    for (word1, word2), count in pair_counts.items():\n",
    "        if count > 1:  # Only include pairs with significant co-occurrences\n",
    "            G.add_edge(word1, word2, weight=count)\n",
    "\n",
    "    # Get the top nodes by degree\n",
    "    top_nodes = dict(sorted(G.degree(weight=\"weight\"), key=lambda x: x[1], reverse=True)[:top_n_words])\n",
    "    subgraph = G.subgraph(top_nodes.keys())\n",
    "\n",
    "    # Draw the network diagram\n",
    "    pos = nx.spring_layout(subgraph, seed=42)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    nx.draw(\n",
    "        subgraph,\n",
    "        pos,\n",
    "        with_labels=True,\n",
    "        node_size=[v * 100 for v in top_nodes.values()],\n",
    "        edge_color=\"lightblue\",\n",
    "        font_size=10,\n",
    "        font_weight=\"bold\",\n",
    "        alpha=0.7,\n",
    "        font_family='IPAPGothic',  # Use Japanese font\n",
    "    )\n",
    "    nx.draw_networkx_edge_labels(\n",
    "        subgraph,\n",
    "        pos,\n",
    "        edge_labels={(u, v): d[\"weight\"] for u, v, d in subgraph.edges(data=True)},\n",
    "        font_family='IPAPGothic',\n",
    "    )\n",
    "    plt.title(\"Network Diagram of Query Words\", fontsize=15)#, fontproperties=japanese_font)\n",
    "    plt.show()\n",
    "\n",
    "create_network_diagram(df_jp[df_jp['clicks'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wGxq_6DDSNlL",
    "outputId": "ff9189c9-53af-4442-d788-1ffa6b466a39"
   },
   "outputs": [],
   "source": [
    "# prompt: make nodes smaller to avoid overrapp\n",
    "\n",
    "def create_network_diagram(df, query_column=\"query\", top_n_words=50):\n",
    "    \"\"\"\n",
    "    Creates a network diagram of words used in queries.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing queries.\n",
    "        query_column (str): The column name containing the queries.\n",
    "        top_n_words (int): Number of top words to include based on co-occurrence.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the network diagram.\n",
    "    \"\"\"\n",
    "    # Extract and split words from queries\n",
    "    all_words = df[query_column].dropna().str.split()\n",
    "    word_pairs = []\n",
    "\n",
    "    # Generate co-occurrences for each query\n",
    "    for words in all_words:\n",
    "        word_pairs.extend(combinations(sorted(set(words)), 2))  # Unique pairs of words\n",
    "\n",
    "    # Count co-occurrences\n",
    "    pair_counts = Counter(word_pairs)\n",
    "\n",
    "    # Create a graph\n",
    "    G = nx.Graph()\n",
    "    for (word1, word2), count in pair_counts.items():\n",
    "        if count > 1:  # Only include pairs with significant co-occurrences\n",
    "            G.add_edge(word1, word2, weight=count)\n",
    "\n",
    "    # Get the top nodes by degree\n",
    "    top_nodes = dict(sorted(G.degree(weight=\"weight\"), key=lambda x: x[1], reverse=True)[:top_n_words])\n",
    "    subgraph = G.subgraph(top_nodes.keys())\n",
    "\n",
    "    # Draw the network diagram\n",
    "    pos = nx.spring_layout(subgraph, seed=42)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    nx.draw(\n",
    "        subgraph,\n",
    "        pos,\n",
    "        with_labels=True,\n",
    "        node_size=[v * 50 for v in top_nodes.values()], # Reduced node size\n",
    "        edge_color=\"lightblue\",\n",
    "        font_size=8, # Reduced font size\n",
    "        font_weight=\"bold\",\n",
    "        alpha=0.7,\n",
    "        font_family='IPAPGothic',  # Use Japanese font\n",
    "        width=0.5 #reduced edge width\n",
    "    )\n",
    "    nx.draw_networkx_edge_labels(\n",
    "        subgraph,\n",
    "        pos,\n",
    "        edge_labels={(u, v): d[\"weight\"] for u, v, d in subgraph.edges(data=True)},\n",
    "        font_size=6, # Reduced font size\n",
    "        font_family='IPAPGothic',\n",
    "    )\n",
    "    plt.title(\"Network Diagram of Query Words\", fontsize=15)#, fontproperties=japanese_font)\n",
    "    plt.show()\n",
    "\n",
    "create_network_diagram(df_jp[df_jp['clicks'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "ofNgkyuqSDJn",
    "outputId": "09b440b2-2bb6-4499-a0cb-829700e37a58"
   },
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "import os\n",
    "\n",
    "def create_network_diagram_interactive(df, query_column=\"query\", top_n_words=50):\n",
    "    \"\"\"\n",
    "    Creates an interactive network diagram of words used in queries.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing queries.\n",
    "        query_column (str): The column name containing the queries.\n",
    "        top_n_words (int): Number of top words to include based on co-occurrence.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the network diagram in a browser.\n",
    "    \"\"\"\n",
    "    # Extract and split words from queries\n",
    "    all_words = df[query_column].dropna().str.split()\n",
    "    word_pairs = []\n",
    "\n",
    "    # Generate co-occurrences for each query\n",
    "    for words in all_words:\n",
    "        word_pairs.extend(combinations(sorted(set(words)), 2))  # Unique pairs of words\n",
    "\n",
    "    # Count co-occurrences\n",
    "    pair_counts = Counter(word_pairs)\n",
    "\n",
    "    # Create a graph\n",
    "    G = nx.Graph()\n",
    "    for (word1, word2), count in pair_counts.items():\n",
    "        if count > 1:  # Only include pairs with significant co-occurrences\n",
    "            G.add_edge(word1, word2, weight=count)\n",
    "\n",
    "    # Get the top nodes by degree\n",
    "    top_nodes = dict(sorted(G.degree(weight=\"weight\"), key=lambda x: x[1], reverse=True)[:top_n_words])\n",
    "    subgraph = G.subgraph(top_nodes.keys())\n",
    "\n",
    "    # Create a Pyvis Network\n",
    "    net = Network(height=\"750px\", width=\"100%\", bgcolor=\"#ffffff\", font_color=\"black\")\n",
    "    net.barnes_hut()\n",
    "\n",
    "    # Add nodes and edges with attributes\n",
    "    for node, degree in subgraph.degree():\n",
    "        net.add_node(node, label=node, size=degree * 5)\n",
    "\n",
    "    for edge in subgraph.edges(data=True):\n",
    "        net.add_edge(edge[0], edge[1], value=edge[2]['weight'])\n",
    "\n",
    "    # Add title and other details\n",
    "    net.set_options(\"\"\"\n",
    "    const options = {\n",
    "      \"nodes\": {\n",
    "        \"shape\": \"dot\",\n",
    "        \"scaling\": {\n",
    "          \"min\": 10,\n",
    "          \"max\": 50\n",
    "        }\n",
    "      },\n",
    "      \"edges\": {\n",
    "        \"color\": {\n",
    "          \"inherit\": true\n",
    "        },\n",
    "        \"smooth\": false\n",
    "      },\n",
    "      \"physics\": {\n",
    "        \"barnesHut\": {\n",
    "          \"gravitationalConstant\": -20000,\n",
    "          \"springLength\": 95\n",
    "        },\n",
    "        \"minVelocity\": 0.75\n",
    "      }\n",
    "    }\n",
    "    \"\"\")\n",
    "\n",
    "    output_dir = 'output'  # Or any directory you prefer\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    html_file_path = os.path.join(output_dir, \"network_diagram.html\")\n",
    "\n",
    "    # Show the network diagram\n",
    "    net.show(html_file_path)\n",
    "\n",
    "create_network_diagram_interactive(df_jp[df_jp['clicks'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l91xS34wa8np"
   },
   "outputs": [],
   "source": [
    "!pip install -q pyvis==0.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbn6qGqpbDJy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "9h6nV8KaGiBD",
    "ylf6KYYMVh0m",
    "rhbvI2FkW1cB",
    "TMqsfW9wXH0w",
    "OCMXbofKXK8O",
    "8IyDGQaIQpIr",
    "ZEG8q5Q49dPk",
    "RImFEt_rcFf0",
    "vySTE8jsKaHx",
    "tGqbtK1J-IX-"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "a42f255187684fd0bfcfec591e0f0db8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "TabModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "TabModel",
      "_titles": {
       "0": "GA4"
      },
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "TabView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6f5905ddb5954b388898a6ef2050ca28"
      ],
      "layout": "IPY_MODEL_0fb956005a3d42598bf014007767f0df",
      "selected_index": 0
     }
    },
    "6f5905ddb5954b388898a6ef2050ca28": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "VBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3d0b35cf47cc4ff6a38eb169f9e8f5f0",
       "IPY_MODEL_285452a6a1994f6881dad7731e4c7952"
      ],
      "layout": "IPY_MODEL_76266de82e4146c49dfeb063b3bdf74f"
     }
    },
    "0fb956005a3d42598bf014007767f0df": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d0b35cf47cc4ff6a38eb169f9e8f5f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DropdownModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "1872shiseido",
       "【新】企業情報サイト",
       "資生堂150年史",
       "GA4 - Shiseido whole JP domain"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "GA4アカウント: ",
      "description_tooltip": null,
      "disabled": false,
      "index": 0,
      "layout": "IPY_MODEL_c9d283909a0f49bbb7319fc9ed2c3427",
      "style": "IPY_MODEL_92c4e657f7c744f89add94c5c48cd17b"
     }
    },
    "285452a6a1994f6881dad7731e4c7952": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DropdownModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "WITH - GA4"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "GA4プロパティ: ",
      "description_tooltip": null,
      "disabled": false,
      "index": 0,
      "layout": "IPY_MODEL_cc9172dd035e4408b92fc3c0904be700",
      "style": "IPY_MODEL_dcb92e63da364140959799caaa54a478"
     }
    },
    "76266de82e4146c49dfeb063b3bdf74f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9d283909a0f49bbb7319fc9ed2c3427": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "max-content"
     }
    },
    "92c4e657f7c744f89add94c5c48cd17b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "110px"
     }
    },
    "cc9172dd035e4408b92fc3c0904be700": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "max-content"
     }
    },
    "dcb92e63da364140959799caaa54a478": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "110px"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}